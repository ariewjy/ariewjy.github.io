[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "¬© Copyright Aditya Arie Wijaya\nThis is my personal website. Nothing here is endorsed by my employer or any organizations of which I am become a part of. Every content in this website can be used without restriction as long as you provide a citation back to the original material. The website is provided under a Creative Commons (CC-BY) 4.0 license and source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2022\n\nCritical Porosity in Understanding Acoustic Porosity Anomalies. A Case Study in Complex Carbonate Reservoir in Indonesia. Wijaya, Aditya Arie, Kunaifi, Sunawar, Laya, Krishna Pratama, Luqman, Ramadhan, Dimmas. Proceedings, Indonesia Petroleum Association. 46th Annual Convention & Exhibition Sep 2022. Paper\n\n\n\n2021\n\nThru-Tubing Integrity Assurance in CO2-Injection Well Using Electromagnetic Corrosion Logging Tool: A Case Study in Far East Test Field, Japan. Aditya Arie Wijaya and Akira Endo and Sarvagya Parashar. SPWLA Japan JFES 26th, Virtual Event, 2021. Abstract Only\nIntegrated Evaluation of Laminated Sand-Shale Gas-Bearing Reservoir Using Tensor Model: A Case Study Combining Data from Triaxial Resistivity, Image, Sonic, and Reservoir Testing in B-Field, Malaysia. Aditya Arie Wijaya, Ivan Zhia Ming Wu, Sarvagya Parashar, Mohammad Iffwad, Amirul Afiq B Yaakob, William Amelio Tolioe, Adib Akmal Che Sidid and Nadhirah Bt. Ahmad. SPWLA 62nd Annual Logging Symposium, Virtual Event. https://doi.org/10.30632/SPWLA-2021-0043, Paper.\nFull Well Corrosion Insight ‚Äì Case Studies in the Added Value of Electromagnetic Thickness Measurements During Well Interventions. Andrew Imrie, Maciej Kozlowski, Omar Torky, and Aditya Arie Wijaya. SPE/ICoTA Well Intervention Conference and Exhibition, Virtual. 2021. https://doi.org/10.2118/204431-MS, Paper.\n\n\n\n2020\n\nMulti-Detector Pulsed Neutron Tool Application in Low Porosity Reservoir ‚Äî A Case Study in Mutiara Field, Indonesia. Aditya Arie Wijaya, Rama Aulianagara, Weijun Guo, Fetty Maria Naibaho, Fransiscus Xaverius Asriwan and Usman Amirudin. SPWLA 61st Annual Logging Symposium, Virtual Online Webinar and Featured in the SPWLA Journal Dec 2020. https://doi.org/10.30632/SPWLA-5082, Paper, SPWLA Journal Article.\n\n\n\n2019\n\nPractical Application of Tensor Model in Laminated Sand Shale Analysis. Aditya Arie Wijaya. Abu Dhabi International Petroleum Exhibition & Conference, Abu Dhabi, UAE, 2019. https://doi.org/10.2118/197208-MS, Paper.\nBehind Casing Gas Identification Using Ultrasonic Wireline Logs: An Overview of Multiwell Field Plug and Abandonment Study, Offshore Malaysia. Andrew Imrie, Mohd Hazwan Zainal Abidin, Aditya Arie Wijaya, Ping Ting, Ulimaz Dhania, Ekky Nugroho Mahardika, Li Juan Saw, Siti Najmi Farhan Bt Zulkipli. SPE Symposium: Decommissioning and Abandonment, Malaysia, 2019. https://doi.org/10.2118/199180-MS, Paper.\nWhere‚Äôs the Water Coming from? ‚Äî A Combined Formation Saturation, Production Logging, Water Flow, and Leak Detection Diagnosis Deployed on Coiled Tubing. Andrew Imrie, Muhammad Bagir, Glen Ricky Himawan, Mahadevan S Iyer and Aditya Arie Wijaya. SPE/IATMI Asia Pacific Oil & Gas Conference and Exhibition, Bali, Indonesia, 2019. https://doi.org/10.2118/196544-MS, Paper.\n\n\n\n2018\n\nSuccess Novel of Integrating Pulsed Neutron and Comprehensive Production Data Analysis to Optimize Well Production, Indonesia. Aditya Arie Wijaya and Muhammad Bagir. SPWLA Asia Pacific Symposium Bogor, Indonesia, 2018. SPWLA-2018-1841, Paper."
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html",
    "href": "posts/20230723-co2-growth/index.html",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "",
    "text": "Our World in Data (OWID) shows some examples from a country that was able to decoupled their economy from CO2 emission. Meaning that they still able to increase their GDP while at the same time reducing CO2 emission. UK is the example used in Figure¬†1.\n\n\n\nFigure¬†1: United Kingdom Decoupled CO2 Emission vs Economic Growth\n\n\nYou can read the full article, but I am going to quote paragraph that interest me to write this article.\n\n‚ÄúThese countries show that economic growth is not incompatible with reducing emissions.‚Äù\n\nThis narrative means that you can grow your economy without emitting more emission - a bold statement considering the other chart from Figure¬†2 showed the opposite, where the GDP of a country is strongly related to the CO2 emission.\nEven OWID themselves explained:\n\n‚ÄúHistorically, CO2 emissions have been strongly correlated with how much money we have. This is particularly true at low-to-middle incomes. The richer we are, the more CO2 we emit. This is because we use more energy ‚Äì which often comes from burning fossil fuels.‚Äù (source: Our World in Data)\n\n\n\n\nFigure¬†2: CO2 Emission per Capita vs GDP per Capita\n\n\n\nHere is where It got me concerned. There should be a big asterix on a narrative that countries can grow their economy while also reducing emission - Only when they are rich.\nThe premise is simple, country needs energy to grow their economy, the higher the energy consumption the higher the CO2 emission would be. I think the narrative that a country emits more CO2 because they are rich can be misleading. I think it is the other way around.\n\nSo not because we are rich we emit more CO2, but we are rich because we emit more CO2 used for energy, to grow the economy.\n\nThis is of course no to undermine the impact of CO2 emission to our global temperature, but rather to manage our expectations and a reality check on what can really be done. Some questions about ‚Äúcan we reduce our CO2 emission but still maintaining economy growth‚Äù? Or ‚Äúdo we have to increase our CO2 to raise our economic growth?‚Äù are some fair questions to be addressed in more detail.\nDespite some articles pointed out the fact that some countries were able to decoupled their economy from CO2 emission as shown in Figure¬†1, it is important to understand the context, in which these countries were positioned compared to rest of the world.\n\nThis article will explore that question.\n\nWhat allows these countries to decouple their economy from their emissions?"
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html#co2-emission-vs-gdp",
    "href": "posts/20230723-co2-growth/index.html#co2-emission-vs-gdp",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "",
    "text": "Our World in Data (OWID) shows some examples from a country that was able to decoupled their economy from CO2 emission. Meaning that they still able to increase their GDP while at the same time reducing CO2 emission. UK is the example used in Figure¬†1.\n\n\n\nFigure¬†1: United Kingdom Decoupled CO2 Emission vs Economic Growth\n\n\nYou can read the full article, but I am going to quote paragraph that interest me to write this article.\n\n‚ÄúThese countries show that economic growth is not incompatible with reducing emissions.‚Äù\n\nThis narrative means that you can grow your economy without emitting more emission - a bold statement considering the other chart from Figure¬†2 showed the opposite, where the GDP of a country is strongly related to the CO2 emission.\nEven OWID themselves explained:\n\n‚ÄúHistorically, CO2 emissions have been strongly correlated with how much money we have. This is particularly true at low-to-middle incomes. The richer we are, the more CO2 we emit. This is because we use more energy ‚Äì which often comes from burning fossil fuels.‚Äù (source: Our World in Data)\n\n\n\n\nFigure¬†2: CO2 Emission per Capita vs GDP per Capita\n\n\n\nHere is where It got me concerned. There should be a big asterix on a narrative that countries can grow their economy while also reducing emission - Only when they are rich.\nThe premise is simple, country needs energy to grow their economy, the higher the energy consumption the higher the CO2 emission would be. I think the narrative that a country emits more CO2 because they are rich can be misleading. I think it is the other way around.\n\nSo not because we are rich we emit more CO2, but we are rich because we emit more CO2 used for energy, to grow the economy.\n\nThis is of course no to undermine the impact of CO2 emission to our global temperature, but rather to manage our expectations and a reality check on what can really be done. Some questions about ‚Äúcan we reduce our CO2 emission but still maintaining economy growth‚Äù? Or ‚Äúdo we have to increase our CO2 to raise our economic growth?‚Äù are some fair questions to be addressed in more detail.\nDespite some articles pointed out the fact that some countries were able to decoupled their economy from CO2 emission as shown in Figure¬†1, it is important to understand the context, in which these countries were positioned compared to rest of the world.\n\nThis article will explore that question.\n\nWhat allows these countries to decouple their economy from their emissions?"
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html#data-importing-and-cleaning",
    "href": "posts/20230723-co2-growth/index.html#data-importing-and-cleaning",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "Data Importing and Cleaning",
    "text": "Data Importing and Cleaning\nI explored the CO2 emission dataset provided by the OWID (Our World in Data). I will be focusing mainly on the CO2 emission and it‚Äôs impact to GDP of a country. My premise stays the same, that a country must burn the energy to grow their economy, and to do that they will have to emit CO2, since more than 80% of energy (see Figure¬†3) in the world still comes from fossil-fuels (oil, gas, coal).\n\n\n\nFigure¬†3: Global Energy Consumption\n\n\nThe dataset is downloaded from the provided link in the code block below.\n\n\nCode\n#importing dataset\n\nimport pandas as pd\nimport warnings\n\n# Ignore future warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'\n\nco2_raw = pd.read_csv('https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv')\nco2_remark = pd.read_csv('https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-codebook.csv')\nco2_raw\n\n\n\n\n\n\n\n\n\ncountry\nyear\niso_code\npopulation\ngdp\ncement_co2\ncement_co2_per_capita\nco2\nco2_growth_abs\nco2_growth_prct\n...\nshare_global_other_co2\nshare_of_temperature_change_from_ghg\ntemperature_change_from_ch4\ntemperature_change_from_co2\ntemperature_change_from_ghg\ntemperature_change_from_n2o\ntotal_ghg\ntotal_ghg_excluding_lucf\ntrade_co2\ntrade_co2_share\n\n\n\n\n0\nAfghanistan\n1850\nAFG\n3752993.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nAfghanistan\n1851\nAFG\n3767956.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.165\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\nAfghanistan\n1852\nAFG\n3783940.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.164\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\nAfghanistan\n1853\nAFG\n3800954.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.164\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAfghanistan\n1854\nAFG\n3818038.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.163\n0.000\n0.000\n0.000\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50593\nZimbabwe\n2017\nZWE\n14751101.0\n2.194784e+10\n0.469\n0.032\n9.596\n-0.937\n-8.899\n...\nNaN\n0.114\n0.001\n0.001\n0.002\n0.0\n115.00\n27.71\n0.910\n9.486\n\n\n50594\nZimbabwe\n2018\nZWE\n15052191.0\n2.271535e+10\n0.558\n0.037\n11.795\n2.199\n22.920\n...\nNaN\n0.114\n0.001\n0.001\n0.002\n0.0\n116.76\n29.37\n0.771\n6.537\n\n\n50595\nZimbabwe\n2019\nZWE\n15354606.0\nNaN\n0.570\n0.037\n11.115\n-0.681\n-5.772\n...\nNaN\n0.113\n0.001\n0.001\n0.002\n0.0\n116.03\n28.70\n0.978\n8.795\n\n\n50596\nZimbabwe\n2020\nZWE\n15669663.0\nNaN\n0.570\n0.036\n10.608\n-0.507\n-4.559\n...\nNaN\n0.112\n0.001\n0.001\n0.002\n0.0\n113.20\n25.99\n1.006\n9.481\n\n\n50597\nZimbabwe\n2021\nZWE\n15993525.0\nNaN\n0.570\n0.036\n11.296\n0.688\n6.488\n...\nNaN\n0.111\n0.001\n0.001\n0.002\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n50598 rows √ó 79 columns\n\n\n\n\nThere are over 70 columns in the original dataset, so we will only use columns that we are interested in, mainly related to GDP and CO2 emission of a country, referenced by year. Some data cleaning (removing any null rows in GDP per Capita, or CO2 consumption per Capita, etc.).\n\n\nCode\n#selecting dataset\nco2 = co2_raw[[ 'country', 'year','population', 'gdp', 'co2_per_capita', 'consumption_co2_per_capita' ]]\n\n#adding gdp per capita column\nco2['gdp_per_capita'] = co2['gdp']/ co2['population']\n\n# dropping any rows with null consumption_co2_per_capita\nco2 = co2[~co2.consumption_co2_per_capita.isnull()].reset_index(drop=True)\n\n#drop gdp column\nco2 = co2.drop(columns='gdp')\n\n#removing any incomplete data\nco2 = co2.query(\" gdp_per_capita&gt;0 & co2_per_capita&gt;0\")\nco2.dropna().sample(5)\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\nco2_per_capita\nconsumption_co2_per_capita\ngdp_per_capita\n\n\n\n\n614\nBurkina Faso\n2015\n18718022.0\n0.198\n0.260\n1401.644279\n\n\n2089\nKyrgyzstan\n2002\n5026646.0\n0.979\n1.118\n3216.956624\n\n\n2751\nNigeria\n2013\n174726128.0\n0.666\n0.645\n5493.963030\n\n\n1663\nHong Kong\n2010\n7132437.0\n5.617\n11.312\n41909.730236\n\n\n3942\nUnited Arab Emirates\n2015\n8916909.0\n24.266\n27.123\n77553.611172\n\n\n\n\n\n\n\n\nI would like to make graphic where I can colored a country based on its continent, just for an additional context to the data. We took that from Gapminder dataset1, and combine it with the previously curated CO2 dataset we just created.\n\n\nCode\n#importing gapminder\ngapminder=pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv')\ngapminder = gapminder[['country', 'continent']]\ngapminder = gapminder.drop_duplicates().reset_index(drop=True)\n\n#merging with original co2 dataset\nco2 = pd.merge(co2, gapminder, on='country', how='inner')\n\n#drop consumption per capita\nco2 = co2.drop(columns='consumption_co2_per_capita')\n\n#sanity check\nco2\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\nco2_per_capita\ngdp_per_capita\ncontinent\n\n\n\n\n0\nAlbania\n1990\n3295073.0\n1.675\n3929.457471\nEurope\n\n\n1\nAlbania\n1991\n3302087.0\n1.299\n2839.369269\nEurope\n\n\n2\nAlbania\n1992\n3303738.0\n0.762\n2647.207768\nEurope\n\n\n3\nAlbania\n1993\n3300715.0\n0.708\n2919.476286\nEurope\n\n\n4\nAlbania\n1994\n3294001.0\n0.584\n3216.795546\nEurope\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2786\nZimbabwe\n2014\n13855758.0\n0.862\n1531.673864\nAfrica\n\n\n2787\nZimbabwe\n2015\n14154937.0\n0.866\n1485.520916\nAfrica\n\n\n2788\nZimbabwe\n2016\n14452705.0\n0.729\n1450.371681\nAfrica\n\n\n2789\nZimbabwe\n2017\n14751101.0\n0.651\n1487.877848\nAfrica\n\n\n2790\nZimbabwe\n2018\n15052191.0\n0.784\n1509.106089\nAfrica\n\n\n\n\n2791 rows √ó 6 columns\n\n\n\n\nI would like to also see depending on the GDP per Capita, from which income class is a certain country belongs to. I am using a rough classification by world-bank. I know this may not be the best representation as income class is not the same every year2, and I am not exactly using exact number here, but I think this is good enough to put some contexts for data illustration.\n\n\nCode\n#creating income class category based on gdp per capita. \n\nbins= [0.00001,1000,4000,12000,1000000] #setting up the group based on bmi bins \nlabels = [\n         'lower',\n         'lower-middle',\n         'upper-middle',\n         'upper'\n         ] #setting up the label on each group\n\nco2['income_class']= pd.cut(\n   co2['gdp_per_capita'], \n   bins=bins, \n   labels=labels,\n   include_lowest=False\n   )\n\nco2\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\nco2_per_capita\ngdp_per_capita\ncontinent\nincome_class\n\n\n\n\n0\nAlbania\n1990\n3295073.0\n1.675\n3929.457471\nEurope\nlower-middle\n\n\n1\nAlbania\n1991\n3302087.0\n1.299\n2839.369269\nEurope\nlower-middle\n\n\n2\nAlbania\n1992\n3303738.0\n0.762\n2647.207768\nEurope\nlower-middle\n\n\n3\nAlbania\n1993\n3300715.0\n0.708\n2919.476286\nEurope\nlower-middle\n\n\n4\nAlbania\n1994\n3294001.0\n0.584\n3216.795546\nEurope\nlower-middle\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2786\nZimbabwe\n2014\n13855758.0\n0.862\n1531.673864\nAfrica\nlower-middle\n\n\n2787\nZimbabwe\n2015\n14154937.0\n0.866\n1485.520916\nAfrica\nlower-middle\n\n\n2788\nZimbabwe\n2016\n14452705.0\n0.729\n1450.371681\nAfrica\nlower-middle\n\n\n2789\nZimbabwe\n2017\n14751101.0\n0.651\n1487.877848\nAfrica\nlower-middle\n\n\n2790\nZimbabwe\n2018\n15052191.0\n0.784\n1509.106089\nAfrica\nlower-middle\n\n\n\n\n2791 rows √ó 7 columns"
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html#data-exploration-and-illustration",
    "href": "posts/20230723-co2-growth/index.html#data-exploration-and-illustration",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "Data Exploration and Illustration",
    "text": "Data Exploration and Illustration\nThe first exploration of the data is to see how much change every country experiencing with over the course of 28 years from 1990-2018, and how the relationship between CO2 emission per Capita vs GDP per Capita look like.\nFigure¬†4 shows the time-lapse between years, annotated to some countries from low (e.g.¬†Ethiopia, Bangladesh), middle (e.g.¬†Indonesia, India) to high (Singapore, USA, UK, etc.) GDP per CO2 Ratio.\n\nThe position is relatively stable especially for high-income countries like US, UK, and Germany, but drastic change for low-middle income countries.\n\n\n\nCode\nimport plotly_express as px\n\nsource = co2\n\n#selected countries to annotate\nhighlighted_countries = ['United States', 'Germany', 'United Kingdom',\n                         'China', 'Singapore','Mexico', 'India', 'Indonesia', \n                         'Nigeria', 'Vietnam', 'Bangladesh', 'Ethiopia'\n                        ]\n\n# Create a new column for text values based on the condition\nsource['text_value'] = source['country'].apply(lambda country: country if country in highlighted_countries else '')\n\nfig = px.scatter(data_frame=source, \n           x=\"co2_per_capita\", \n           y=\"gdp_per_capita\", \n           animation_frame=\"year\", \n           animation_group=\"country\",\n           size=\"population\", \n           color=\"continent\", \n           hover_name=\"country\", \n           log_x = True, log_y=True,\n           size_max=80,\n           width=700,\n           height=800,\n           # text_baseline='bottom',\n           range_x=[0.01,100], \n           range_y=[400,90000],\n           text='text_value'\n          )\n\n\nfig.update_layout(\n    # title='CO2 Emission vs GDP of Countries',\n    xaxis_title='CO2 Emission per Capita (tonnes)',\n    yaxis_title='GDP per Capita (USD)',\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n    y=1.02,\n    xanchor=\"right\",\n    x=1\n)\n)\n\nfig.show()\n\n\n\n\n                                                \nFigure¬†4: 1990-2018 Time-lapse Chart of CO2 Emission and GDP per Country\n\n\n\n\nCountry with Decreased CO2 Emission & Increased GDP\nIf we recall some articles from OWID, where they pointed out some countries such as United Kingdom, where the economic growth still happening while at the same time, reducing the CO2 emission as shown in Figure¬†1. It sounds impressive, but from the Figure¬†5 below, it is quite clear on why some countries like United Kingdom, Germany or USA were able to decoupled their economy from their CO2 emissions.\n\nBecause they are already rich.\n\nIt is easy to ignore the fact that those countries were sitting on top of other countries in terms of income level, with GDP per Capita at the high-income class countries, consistently above 20,000 USD ever since 1990.\nThe narrative that a country can really keep increasing their GDP per Capita without producing more CO2 emission is really an oversimplification of the whole set of conditions that allow a country to do so.\n\n\nCode\nimport altair as alt\n\nhighlighted_countries = ['India', 'Indonesia', 'United Kingdom', 'Germany', 'United States']\n\nsource=co2[co2['country'].isin(highlighted_countries)]\n\nalt.Chart(\n    source,\n    title=alt.Title(\n        \"GDP per Capita vs CO2 Emission\",\n        subtitle=[\"Different CO2 vs GDP rate in different Countries\"],\n        anchor='middle',\n        offset=10, fontSize=16, \n    )\n    ).mark_point(size=90, filled=True, opacity=0.7).encode(\n    x=alt.X(\n        'co2_per_capita:Q', title='CO2 per Capita',\n        scale=alt.Scale(type=\"log\", domain=[0.3, 30])\n        ),\n    y=alt.Y(\n        'gdp_per_capita:Q', title='GDP per Capita',\n        scale=alt.Scale(type=\"log\")\n        ),\n    color=alt.Color('year', title='Year'),\n    shape=alt.Shape('country', title='Country'),\n    tooltip=['country', 'population', 'co2_per_capita', 'gdp_per_capita', 'year']\n).properties(\n    width='container',\n    height=400,\n).interactive()\n\n\n\n\n\n\n\nFigure¬†5: CO2 vs GDP per Capita for some countries with different trend\n\n\n\nA country needs to be rich first, at around 20,000 USD GDP per Capita before they can decouple their CO2 emission from GDP. To be rich, a country will have to use more energy, and likely to produce more CO2 emissions because of that. Even using a modest estimation of 5% GDP growth, and 2% inflation rate, it will take 7 years, before Indonesia can reach 20,000 USD per Capita level.\nAnother view is to see if there is country where the CO2 emission is reduced, but at the same time, their economy halted, and their GDP per Capite is reduced as well. To do that, let‚Äôs create a new sets of columns, of the difference in GDP per Capita per country.\n\n\nCode\n# create a column for gdp changes between 1990-2018\n\n# Pivot the DataFrame to have years as columns\ngdp_df = co2.pivot(index='country', columns='year', values='gdp_per_capita')\n# pivoted_df\n\n# Calculate the difference between GDP values for years 1980 and 2018\ngdp_df['gdp_diff'] = gdp_df[2018] - gdp_df[1990]\n\n# Reset the index to convert the DataFrame back to the original format\ngdp_df.reset_index(inplace=True)\n\n# Merge the calculated difference back to the original DataFrame\nmerged_df = pd.merge(co2, gdp_df[['country', 'gdp_diff']], on='country', how='left')\n\nmerged_df\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\nco2_per_capita\ngdp_per_capita\ncontinent\nincome_class\ntext_value\ngdp_diff\n\n\n\n\n0\nAlbania\n1990\n3295073.0\n1.675\n3929.457471\nEurope\nlower-middle\n\n7891.497683\n\n\n1\nAlbania\n1991\n3302087.0\n1.299\n2839.369269\nEurope\nlower-middle\n\n7891.497683\n\n\n2\nAlbania\n1992\n3303738.0\n0.762\n2647.207768\nEurope\nlower-middle\n\n7891.497683\n\n\n3\nAlbania\n1993\n3300715.0\n0.708\n2919.476286\nEurope\nlower-middle\n\n7891.497683\n\n\n4\nAlbania\n1994\n3294001.0\n0.584\n3216.795546\nEurope\nlower-middle\n\n7891.497683\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2786\nZimbabwe\n2014\n13855758.0\n0.862\n1531.673864\nAfrica\nlower-middle\n\n-659.894783\n\n\n2787\nZimbabwe\n2015\n14154937.0\n0.866\n1485.520916\nAfrica\nlower-middle\n\n-659.894783\n\n\n2788\nZimbabwe\n2016\n14452705.0\n0.729\n1450.371681\nAfrica\nlower-middle\n\n-659.894783\n\n\n2789\nZimbabwe\n2017\n14751101.0\n0.651\n1487.877848\nAfrica\nlower-middle\n\n-659.894783\n\n\n2790\nZimbabwe\n2018\n15052191.0\n0.784\n1509.106089\nAfrica\nlower-middle\n\n-659.894783\n\n\n\n\n2791 rows √ó 9 columns\n\n\n\n\nDo the same for CO2 Emissions between 1990-2018.\n\n\nCode\n# create a column for co2 changes between 1990-2018\n\n# Pivot the DataFrame to have years as columns\nco2_df = co2.pivot(index='country', columns='year', values='co2_per_capita')\n# pivoted_df\n\n# Calculate the difference between GDP values for years 1980 and 2018\nco2_df['co2_diff'] = co2_df[2018] - co2_df[1990]\n\n# Reset the index to convert the DataFrame back to the original format\nco2_df.reset_index(inplace=True)\n\n# Merge the calculated difference back to the original DataFrame\nmerged_df = pd.merge(merged_df, co2_df[['country', 'co2_diff']], on='country', how='left')\nmerged_df\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\nco2_per_capita\ngdp_per_capita\ncontinent\nincome_class\ntext_value\ngdp_diff\nco2_diff\n\n\n\n\n0\nAlbania\n1990\n3295073.0\n1.675\n3929.457471\nEurope\nlower-middle\n\n7891.497683\n0.057\n\n\n1\nAlbania\n1991\n3302087.0\n1.299\n2839.369269\nEurope\nlower-middle\n\n7891.497683\n0.057\n\n\n2\nAlbania\n1992\n3303738.0\n0.762\n2647.207768\nEurope\nlower-middle\n\n7891.497683\n0.057\n\n\n3\nAlbania\n1993\n3300715.0\n0.708\n2919.476286\nEurope\nlower-middle\n\n7891.497683\n0.057\n\n\n4\nAlbania\n1994\n3294001.0\n0.584\n3216.795546\nEurope\nlower-middle\n\n7891.497683\n0.057\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2786\nZimbabwe\n2014\n13855758.0\n0.862\n1531.673864\nAfrica\nlower-middle\n\n-659.894783\n-0.754\n\n\n2787\nZimbabwe\n2015\n14154937.0\n0.866\n1485.520916\nAfrica\nlower-middle\n\n-659.894783\n-0.754\n\n\n2788\nZimbabwe\n2016\n14452705.0\n0.729\n1450.371681\nAfrica\nlower-middle\n\n-659.894783\n-0.754\n\n\n2789\nZimbabwe\n2017\n14751101.0\n0.651\n1487.877848\nAfrica\nlower-middle\n\n-659.894783\n-0.754\n\n\n2790\nZimbabwe\n2018\n15052191.0\n0.784\n1509.106089\nAfrica\nlower-middle\n\n-659.894783\n-0.754\n\n\n\n\n2791 rows √ó 10 columns\n\n\n\n\nWith this new dataset, we can confirm our exploratory analysis before, what countries that can decoupled their CO2emission from economic growth (GDP)?\nFigure¬†6 shows a distribution of countries where the their GDP per Capita increases between 1990-2018, while their CO2 emissions were decreased. One interesting fact is all countries were sitting above upper-middle income class at around 4,500 USD GDP per Capita, and some are still on the increasing trend (GDP and CO2 emission increase) for the past 10 years e.g.¬†Columbia.\nAs shown, not every country created equally, as different country has different tipping point when they started to be on decreasing trend on CO2 emission while still increasing their GDP per Capita.\n\n\nCode\nimport altair as alt\n\n#country with increase gdp, but decreased co2\nsource=merged_df.query(\" gdp_diff&gt;0 & co2_diff&lt;0 & population &gt;= 2_000_000\")\n\nalt.Chart(\n    source,\n    title=alt.Title(\n        \"GDP per Capita vs CO2 Emission for Countries\",\n        subtitle=[\"Countries with minimum 2 Million population,\", \n                  \"increased GDP and reduced CO2 Emissions\"],\n        fontSize=16, offset=10\n    )\n    ).mark_point(size=90, filled=True, opacity=0.6).encode(\n    x=alt.X(\n        'co2_per_capita:Q', title='CO2 per Capita',\n        scale=alt.Scale(type=\"log\", domain=[0.5,50])\n        ),\n    y=alt.Y(\n        'gdp_per_capita:Q', title='GDP per Capita',\n        scale=alt.Scale(type=\"log\")\n        ),\n    color=alt.Color('country', title='Country'),\n    size=alt.Size('year:O', scale=alt.Scale(domain=[1990,1995,2000,2005,2010,2015,2020]), title='Year'),\n    tooltip=['country', 'population', 'co2_per_capita', 'gdp_per_capita', 'year']\n).properties(\n    width='container',\n    height=400,\n).interactive()\n\n\n\n\n\n\n\nFigure¬†6: Countries with low CO2 Emission and increase GDP (countries with minimum 2 Million population)\n\n\n\n\n\nCountry with Decreased CO2 Emission & Decreased GDP\nThis is part where we should expect some of the poorest country in the world, where their CO2 emission should not be halted because it is directly related to the their economic growth. Figure¬†8 shows country with worse condition in 2018 than they are in 1990, Zimbabwe. This country needs help, and if that means emitting more CO2 emission, so be it. In fact their carbon budget is still below their fair share, as shown in Figure¬†7, a night and day difference compared to UK carbon share3.\n\n\n\nFigure¬†7: Cumulative emission on UK and Zimbabwe compared to their respective Carbon Budget (Fanning and Hickel 2023)\n\n\n\n\nCode\nimport altair as alt\n\n#country with decreased CO2 and GDP\nsource=merged_df.query(\" gdp_diff&lt;0 & co2_diff&lt;0 \")\n\nalt.Chart(\n    source,\n    title=alt.Title(\n        \"GDP per Capita vs CO2 Emission for Countries\",\n        subtitle=[\"Countries with decreased GDP and CO2 Emission\"],\n        fontSize=16, offset=10\n    )\n    ).mark_point(size=90, filled=True, opacity=0.6).encode(\n    x=alt.X(\n        'co2_per_capita:Q', title='CO2 per Capita',\n        scale=alt.Scale(type=\"log\", domain=[0.5,50])\n        ),\n    y=alt.Y(\n        'gdp_per_capita:Q', title='GDP per Capita',\n        scale=alt.Scale(type=\"log\")\n        ),\n    color=alt.Color('country', title='Country'),\n    size=alt.Size('year:O', scale=alt.Scale(domain=[1990,1995,2000,2005,2010,2015,2020]), title='Year'),\n    tooltip=['country', 'population', 'co2_per_capita', 'gdp_per_capita', 'year']\n).properties(\n    width='container',\n    height=400,\n).interactive()\n\n\n\n\n\n\n\nFigure¬†8: Countries with low CO2 Emission and low GDP\n\n\n\n\n\n\nCountry with Increased CO2 Emission & Increased GDP\nThis is probably the category where everyone is trying to judge on, country that is trying to grow their economy and in doing so- increase their CO2 emission. As shown inf Figure¬†9, these countries are all on the increasing trend, their GDP per Capita is increasing but at the same time they were emitting CO2, which is understandable.\n\n\nCode\nimport altair as alt\n\n#country with increased CO2 and GDP\nsource=merged_df.query(\" gdp_diff&gt;0 & co2_diff&gt;0 \")\n\nalt.Chart(\n    source,\n    title=alt.Title(\n        \"GDP per Capita vs CO2 Emission for Countries\",\n        subtitle=[\"Countries with Increased GDP and CO2 Emission\"],\n        fontSize=16, offset=10\n    )\n    ).mark_point(size=90, filled=True, opacity=0.6).encode(\n    x=alt.X(\n        'co2_per_capita:Q', title='CO2 per Capita',\n        scale=alt.Scale(type=\"log\",)\n        ),\n    y=alt.Y(\n        'gdp_per_capita:Q', title='GDP per Capita',\n        scale=alt.Scale(type=\"log\")\n        ),\n    color=alt.Color('country', title='Country'),\n    size=alt.Size('year:O', scale=alt.Scale(domain=[1990,1995,2000,2005,2010,2015,2020]), title='Year'),\n    tooltip=['country', 'population', 'co2_per_capita', 'gdp_per_capita', 'year']\n).properties(\n    width='container',\n    height=500,\n).interactive()\n\n\n\n\n\n\n\nFigure¬†9: Countries with increased CO2 Emission and GDP\n\n\n\n\nIf we are looking at some countries such as Indonesia, Vietnam, India, it is clear that these countries are trying to grow their economy from below 4,000 USD GDP per Capita in 1990, to hopefully near 20,000 USD GDP per Capita (just like any other rich countries in Figure¬†5), in the foreseeable future.\n\n\nCode\nimport altair as alt\n\n#country with increased CO2 and GDP\nsource=merged_df.query(\" gdp_diff&gt;0 & co2_diff&gt;0 & population &gt; 50_000_000 \")\n\nalt.Chart(\n    source,\n    title=alt.Title(\n        \"GDP per Capita vs CO2 Emission for Countries\",\n        subtitle=[\"Countries with Increased GDP and CO2 Emission\", \" with population more than 50 Million \"],\n        fontSize=16, offset=10\n    )\n    ).mark_point(size=90, filled=True, opacity=0.6).encode(\n    x=alt.X(\n        'co2_per_capita:Q', title='CO2 per Capita',\n        scale=alt.Scale(type=\"log\",)\n        ),\n    y=alt.Y(\n        'gdp_per_capita:Q', title='GDP per Capita',\n        scale=alt.Scale(type=\"log\")\n        ),\n    color=alt.Color('country', title='Country'),\n    size=alt.Size('year:O', scale=alt.Scale(domain=[1990,1995,2000,2005,2010,2015,2020]), title='Year'),\n    tooltip=['country', 'population', 'co2_per_capita', 'gdp_per_capita', 'year']\n).properties(\n    width='container',\n    height=400,\n).interactive()\n\n\n\n\n\n\n\nFigure¬†10: Countries with increased CO2 Emission and GDP\n\n\n\n\nOn the one hand, some developed countries, the one that were referred as an example for a country that successfully decoupled their CO2 emission from their economic growth are proved to be still overemitting. In fact, the top 5 countries such as United States, Germany and United Kingdom are still in the excess shares with respect to the net-zero scenario (Fanning and Hickel 2023).\nOn the other hand, some developing countries like India, Indonesia, Pakistan, and even China still have their carbon share to be used, as they are sacrificing their fair shares with respect to the net-zero scenario. See Figure¬†11 for more details.\n\n\n\nFigure¬†11: Top 5 Overemitting and Underemitting Countries with respect to Net Zero scenario (Fanning and Hickel 2023)"
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html#closing-words",
    "href": "posts/20230723-co2-growth/index.html#closing-words",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "Closing Words",
    "text": "Closing Words\nSome developed countries are at a better position to afford moving towards decreasing CO2 emission while still flourishing in their economic growth. Some developing countries are still trying to go to the level of those developed countries, while at the same time emitting CO2 emission, which in some conditions could still be within their carbon budget (Figure¬†12 and Figure¬†7) to do so.\n\n\n\nFigure¬†12: Indonesia Carbon Budget vs The Cummulative Emission (Fanning and Hickel 2023)"
  },
  {
    "objectID": "posts/20230723-co2-growth/index.html#footnotes",
    "href": "posts/20230723-co2-growth/index.html#footnotes",
    "title": "Decoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGapminder is an independent educational non-proÔ¨Åt Ô¨Åghting global misconceptions. Complete website is available here‚Ü©Ô∏é\nThe low income countries is classified as country with income about 1,000 USD or lower, middle is divided to two: lower-middle income is roughly between 1,000 USD - 4,000 USD, upper middle income is between 4,000 USD and 13,000 USD, and high income countries is any country with more than 13,000 USD income.‚Ü©Ô∏é\nWebsite to display the cumulative emission with respect to their fair shares on carbon budgets. Link can be found here‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html",
    "href": "posts/007-quarto-python-tutorial/index.html",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "",
    "text": "Photo by Glenn Carstens-Peters on Unsplash"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#pay-gate-wall",
    "href": "posts/007-quarto-python-tutorial/index.html#pay-gate-wall",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Pay Gate Wall",
    "text": "Pay Gate Wall\nIt is good if you are getting paid for what you were sharing on, but if your main purpose is for branding yourself (like me), then a pay gate wall would not do you any good (Figure¬†1). Especially, when you have just started on. Though it is fairly cheap, it is just another stopper for your first audience. Worse, the paid-subscription is only available for certain countries, Indonesia for example is not supported.\n\n\nFigure¬†1: Pay Gate in medium.com"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#reproducibility",
    "href": "posts/007-quarto-python-tutorial/index.html#reproducibility",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Reproducibility",
    "text": "Reproducibility\nWhen I do my first data-science project, I wanted to make sure that someone can just follow along and then get the same result as I was. Using platform like medium, I could not really do that, and I believe it was not meant for tutorial in details. Rather it was for an explanatory writing, rather than exploratory writing. Quarto with its rendered codes as an output, makes it easier to do just that. The below code cell is me importing libraries, importing a dataset, and displaying the table.\n\n\nCode\n#importing libraries\nimport pandas as pd\nimport hvplot.pandas\nfrom bokeh.sampledata.penguins import data as penguins\n\n#displaying first 5 rows\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\nWhat if I want to show the last 6 rows? I can just change the code.\n\n\nCode\n#importing libraries\nimport pandas as pd\nimport hvplot.pandas\nfrom bokeh.sampledata.penguins import data as penguins\n\n#displaying first 5 rows\npenguins.tail(6)\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n338\nGentoo\nBiscoe\n47.2\n13.7\n214.0\n4925.0\nFEMALE\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMALE"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#contents-ownership",
    "href": "posts/007-quarto-python-tutorial/index.html#contents-ownership",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "3. Contents Ownership",
    "text": "3. Contents Ownership\nIf one day the platform (medium) goes bankrupt, I have no worry because I kept all my posts in a github, and my local folder. If the domain is somehow being hacked, I can just changed it. I own the content, and nobody else.\nThis gives you not only a total control, but also a freedom to experiment to your heart desires.\n\n\n\nvia GIPHY\n\n\nOf course there are benefits of using low-code platform, but for my use case, I much prefer to have my own personal website."
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#creating-a-repo",
    "href": "posts/007-quarto-python-tutorial/index.html#creating-a-repo",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Creating a Repo",
    "text": "Creating a Repo\nMake a new repository with the same name as your github account. For example, if your github account is johndoe, then make a new repository named john.github.io (see Figure¬†2).\n\n\nFigure¬†2: Creating a New Repo\n\n\n\nWhat‚Äôs cool about this repo is it will be accessible later on as a domain for your personal website, completely free, and secure (https). This of course can be customized with a domain of your choice, but for now, we want to make this as easy as possible."
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#starting-from-a-notebook",
    "href": "posts/007-quarto-python-tutorial/index.html#starting-from-a-notebook",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Starting from a Notebook",
    "text": "Starting from a Notebook\nLet‚Äôs make a content, using jupyter notebook, with a combination of code cells and markdown cells. I am gonna use the content in this post, where I load a libraries, dataset from palmerpenguins, and then I will do some plotting using altair to spice things up, cause why not. See below Figure¬†3 for example.\n\n\nCode\n#importing libraries\nimport altair as alt\nfrom bokeh.sampledata.penguins import data as penguins\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#plotting\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(\n  data=penguins, \n  title=\"Palmer Penguins Dataset\",\n  ).mark_circle(size=60).encode(\n  alt.X('bill_length_mm', scale=alt.Scale(domain=[30,60])),\n  alt.Y('bill_depth_mm', scale=alt.Scale(domain=[12,22])),\n  color='species',\n  ).add_selection(\n    brush\n)\n\nbars = alt.Chart(penguins).mark_bar().encode(\n    y='island',\n    color='island',\n    x='count(island)'\n).transform_filter(\n    brush\n)\n\npoints & bars\n\n\n\n\n\n\n\n\n\nFigure¬†3: Example Notebook"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#create-a-project",
    "href": "posts/007-quarto-python-tutorial/index.html#create-a-project",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Create a Project",
    "text": "Create a Project\nThe first step is to create a set of folders for website project, using quarto by executing the following command inside the project folder:\nmkdir posts\nquarto create-project . --type website\nwhich will output the following Figure¬†4, essentially the post folder would be where we keep our blogpost files.\n\n\nFigure¬†4: Output Projects"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#convert-notebook-to-quarto-markdown",
    "href": "posts/007-quarto-python-tutorial/index.html#convert-notebook-to-quarto-markdown",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Convert Notebook to Quarto Markdown",
    "text": "Convert Notebook to Quarto Markdown\nThe second step is to convert the jupyter notebook file (example.ipynb) to quarto markdown file (example.qmd) by running the following command:\nquarto convert ./example.ipynb\nwhich will create an output of qmd file which is important, because this would then be rendered to html (as shown in Figure¬†5) by running the following command:\nquarto render\n\n\nFigure¬†5: Output Render"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#setting-up-the-website",
    "href": "posts/007-quarto-python-tutorial/index.html#setting-up-the-website",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Setting-up the Website",
    "text": "Setting-up the Website\nYAML file is basically where we fine tune our website settings. For this case, we need to move the example.qmd file to post folder, and add the qmd file inside the navbar, below the about.qmd as shown in Figure¬†6\n\n\nFigure¬†6: YAML setting"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#previewing-the-website",
    "href": "posts/007-quarto-python-tutorial/index.html#previewing-the-website",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Previewing the Website",
    "text": "Previewing the Website\nAll things set, now all we need to do is preview our website by running the following command:\nquarto preview\nwhich will output our new website inside a browser as shown in the following Figure¬†7. This is a preview mode which means, any changes made to the website, it will be rendered and displayed real-time.\n\n\nFigure¬†7: Website Preview\n\n\n\nVoila! You got it done! Congrats!"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#deploying-the-website",
    "href": "posts/007-quarto-python-tutorial/index.html#deploying-the-website",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Deploying the Website",
    "text": "Deploying the Website\nThere is a documentation on Quarto to do this, but in layman terms, there are essentially three things to do:\n\nCreate a docs folder and set the output to be that folder in _quarto.yml file as shown in Figure¬†8 below\n\n\n\nFigure¬†8: docs settting\n\n\n\n\nSet the website repo setting to use the docs folder as the branch source as shown in Figure¬†9 below\n\n\n\nFigure¬†9: repo settting\n\n\n\n\nPush your local Repo to the Github!\n\ngit add .\ngit commit -m \"my first website\"\ngit push\nand you should see your website is up and running after the builds and deployment as shown in Figure¬†10 finished!\n\n\nFigure¬†10: Builds Up and Deployment\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSince this is a static pages, we will have to run quarto-render everytime before pushing it to the github. Otherwise, it may appear in our preview-mode, but will not show up in the website online."
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#make-a-homepage",
    "href": "posts/007-quarto-python-tutorial/index.html#make-a-homepage",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "1. Make A Homepage",
    "text": "1. Make A Homepage\nIf you have your website deployed, congrats! However, if you stay, I can assure you that it will make your website much better.\nSome nicer things to do for your website is to render the index.qmd in the root folder as a listing, so any post in your post folder will be listed as a content, as shown in Figure¬†11 below:\n\n\nFigure¬†11: Setting Up the Website\n\n\n\nHere is what I did to the initial website:\n\nMake a folder inside posts, create a 001-first-post folder, and change the filename to index.qmd\nEdit the index.qmd in the root folder to list the contents\nEdit the _quarto.yml file back to its original setting.\n\nWith this setting, you will have a nicer homepage like Figure¬†12 below:\n\n\nFigure¬†12: Homepage"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#use-quarto-in-vscode",
    "href": "posts/007-quarto-python-tutorial/index.html#use-quarto-in-vscode",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "2. Use Quarto in VSCode",
    "text": "2. Use Quarto in VSCode\nAlthough so far we can get by just by using jupyter notebook, we were missing out (big time) in quarto extension capabilities. Autocompletion, autosuggestions, which are available in VSCode but not the jupyter notebook.\nUsing VSCode and Quarto, we can easily shift between visual-mode and source-mode, for a nicer GUI. You can just right-click and select edit in visual-mode and it will automatically brings you to a GUI version of the markdown.\n\nFrom there, we can change the heading-style, bold, italic, add numbers, lists, picture, callnote, a very powerful GUI for markdown! Loved it!\n\nHere is the comparison side-by-side in Figure¬†13:\n\n\nFigure¬†13: Source-mode vs Visual-mode\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the nicest things that I used A LOT is the ability to just copy-paste an image from clipboard to a qmd file via visual-mode, and it will automatically create a folder named images, save the image we pasted into the folder, and displayed it (by referencing) into the qmd file. All happen instaneously!"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#craving-for-more",
    "href": "posts/007-quarto-python-tutorial/index.html#craving-for-more",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "3. Craving for More?",
    "text": "3. Craving for More?\nThere is a good chance that this blogpost would not be enough for you after you add some more contents, and wanted to change some views/ aesthetics of the websites. My suggestion has always been to the excellent Quarto documentations for website, Mickael make a repo dedicated just for Quarto (extensions, slides, tutorial and alike) make sure to check his repo, follow him also in twitter, as I always find the latest new things from him from there like this one:\n\n\nü§ñ From #AwesomeQuarto: 'How to add some personality to your Quarto Blog' (https://t.co/nK3j7rPty1)A blog post sharing some of the added features and tweaks users can make on top of the standard blog templates to inject some personality into their blog.#QuartoPub\n\n‚Äî Micka√´l CANOUIL (@MickaelCanouil@fosstodon.org) (@MickaelCanouil) March 19, 2023"
  },
  {
    "objectID": "posts/007-quarto-python-tutorial/index.html#footnotes",
    "href": "posts/007-quarto-python-tutorial/index.html#footnotes",
    "title": "Personal Website using Jupyter Notebook and Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nyou can just use a notebook, but some of the autocompletion, visual-mode (a powerful mode to make blogging easier) won‚Äôt work.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/20230715-plotpetrophysics/index.html",
    "href": "posts/20230715-plotpetrophysics/index.html",
    "title": "Plotpetrophysics",
    "section": "",
    "text": "Welcome Page\n\n\nThis app was built to answer the need for a quicklook petrophysical analysis for well-log data. So far, author found no web application that was available for FREE to students and enthusiast-alike.\nLike of the website can be found here plotpetrophysics\n\n\n\n\nIf you don‚Äôt have any other dataset to play with, you can use the preloaded file to see what the app can offer. Otherwise, feel free to load your own LAS file data to it (just make sure it is LAS 2.0).\n\n\n\nFile Selection\n\n\n\n\n\nInformation about the well, like a well-name, the top and bottom depth, company, etc. can be displayed.\n\n\n\nWell Information\n\n\n\n\n\nAll the curves available in the data will be displayed as a bar chart from top to bottom of the log (scaled by number of rows depending on the depth step), any missing data would be displayed here as shorter than others.\n\n\n\n\n The plot can be adjusted based on depths, scales, number of grids, and the shadings between curves (plot settings on the left)\n\n\n\nOne can also turned-on Formaton Evaluation mode and then displayed as shale volume, bulk volume of water, and water saturation as follows.\n\n\n\nCurves Overview\n\n\n\n\n\n\nYou can download all plot as PDF files\nYou can download final result as CSV\nThere is histogram view of a curve\nThere is a scatter-plot view of the curves"
  },
  {
    "objectID": "posts/20230715-plotpetrophysics/index.html#features-are-as-follows",
    "href": "posts/20230715-plotpetrophysics/index.html#features-are-as-follows",
    "title": "Plotpetrophysics",
    "section": "",
    "text": "If you don‚Äôt have any other dataset to play with, you can use the preloaded file to see what the app can offer. Otherwise, feel free to load your own LAS file data to it (just make sure it is LAS 2.0).\n\n\n\nFile Selection\n\n\n\n\n\nInformation about the well, like a well-name, the top and bottom depth, company, etc. can be displayed.\n\n\n\nWell Information\n\n\n\n\n\nAll the curves available in the data will be displayed as a bar chart from top to bottom of the log (scaled by number of rows depending on the depth step), any missing data would be displayed here as shorter than others.\n\n\n\n\n The plot can be adjusted based on depths, scales, number of grids, and the shadings between curves (plot settings on the left)\n\n\n\nOne can also turned-on Formaton Evaluation mode and then displayed as shale volume, bulk volume of water, and water saturation as follows.\n\n\n\nCurves Overview\n\n\n\n\n\n\nYou can download all plot as PDF files\nYou can download final result as CSV\nThere is histogram view of a curve\nThere is a scatter-plot view of the curves"
  },
  {
    "objectID": "posts/004-national-exam/index.html",
    "href": "posts/004-national-exam/index.html",
    "title": "Webscraping National Exams of Indonesia",
    "section": "",
    "text": "Photo by Ed Us on Unsplash"
  },
  {
    "objectID": "posts/004-national-exam/index.html#the-role-of-national-exam",
    "href": "posts/004-national-exam/index.html#the-role-of-national-exam",
    "title": "Webscraping National Exams of Indonesia",
    "section": "The Role of National Exam",
    "text": "The Role of National Exam\nFor the longest time, national exam, like many other countries in the world is used as one of the metric to measure how well our future would be (re: children). The concept is simple, each and every children in the country will be given a set of questions/ tasks, that has to be completed for a given time, on different subjects (likely to be Math, Language, Science).\n\n\nIndonesia Minister of Education\n\n\n\nIn Indonesia, this was used to be the sole metric to say if one student passed the exam or not. This has been changed ever since the newly appointed education minister of Indonesia, Nadiem Makarim (also the co-founder of SE Asia Decacorn- Gojek, Now GoTo). Nevertheless, it is interesting to see how this metric pan out over the course of 2015-2019, to see if the decentralized quality of education still persist in Java, Bali and its surrounding only. Leaving out Kalimantan, Sulawesi, Sumatra, NTT, and Papua."
  },
  {
    "objectID": "posts/004-national-exam/index.html#data-gathering",
    "href": "posts/004-national-exam/index.html#data-gathering",
    "title": "Webscraping National Exams of Indonesia",
    "section": "Data Gathering",
    "text": "Data Gathering\n\nWebscraping using Pandas\nWe‚Äôre taking a look on the data from Ministry of Education (Kemendikbud), from 2019 on national exams as follows.\nFor this use we will need to scrape the data from the website, but the catch is‚Äìwe are using just pandas library!\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nEnter pandas.read_html()\n\n\nCode\nun_2019= \\\n(pd\n .read_html('https://npd.kemdikbud.go.id/?appid=hasilun&tahun=2019')\n [1][0:547]\n)\n\nun_2019.head()\n\n\n\n\n\n\n\n\n\nNo.\nKode Wilayah\nNama Wilayah\nPropinsi\nPersentase Anggaran Pendidikan tanpa Transfer Daerah (%)\nUN SMP\nUN SMA IPA\nUN SMA IPS\nUN SMA BAHASA\nUN SMK\nIIUN SMP\nIIUN SMA IPA\nIIUN SMA IPS\nIIUN SMA BAHASA\nIIUN SMK\n\n\n\n\n0\n1\n010000\nProv. D.K.I. Jakarta\nProv. D.K.I. Jakarta\n18.96\n62.17\n67.99\n63.75\n0\n51.41\n100\n100\n100\n0\n100\n\n\n1\n2\n010100\nKab. Kepulauan Seribu\nProv. D.K.I. Jakarta\n0.00\n45.26\n0\n0\n0\n0\n100\n0\n0\n0\n0\n\n\n2\n3\n016000\nKota Jakarta Pusat\nProv. D.K.I. Jakarta\n0.00\n60.97\n69.43\n64.01\n0\n55.13\n100\n100\n100\n0\n100\n\n\n3\n4\n016100\nKota Jakarta Utara\nProv. D.K.I. Jakarta\n0.00\n60.09\n66.59\n61.53\n0\n48.85\n100\n100\n100\n0\n100\n\n\n4\n5\n016200\nKota Jakarta Barat\nProv. D.K.I. Jakarta\n0.00\n60.42\n69.31\n63.85\n0\n49.65\n100\n100\n100\n0\n100\n\n\n\n\n\n\n\nLet‚Äôs see what features do we have, and which one do we really care about in this project.\n\n\nCode\nun_2019.columns\n\n\nIndex(['No.', 'Kode Wilayah', 'Nama Wilayah', 'Propinsi',\n       'Persentase Anggaran Pendidikan tanpa Transfer Daerah (%)', 'UN SMP',\n       'UN SMA IPA', 'UN SMA IPS', 'UN SMA BAHASA', 'UN SMK', 'IIUN SMP',\n       'IIUN SMA IPA', 'IIUN SMA IPS', 'IIUN SMA BAHASA', 'IIUN SMK'],\n      dtype='object')\n\n\nWe will leave some columns and just using region information (‚ÄòNama Wilayah‚Äô, ‚ÄòPropinsi‚Äô), and national exams score (‚ÄòUN SMP‚Äô,‚ÄòUN SMA IPA‚Äô, ‚ÄòUN SMA IPS‚Äô)\n\n\nCode\nun_2019 = un_2019[['Nama Wilayah', 'Propinsi', 'UN SMP','UN SMA IPA', 'UN SMA IPS']]\nun_2019.head()\n\n\n\n\n\n\n\n\n\nNama Wilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\n\n\n\n\n0\nProv. D.K.I. Jakarta\nProv. D.K.I. Jakarta\n62.17\n67.99\n63.75\n\n\n1\nKab. Kepulauan Seribu\nProv. D.K.I. Jakarta\n45.26\n0\n0\n\n\n2\nKota Jakarta Pusat\nProv. D.K.I. Jakarta\n60.97\n69.43\n64.01\n\n\n3\nKota Jakarta Utara\nProv. D.K.I. Jakarta\n60.09\n66.59\n61.53\n\n\n4\nKota Jakarta Barat\nProv. D.K.I. Jakarta\n60.42\n69.31\n63.85\n\n\n\n\n\n\n\nLet‚Äôs also check if the dtypes is already correct e.g.¬†digit would be float64 or int64 dtype.\n\n\nCode\nun_2019.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 547 entries, 0 to 546\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Nama Wilayah  547 non-null    object\n 1   Propinsi      547 non-null    object\n 2   UN SMP        547 non-null    object\n 3   UN SMA IPA    547 non-null    object\n 4   UN SMA IPS    547 non-null    object\ndtypes: object(5)\nmemory usage: 21.5+ KB\n\n\nLet‚Äôs make some amendments on dtypes, to allow plotting.\n\n\nCode\nun_2019 = \\\nun_2019.astype({\n    'UN SMP': float,\n    'UN SMA IPA': float,\n    'UN SMA IPS': float,\n})\nun_2019.dtypes\n\n\nNama Wilayah     object\nPropinsi         object\nUN SMP          float64\nUN SMA IPA      float64\nUN SMA IPS      float64\ndtype: object\n\n\nJust me, but I dont like redundant information, so let‚Äôs remove some of the words in the column.\n\n\nCode\nun_2019 = un_2019.rename(columns={\n    'Nama Wilayah': 'Wilayah'})\nun_2019.head()\n\n\n\n\n\n\n\n\n\nWilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\n\n\n\n\n0\nProv. D.K.I. Jakarta\nProv. D.K.I. Jakarta\n62.17\n67.99\n63.75\n\n\n1\nKab. Kepulauan Seribu\nProv. D.K.I. Jakarta\n45.26\n0.00\n0.00\n\n\n2\nKota Jakarta Pusat\nProv. D.K.I. Jakarta\n60.97\n69.43\n64.01\n\n\n3\nKota Jakarta Utara\nProv. D.K.I. Jakarta\n60.09\n66.59\n61.53\n\n\n4\nKota Jakarta Barat\nProv. D.K.I. Jakarta\n60.42\n69.31\n63.85\n\n\n\n\n\n\n\nSimilarly, let‚Äôs remove ‚ÄòProv‚Äô from each data in ‚ÄòPropinsi‚Äô column.\n\n\nCode\nun_2019['Propinsi'] = \\\nun_2019['Propinsi'].str.replace('Prov. ', '', regex=False)\n\nun_2019.head()\n\n\n\n\n\n\n\n\n\nWilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\n\n\n\n\n0\nProv. D.K.I. Jakarta\nD.K.I. Jakarta\n62.17\n67.99\n63.75\n\n\n1\nKab. Kepulauan Seribu\nD.K.I. Jakarta\n45.26\n0.00\n0.00\n\n\n2\nKota Jakarta Pusat\nD.K.I. Jakarta\n60.97\n69.43\n64.01\n\n\n3\nKota Jakarta Utara\nD.K.I. Jakarta\n60.09\n66.59\n61.53\n\n\n4\nKota Jakarta Barat\nD.K.I. Jakarta\n60.42\n69.31\n63.85\n\n\n\n\n\n\n\n\n\nQuick DataViz‚Ä¶\nLet‚Äôs plot senior high school (SMA) data:\n\n\nCode\n#making the barchart\nplt.style.use('ggplot')\n\n\n(un_2019.groupby('Propinsi')\n [[\n     # 'UN SMP',\n     'UN SMA IPA', \n     'UN SMA IPS'\n ]]\n .median()\n .sort_values(\n     by='UN SMA IPA',\n     ascending=True,\n )\n .plot(\n     kind='barh',\n     # ax=axs[0],\n     figsize=(6,10),\n ));\n\n\n\n\n\nSo, from the look of it, 2019 seems to be the year of our capital city: D.K.I. Jakarta, followed by Yogyakarta, Jawa Tengah, Timur, Bali etc. Excluding Bangka Belitung, the big five is all from Java and Bali.\nDecentralized? From 2019- YES.\nBut of course this won‚Äôt do it justice, we need more data right?\n\n\nScrape More Data\nLet‚Äôs all be human, not abusive towards our notebook, and create a function to scrape our data-change the dtypes-change the column name etc.\n\n\nCode\ndef webscrap(tahun):\n    df = pd.read_html(f'https://npd.kemdikbud.go.id/?appid=hasilun&tahun={tahun}')[1][0:547] #ambil data dari web\n    df = df[['Nama Wilayah', 'Propinsi', 'UN SMP','UN SMA IPA', 'UN SMA IPS']] #extract kolom\n    df = df.astype({'UN SMP': float, 'UN SMA IPA': float, 'UN SMA IPS': float}) #ubah dtypes\n    df = df.rename(columns={'Nama Wilayah': 'Wilayah'}) #rename kolom 'Nama Wilayah' jadi 'Wilayah'\n    df['Propinsi'] = df['Propinsi'].str.replace('Prov. ', '', regex=False) #menghapus 'Prov.' \n    df['Tahun'] = tahun #menambahkan kolom tahun\n    return df\n\n\nThen let‚Äôs scrap all years data from 2015-2019.\n\n\nCode\nun_2015 = webscrap(2015)\nun_2016 = webscrap(2016)\nun_2017 = webscrap(2017)\nun_2018 = webscrap(2018)\nun_2019 = webscrap(2019)\n\n\nLet‚Äôs do quick sanity check:\n\n\nCode\nun_2015.head()\n\n\n\n\n\n\n\n\n\nWilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\nTahun\n\n\n\n\n0\nProv. D.K.I. Jakarta\nD.K.I. Jakarta\n73.96\n72.99\n66.05\n2015\n\n\n1\nKab. Kepulauan Seribu\nD.K.I. Jakarta\n62.80\n0.00\n0.00\n2015\n\n\n2\nKota Jakarta Pusat\nD.K.I. Jakarta\n74.02\n72.82\n63.99\n2015\n\n\n3\nKota Jakarta Utara\nD.K.I. Jakarta\n71.22\n69.54\n63.31\n2015\n\n\n4\nKota Jakarta Barat\nD.K.I. Jakarta\n71.86\n71.79\n63.71\n2015\n\n\n\n\n\n\n\n\n\nCode\nun_2019.head()\n\n\n\n\n\n\n\n\n\nWilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\nTahun\n\n\n\n\n0\nProv. D.K.I. Jakarta\nD.K.I. Jakarta\n62.17\n67.99\n63.75\n2019\n\n\n1\nKab. Kepulauan Seribu\nD.K.I. Jakarta\n45.26\n0.00\n0.00\n2019\n\n\n2\nKota Jakarta Pusat\nD.K.I. Jakarta\n60.97\n69.43\n64.01\n2019\n\n\n3\nKota Jakarta Utara\nD.K.I. Jakarta\n60.09\n66.59\n61.53\n2019\n\n\n4\nKota Jakarta Barat\nD.K.I. Jakarta\n60.42\n69.31\n63.85\n2019\n\n\n\n\n\n\n\nEven better, let‚Äôs combine all of the data into one big data:\n\n\nCode\n#merge data\nlist_df = [un_2015, un_2016, un_2017, un_2018, un_2019]\nun_2015_2019 = pd.concat(list_df, axis='rows')\n\n#sanity check\nun_2015_2019.Tahun.value_counts()\n\n\n2015    547\n2016    547\n2017    547\n2018    547\n2019    547\nName: Tahun, dtype: int64\n\n\nData looks good, but just to be safe, let‚Äôs save it before we do anything stupid.\n\n\nCode\nun_2015_2019.to_csv(\"un_2015_2019.csv\", index=False)\n\n\n\n\nMore DataViz‚Ä¶\nLet‚Äôs see which 10 regions have the highest mean national exams.\n\n\nCode\norder_new = \\\n(un_2019\n [['Propinsi', 'UN SMA IPA']]\n .groupby(\"Propinsi\")\n .mean()\n .sort_values(by='UN SMA IPA', ascending=False)\n .reset_index()\n)\n\norder_new.head(10)\n\n\n\n\n\n\n\n\n\nPropinsi\nUN SMA IPA\n\n\n\n\n0\nD.I. Yogyakarta\n65.650000\n\n\n1\nJawa Tengah\n61.510000\n\n\n2\nD.K.I. Jakarta\n58.410000\n\n\n3\nJawa Timur\n57.377949\n\n\n4\nBali\n55.901000\n\n\n5\nBangka Belitung\n55.407500\n\n\n6\nSumatera Barat\n54.978500\n\n\n7\nKalimantan Timur\n54.211818\n\n\n8\nJawa Barat\n53.813929\n\n\n9\nBanten\n53.695556\n\n\n\n\n\n\n\nLet‚Äôs do some plotting for all years.\n\n\nCode\nfig, axs = plt.subplots(figsize=(10,12))\n\n(sns\n .barplot(\n     data=un_2015_2019.query(\" Tahun&gt;2014 \"), \n     x='UN SMA IPA', \n     y='Propinsi', \n     hue='Tahun',\n     ax=axs, \n     errorbar=('ci', False),\n     palette='viridis_r',\n     order=order_new['Propinsi']\n )\n);\n\n\n\n\n\nFrom the above figure, it looks like the Bali, D.I. Yogyakarta, similar to 2019 dataset is indeed appearing at the top-5.\nBut, let‚Äôs exclude year 2015 and 2016, as I think it is a bit an outlier in terms of value here.\n\n\nCode\nfig, axs = plt.subplots(figsize=(10,12))\n\n(sns\n .barplot(\n     data=un_2015_2019.query(\" Tahun&gt;2016 \"), \n     x='UN SMA IPA', \n     y='Propinsi', \n     hue='Tahun',\n     ax=axs, \n     errorbar=('ci', False),\n     palette='viridis_r',\n     order=order_new['Propinsi']\n )\n);\n\n\n\n\n\nJust by looking at it, the big-five are from Java and Bali. Not until we reach rank 6 and below we see Bangka Belitung, Sumatera, and Kalimantan. Region outside Java and Bali.\nNow let‚Äôs see if our thinking is right, by getting a top region on each national exams from junior to senior high schools.\n\n\nCode\ndef get_max_prop(df):\n    max_un_smp = df.groupby(['Propinsi'])['UN SMP'].mean().nlargest(1).index[0]\n    max_un_sma_ipa = df.groupby(['Propinsi'])['UN SMA IPA'].mean().nlargest(1).index[0]\n    max_un_sma_ips = df.groupby(['Propinsi'])['UN SMA IPS'].mean().nlargest(1).index[0]\n    return [max_un_smp, max_un_sma_ipa, max_un_sma_ips]\n\n\nLet‚Äôs do a loop over years;\n\n\nCode\nyears = [un_2015, un_2016, un_2017, un_2018, un_2019]\nfor year in years:\n    print(get_max_prop(year))\n\n\n['D.K.I. Jakarta', 'Bali', 'Bali']\n['D.I. Yogyakarta', 'Bali', 'Sumatera Utara']\n['D.I. Yogyakarta', 'Sulawesi Tenggara', 'D.I. Yogyakarta']\n['D.I. Yogyakarta', 'D.I. Yogyakarta', 'D.I. Yogyakarta']\n['D.I. Yogyakarta', 'D.I. Yogyakarta', 'D.I. Yogyakarta']\n\n\nFrom the above result, it was clear that excluding Sulawesi Tenggara, all of provinces here is coming from Java and Bali. Most notably D.I. Yogyakarta, D.K.I. Jakarta and Bali."
  },
  {
    "objectID": "posts/004-national-exam/index.html#lets-talk-about-kalimantan-timur",
    "href": "posts/004-national-exam/index.html#lets-talk-about-kalimantan-timur",
    "title": "Webscraping National Exams of Indonesia",
    "section": "Let‚Äôs Talk about Kalimantan Timur",
    "text": "Let‚Äôs Talk about Kalimantan Timur\nWhy? Cause I was raised in Tenggarong, Kutai Kartanegara region, Kalimantan Timur Province. I spent my senior high school here, so I have my own thinking here.\nMy theory is even if Kalimantan Timur was up there at rank 8, it was not from Tenggarong.\n\n\nCode\n(un_2015_2019.query(\"Propinsi == 'Kalimantan Timur'\")\n .sort_values(by='UN SMA IPA', ascending=False, ignore_index=True)\n .head(10)\n\n)\n\n\n\n\n\n\n\n\n\nWilayah\nPropinsi\nUN SMP\nUN SMA IPA\nUN SMA IPS\nTahun\n\n\n\n\n0\nKab. Kutai Barat\nKalimantan Timur\n67.48\n67.03\n66.30\n2015\n\n\n1\nKota Bontang\nKalimantan Timur\n61.46\n66.26\n58.32\n2019\n\n\n2\nKota Bontang\nKalimantan Timur\n57.47\n63.93\n61.97\n2017\n\n\n3\nKota Bontang\nKalimantan Timur\n58.06\n63.40\n58.75\n2018\n\n\n4\nKab. Kutai Kartanegara\nKalimantan Timur\n55.74\n62.27\n61.03\n2015\n\n\n5\nKota Balikpapan\nKalimantan Timur\n59.49\n61.71\n53.79\n2019\n\n\n6\nKota Balikpapan\nKalimantan Timur\n57.87\n61.30\n51.25\n2015\n\n\n7\nKota Bontang\nKalimantan Timur\n60.51\n61.16\n56.19\n2016\n\n\n8\nKota Balikpapan\nKalimantan Timur\n55.50\n60.94\n53.81\n2017\n\n\n9\nKota Balikpapan\nKalimantan Timur\n56.65\n60.51\n51.45\n2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Bontang and Balikpapan?\n\n\n\nWelp, no surprise there, I was right. My hometown (Kab. Kutai Kartanegara) did a good one in 2015, but that was it. The rest is Bontang, Balikpapan. Bontang, has a big industry from Pupuk Kaltim (PKT), and Balikpapan is a big Oil City with Airport, etc."
  },
  {
    "objectID": "posts/004-national-exam/index.html#what-do-we-get",
    "href": "posts/004-national-exam/index.html#what-do-we-get",
    "title": "Webscraping National Exams of Indonesia",
    "section": "What do We Get?",
    "text": "What do We Get?\nI think it was clear now that education still very much an exclusive trait, owned by province, and place where there is an industry like Pupuk Kaltim Bontang, and Oil companies in Balikpapan. Places where not so much happening like Tenggarong, is very much stay behind the rest of Kalimantan Timur, and further behind the rest of the big cities in Java and Bali.\nWith the exclusion of National Exams as the only metric for student passing the bar, I wonder how could we as a nation measure the level of education on every province of Indonesia. For that, I wish my country the best - TIme will tell..I guess."
  },
  {
    "objectID": "posts/008-guides-norway/index.html",
    "href": "posts/008-guides-norway/index.html",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "",
    "text": "Foto Dermaga. Rogaland adalah nama Provinsi dari Stavanger (bagian dari Provinsi)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#nomor-identifikasi-di-norway",
    "href": "posts/008-guides-norway/index.html#nomor-identifikasi-di-norway",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Nomor Identifikasi di Norway",
    "text": "Nomor Identifikasi di Norway\nDi Norway, setiap orang memiliki nomor identifikasi personal berupa 11 digit angka (ini bukanlah DUF-number1). Ini berlaku untuk orang dewasa ataupun anak kecil/ balita. Ada dua tipe ID number ini, yakni National Identity Numbers/ Personal Number/ Personnummer (disingkat sebagai P-num), dan D-numbers (disingkat D-num). Anda hanya dapat memiliki salah satu dari dua tipe ini, tergantung pada kondisi dan situasi anda ketika tiba di Norway, yang dapat dibaca lebih lanjut di website resminya.\nTanpa adanya nomor ini, anda tidak akan bisa membuat digital ID seperti MinID, BuypassID, dan BankID. Digital ID 2 atau e-ID ini akan dibutuhkan ketika membuat akun bank, mengakses pajak, akses ke klinik kesehatan (GP), sampai pada dompet digital bernama Vipps yang umum digunakan di Norway (mirip seperti Gopay kalau di Indonesia).\n\nD-num\n\nD-num umumnya diberikan sebagai nomor identifikasi sementara untuk orang asing di Norway yang berencana tinggal kurang dari 6 bulan, atau lebih dari 6 bulan namun tidak memenuhi syarat untuk mendapatkan P-num. Pengalaman rekan kerja di kantor, ada yang langsung diberikan P-num (seperti saya), ada juga yang diberikan D-num terlebih dahulu kemudian diberikan P-num.\n\nSalah satu kekurangan D-num adalah tidak ada akses ke fasilitas kesehatan publik atau Public Health (Helsenorge). Untuk bisa mendapatkan fasilitas kesehatan publik, minimal P-num harus ada.\n\n\n\nP-num\n\nP-num ini umumnya diberikan untuk warga negara Norway, atau orang asing yang bertempat tinggal lebih dari 6 bulan di Norway dan memenuhi syarat mendapatkan P-num. Detil syaratnya saya tidak tahu, namun dari beberapa orang yang saya kenal dan baru-baru ini pindah ke Norway, hampir semua mendapatkan P-num langsung.\n\nSayangnya proses mendapatkan P-num ini bervariasi dari 0 minggu, sampai 4-6 minggu setibanya di Norway. Saya pribadi mendapatkan P-number pada hari pertama datang ke SUA (kantor imigrasi Stavanger). Namun beberapa teman saya mendapatkannya setelah 2 minggu. Durasi ini bergantung pada banyaknya aplikasi pada saat itu, dan akhir-akhir ini banyak refugee yang datang ke Norway, sehingga bisa saja menjadi lebih lama dari biasanya.\nPengamatan saya pribadi, lebih besar kemungkinan dapat P-num jika anda membawa serta keluarga anda ketika datang ke SUA (Imigrasi), dan tidak ada ruginya bertanya, meminta apakah anda bisa mendapatkan P-num, karena sebab-sebab di atas (akses ke kesehatan, buat akun bank, dll).\n\nP number ini tidak usah dihapal semua, cukup 5 digit terakhir saja, karena 6 digit pertama berkaitan dengan tanggal-bulan-tahun lahir anda.\n\n\n\n\n\n\nTip\n\n\n\nProses membuat akun bank lokal membutuhkan kurang lebih 2-4 minggu, ditambah dengan proses menunggu hingga P-num anda tersedia, selama maksimal 2 bulan anda tidak akan memiliki akun pembayaran untuk kebutuhan sehari-hari di Norway. Disarankan untuk membawa minimal 2 jenis kartu kredit untuk pembayaran sehari-hari, bisa menggunakan VISA ataupun mastercard. Tidak disarakan menggunakan cash, karena di sini sangat jarang transaksi menggunakan cash, dan untuk menghindari sebaran virus seperti Covid-19. Saya sendiri menggunakan Wise dan tidak pernah ada masalah."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#transportasi-publik",
    "href": "posts/008-guides-norway/index.html#transportasi-publik",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Transportasi Publik",
    "text": "Transportasi Publik\nTransportasi di sini kebanyakan berupa bis, dan beberapa area tersedia kereta.\n\nDari Bandara ke Stavanger/ Sebaliknya\nUntuk transportasi dari Bandara ke Kota Stavanger (Stavanger Sentrum) dapat menggunakan Bis antar Bandara bernama Flybussen. Seperti terlampir di bawah, ada banyak pemberhentian bis ini dari dan ke bandara (Stavanger Lufthavn Sola/ SVG).\n\nWebsite-nya sekarang cukup intuitif dan mudah digunakan, terlampir contoh penggunaan aplikasi website Flybussen untuk tiket dari bandara ke statens hus (sebelahnya SUA).\n\nBiaya per orang dewasa sebesar NOK 158,00 (koma di sini bermakna koma, seperti di indonesia, jadi ini sebesar seratus lima puluh delapan Norwegian Kroner). Per tanggal artikel ini ditulis, 1 NOK = 1450 rupiah. Pembayaran dapat menggunakan kartu kredit VISA ataupun mastercard. Aplikasi Vipps seperti yang disebutkan sebelumnya, adalah digital wallet khusus Norway (akan dibahas mendetail di bab selanjutnya)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#dalam-kota",
    "href": "posts/008-guides-norway/index.html#dalam-kota",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Dalam Kota",
    "text": "Dalam Kota\nUntuk transportasi di Kota, bisa menggunakan Kolumbus. Ini semacam penyedia jasa bis dalam kota stavanger. Tiket bisa beli offline di stasiun bis, atau (seperti saya) beli online di aplikasinya. Ada dua aplikasi Kolumbus, ada yg untuk beli tiket (Kolumbus Billet).\n\nAda untuk bus tracking, bernama Kolumbus. Aplikasi ini cukup intuitif dan akurat dalam menentukan posisi bis secara hampir real-time.\n\nKetika pertamakali tiba di Stavanger, saya langsung membeli tiket via aplikasi dan melakukan pembayaran menggunakan kartu kredit. Anda bisa membeli tiket harian, mingguan, dan bulanan tergantung kebutuhan. Saya pribadi membeli tiket bulanan senilai NOK 650/ zone, untuk harga orang dewasa (anak dibawah umur tidak perlu membeli tiket). Setelah membeli anda akan mendapatkan akses ke real-time barcode di aplikasi Billet. Untuk region Stavanger-Sentrum-Madla-Tananger-Sola, masih masuk satu zonasi yang sama, sehingga cukup bayar untuk satu zona. Tiket ini juga sudah termasuk akses kereta lokal yang menghubungkan Stavanger sampai Egersund, dan juga kapal feri.\n\nHarap diwaspadai, kalau sejauh ini memang tidak ada kewajiban untuk memindai barcode sebelum naik bis, namun secara rutin dan acak akan ada pengecekan penumpang oleh polisi setempat. Jika ditemukan anda menaiki bis tanpa mempunyai tiket yang valid, denda per orang adalah senilai NOK 950 (data valid per Oktober 2023).\n\n\n\n\n\n\nKhusus Penduduk Stavanger\n\n\n\nMulai dari 3 juli 2023, untuk warga yang bertempat tinggal di Stavanger, transportasi via Kolumbus (bis, kereta, ferry) digratiskan. Untuk bisa mendapatkan tiket harus mendaftar ke stavanger kommune Registrasi Tiket Kolumbus Gratis. Anda membutuhkan D-num/ personal number dan nomor handphone lokal (untuk sms verifikasi)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#alat-pembayaran",
    "href": "posts/008-guides-norway/index.html#alat-pembayaran",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Alat Pembayaran",
    "text": "Alat Pembayaran\n\nKartu Kredit\nKebanyakan pembayaran dilakukan via kartu, atau apps (Vipps yg kita bahas nanti). Hanya 1-2x dalam dua bulan terakhir saya lihat ada orang bayar pakai cash.\nKartu apa yg bisa dipakai? Semua kartu yg ada logo Visa atau Mastercardnya. Saya sendiri pakai Wise card, dan Bigpay card. Keduanya berfungsi sama baiknya, kendati seringkali saya pakai Wise card, karena preferensi pribadi saja. Wise lebih luas jangkauannya, karena Bigpay ini sebarannya terkhusus di Asia saja, sedangkan untuk Wise, Eropa (termasuk Norway), UK jg masuk.\n\n\n\nAplikasi Vipps\nVipps merupakan aplikasi digital wallet seperti Gopay di Indonesia. Aplikasi inii cukup simpel karena memang hanya untuk bayar dan terima uang berdasarkan akun Vipps yang terhubung ke nomor handphone anda. Dibutuhkan Digital ID berupa BankID untuk membuat akun Vipps, karena termasuk ke dalam aplikasi keuangan."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#nomor-hp-lokal-dan-paket-data",
    "href": "posts/008-guides-norway/index.html#nomor-hp-lokal-dan-paket-data",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Nomor HP Lokal dan Paket Data",
    "text": "Nomor HP Lokal dan Paket Data\nSebagai disclaimer dulu, ini tidak urgen untuk awal-awal. Karena pertimbangan pribadi saya putuskan untuk beli nomor lokal. Klo tidak butuh sekali, bisa pakai paket roaming, yang saya yakin akan masih lebih murah daripada beli nomor dan paket data lokal. Saya pakai mycall, provider yg harga paketnya paling murah di angka 1GB (79 NOK), sampai 10GB (299 NOK), dan paket telpon unlimited di Norway saja (99 NOK), hingga untuk telpon ke Asia-Middle East-Africa, yang harganya tentu saja lebih mahal.\n\nCara belinya tinggal ke 7-11 terdekat, bilang saja mau beli nomor hape. Nanti tinggal bawa pasport, dan berkas residence permit dari UDI (jaga-jaga kalau ditanya), nanti ketika ditanya mau isi berapa, bilang saja unlimited call di Norway + paket data 3GB. Pengalaman saya 3GB cukup, asalkan pakai Wifi klo di rumah. Kalau sering browsing atau nonton youtube 6 jaman sehari, tidak akan cukup. Nanti bisa download aplikasinya yg wujudnya seperti terlampir, ini bisa didownload tanpa ganti region terlebih dahulu (saya jelasin di paragraf selanjutnya)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#aplikasi-tanpa-ganti-region-negara",
    "href": "posts/008-guides-norway/index.html#aplikasi-tanpa-ganti-region-negara",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Aplikasi Tanpa Ganti Region Negara",
    "text": "Aplikasi Tanpa Ganti Region Negara\nBeberapa aplikasi seperti Gojek misalnya, hanya bisa diinstall kalau hape kita appstorenya atau playstorenya terdaftar di negara Indonesia, atau negara yg ada Gojeknya. Demikian jg di Norway, ada beberapa aplikasi seperti Rema, dan Finn yg hanya bisa didownload jika regionnya terdaftar di Norway, kalau masih ngikut negara sebelumnya yg bukan Norway, kita gak bisa donlod.\nNamun demikian, untungnya beberapa aplikasi yg krusial bisa didonlod tanpa ganti region, seperti:\n\nKolumbus App dan Billet (aplikasi bis)\nMycall (untuk aplikasi beli paket data)\nSparebank (untuk aplikasi Bank)"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#browsing-pakailah-edge",
    "href": "posts/008-guides-norway/index.html#browsing-pakailah-edge",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Browsing? Pakailah Edge!",
    "text": "Browsing? Pakailah Edge!\nUntuk kalian pemakai iphone, pasti default nya akan pakai safari, sedangkan klo android biasanya pakai Chrome. Untuk mempermudah hidup anda, pakailah Edge (browser microsoft) ketika browsing di Norway. Alasannya, adalah Edge punya fiture built-in translator yg bisa dipakai untuk translate apapun yg tampil di browser kita dari Norwegian ke English/Indonesia. Saya belum nemu alternatif yg se-smooth Edge dari sisi implementasi dan user experience. Edge tersedia dalam browser windows, macos, ios dan android."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#mailbox-kotak-surat-adalah-keharusan",
    "href": "posts/008-guides-norway/index.html#mailbox-kotak-surat-adalah-keharusan",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Mailbox/ Kotak Surat adalah Keharusan",
    "text": "Mailbox/ Kotak Surat adalah Keharusan\nDi Norway, meskipun negaranya menurut saya sangat terintegrasi sistemnya, masih menggunakan old-school method, dengan mengirimkan berkas-berkas, ke kotak surat anda. Dari berkas bank, kartu kredit, verifikasi digital ID, hampir SEMUA HAL. Sehingga, ketika tidak ada mailbox, hampir pasti anda akan direpotkan, bahkan bisa jadi gak bisa. Saya ambil contoh, untuk residence card, dan kartu debit bank, saya bisa minta untuk dikirim ke office/bank saja, sehingga saya tinggal ambil di office/bank yang bersangkutan. Namun dalam banyak kondisi, seperti ketika mengurus digital ID, semua berkas akan dikirm ke mailbox.\n\n\n\n\n\n\nMailbox di Hotel\n\n\n\nKalau anda bertempat tinggal sementara di tempat yang tidak menyediakan mailbox tersendiri seperti hotel, anda bisa meminta mereka untuk meminjamkan mailbox hotel mereka untuk dikirimi berkas anda. Caranya bisa dengan meminta ijin terlebih dahulu ke pihak hotel (biasanya mereka paham), dan menempel nama lengkap anda, istri, pada mailbox (jika tidak ada nama lengkap/ salah, surat/ berkas akan dikirimkan kembali ke pengirim)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#residence-card",
    "href": "posts/008-guides-norway/index.html#residence-card",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Residence Card",
    "text": "Residence Card\nResidence card ini prinsipnya sama seperti KTP. Biasanya akan terpakai ketika kita butuh verifikasi diri, seperti ketika mengambil obat di apotik. Daripada membawa paspor kemana-mana, residence card sudah cukup. Tidak ada informasi mengenai P-num di dalam residence card, kecuali tentu 6 digit tanggal-bulan-tahun lahir anda, sehingga pastikan anda ingat 5 digit terakhir anda, karena akan diminta juga ketika verifikasi diri."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#digital-ids",
    "href": "posts/008-guides-norway/index.html#digital-ids",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Digital IDs",
    "text": "Digital IDs\nKetika kita mencoba mengakses website, akan ada langkah verifikasi diri online. Biasanya pilihan verifikasinya hanya terbatas dari salah dua atau salah tiga dari e-ID di bawah, tergantung pada tipe website yang anda akses. Pada website pajak misalnya, tampilannya seperti di bawah (ada tiga pilihan). Namun pada website kesehatan tampilan di bawah berganti menjadi BankID dan BuypassID saja.\n\nDigital ID atau Electronic ID ini terbagi jadi 4 jenis, namun jenis keempat saya skip saja karena gak relevan untuk tulisan ini. Saya hanya bahas tiga teratas. Fungsinya adalah untuk identifikasi/ autentifikasi identitas kita pada situs-situs penting seperti tax, kesehatan, national registry etc. Hanya bisa dipesan, jika kita sudah punya personal number (11 digit).\n\nMinID\nMinID ini adalah e-ID paling BASIC dan gratis. Jadi bisa digunakan di hampir semua akses, kecuali kesehatan. Jadi untuk bisa akses situs kesehatan, untuk bikin janji sama dokter kita (GP), harus pakai e-ID di atas minID, yakni BankID atau BuypassID.\n\n\n\nBuypassID\nBuypass ID, ini e-ID yg medium, artinya dia bisa akses apa yg minID bisa akses + beberapa situs tambahan, seperti website kesehatan untuk bikin janji temu dengan dokter. Kekurangannya, ini berbayar, sebesar 998 NOK, atau sekitar 1,2 juta IDR. Namun menurut saya pribadi worth it karena bisa ambil di Posten, semacam Pos Indonesia-nya Norway, dan tidak dikirm ke mailbox seperti MinID, jadi di kasus saya yang tidak ada mailbox pribadi, memudahkan. Namun patut dipahami, buypassID ini gak bisa dipesan tanpa P-num dan personal data. Semacam dokumen dari national registry, yg menyatakan nama kita, tempat tanggal lahir, dan bertempat tinggal di mana di Norway. Intinya, memastikan kita memang berdomisili dan legal di Norway.\n\n\n\n\n\n\n\nNote\n\n\n\nYang counter-intuitive adalah, personal data ini bisa dipesan via skatten (tax website), yang cuma bisa diakses via e-ID seperti minID. Jadi saran saya, segera pesan minID segera setelah dapat P-num. Namun kalau tidak bisa menunggu, bisa telpon CS dari kantor pajak, dan minta dikirim ke mailbox.\n\n\n\n\nBankID\nBankID ini adalah e-ID paling powerful dari semuanya. Bisa akses semua, dan satu-satunya yang diterima oleh bank, sehingga e-ID ini saja yang bisa dipakai untuk registrasi Vipps. Semacam e-wallet orang Norway. Hampir semua merchant menerima Vipps, dan hampir di banyak tempat yg tidak terlalu besar tokonya, cuma nerima Vipps. Seperti bazaar pasar makanan Indonesia.\n\nTapi, bankID ini buatnya paling susah. Disamping karena paspor saya bukan e-paspor, juga memang ada banyak dokumen yang disiapkan. Yang cukup lama buat saya adalah residence card yang baru jadi sekitar 6 minggu pasca dapat P-num. Itupun, harus kirim dokumen lagi ke Oslo, untuk verifikasi bahwa paspor kita valid (karena gak ada chipnya).\n\n\n\n\n\n\nTip\n\n\n\nTo recap:\nDapat personal num ‚Üí bikin minID ‚Üí beli BuypassID\ndapat residence card ‚Üí daftar DNB ‚Üí bikin BankID"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#akun-bank-lokal",
    "href": "posts/008-guides-norway/index.html#akun-bank-lokal",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Akun Bank Lokal",
    "text": "Akun Bank Lokal\n\nSparebank\nSparebank-1 ini adalah bank pertama saya, karena tidak butuh residence card, bankID. Jadi hanya perlu personel num, dan paspor. Tinggal daftar online, nanti jika sudah ready akan dipanggil ke bank terdekat untuk verifikasi dan pembuatan akun bank. 1-2minggu setelahnya akan dikirimkan kartu debit ke mailbox.\n\n\n\n\n\n\n\nCaution\n\n\n\nPatut dicatat untuk ganti region sesuaikan sama postcode ketika mendaftar, karena nanti akan diarahkan ke website yang berbeda.\n\n\nUntuk yang tidak ada mailbox (seperti saya), ada opsi untuk pengambilan kartu debit dari akun bank anda diberikan di kantor tempat anda mendaftar. Saya sendiri mendapatkan kartu debit kurang lebih seminggu setelah akun bank dibuat.\n\nTips dari saya adalah langsung minta kartu kredit juga untuk pribadi dan istri / minta naik limit CC (kalau dibutuhkan), karena kalau kita minta setelahnya dan online, berhubung kita belum ada rekam kredit di Norway, hampir pasti akan ditolak. Jadi baiknya langsung ngobrol sama CS bank, dan minta dibuatkan kartu kredit."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#akun-altinn-digital-docs",
    "href": "posts/008-guides-norway/index.html#akun-altinn-digital-docs",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Akun Altinn (Digital Docs)",
    "text": "Akun Altinn (Digital Docs)\nAltin ini pada dasarnya seperti mailbox digital semua hal terkait informasi kita di Norway. Hanya bisa login ke sini kalau kita ada buypassID atau bankID. MinID gak bisa. Kalau kita melakukan perubahan alamat misalnya, akan ada email beserta dokumen pdf di Altinn, kalau kita ajukan tax card etc. Contoh inbox di bawah:"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#public-health-helsenorge",
    "href": "posts/008-guides-norway/index.html#public-health-helsenorge",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Public Health (Helsenorge)",
    "text": "Public Health (Helsenorge)\n\nGP (Personal Doctor/ Clinic)\nHelsenorge (Helsenorge ‚Äì din helse p√• nett - Helsenorge) adalah website untuk akses informasi kesehatan pribadi. Di website inilah kita buat temujanji dengan dokter (GP) kita yang sudah didaftarkan atas nama kita. Kita bisa cek kita terdaftar dengan GP siapa di Bytte fastlege - Helsenorge. Pastikan nomor hp anda terdaftar di helsenorge, sehingga setiap kali ada notifikasi maka anda akan dapat pesan teks. Selalu cek pesan di hp anda, dan email anda untuk setiap notifikasi terkait jadwal cek dokter, vaksin, temu janji dll.\n\nYang patut diwaspadai ketika ke klinik, atau ke pengobatan/ pengecekan yang melibatkan klinik apapun, pastikan anda bertanya dengan perawat apakah yang anda dapatkan berbayar atau gratis. Bila berbayar, PASTIKAN anda membayar segera setelah perawatan/ pengecekan. Misalnya untuk cek ke klinik GP, setelah bertemu dokter, pastikan anda membayar di mesin bayar. Karena jika tidak, akan ada ekstra invoice sebesar NOK 100 yang harus anda bayarkan di atas biaya dokter tadi. Mesin bayarnya seperti berikut, anda tinggal masukkan P-num, dan bayar menggunakan kredit kard/ debit.\n\n\n\nEmergency/ Legevakten\nPatut dipahami, GP hanya akan handle di jam dan hari kerja, ketika di luar hari kerja dan jam kerja, maka akan dialihkan ke UGD nya sini, bernama Legevakten.\nDisarankan telpon dulu, namun jangan kaget, karena kalau belum benar-benar emergency, maka tidak akan disuruh ke rumah sakit. Jadi sebaiknya telpon dulu, agar nantinya disuruh ke rumah sakit, sudah dapat nomor antrian. Apa yang dianggap emergency? Ada beberapa, namun yang saya ingat adalah:\n\nKesulitan bernapas\nPendarahan\nKehilangan kesadaran\nKejang-kejang\n\nKalau hanya panas dan batuk? Biasanya disuruh istirahat saja. üôÇ"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#nav",
    "href": "posts/008-guides-norway/index.html#nav",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Nav",
    "text": "Nav\nNav.no ini semacam website ketenagakerjaan, apapun yg berkaitan sama pekerjaan seperti pensiun scheme, surat ijin sakit, akan masuk ke mailbox website ini. Ketika teman saya di Norway mengalami layoff, maka akan ada bantuan pemerintah senilai 80% salary sampai yang bersangkutan dapat pekerjaan lagi.\nSalah satu hal favorit saya adalah mendaftarkan anak saya sehingga saya dapat keringanan dalam wujud suport finansial berupa uang ditransfer ke rekening ayah/ ibu senilai kurang lebih NOK 1700 (2,4 Juta IDR) per bulannya. Terus terang ini cukup membantu, paling tidak meringankan biaya sekolah anak (Barnehage)."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#sekolah-anak",
    "href": "posts/008-guides-norway/index.html#sekolah-anak",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Sekolah Anak",
    "text": "Sekolah Anak\n\nBarnehage (Daycare+Playgroup)\nDi Norway, anak di bawah 7 tahun wajib masuk Barnehage (daycare + playgroup). Pendidikan wajib di Norway, dan merupakan hak anak dan kewajiban orangtua untuk mendaftarkan. Minimal e-ID adalah MinID untuk masuk ke website pendaftaran Vigilo.\n\nSetelah masuk, anda bisa memulai aplikasi baru dan mendaftar lebih dari satu barnehage. Biasanya akan dipilihkan lokasi yang paling dekat ke lokasi rumah/ tempat tinggal anda yang terdaftar di national registry, sehingga pastikan data anda valid dan up-to-date.\n\nSaran saya ambil saja mana yang paling awal menerima anak anda, dan jangan khawatir ketika anda menerima satu barnehage, maka barnehage lain tidak akan terhapus dan prosesnya tetap berjalan. Sebagai contoh, dalam kasus saya, anak saya mendaftar di dua barnehage yang berbeda dekat rumah. Barnehage yang menerima pertamakali adalah barnehage yg letaknya 5-10min berjalan kaki dari rumah, sehingga kami langsung terima.\nNamun, setelah kurang lebih satu bulan, barnehage prioritas kami yang berjarak paling dekat ke rumah akhirnya kosong dan menerima. Sehingga, pada bulan kedua, kami pindah ke barnehage baru."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#belanja-bahan-makanan",
    "href": "posts/008-guides-norway/index.html#belanja-bahan-makanan",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Belanja Bahan Makanan",
    "text": "Belanja Bahan Makanan\n\nSupermarket Umum\nUntuk keseharian, pilihan ada di antara CoopMega, Extra, Joker, dan Rema1000. Extra dan Rema1000 ini mirip seperti Alfamart dan Indomaret, di mana-mana ADA. Sedangkan CoopMega ini seperti Giant. Baik Extra, Joker dan Rema buka setiap hari senin-sabtu, dari jam 8-23, kecuali sabtu (tutup/ buka lebih awal). Untuk hari minggu, kebanyakan tutup, ada beberapa yang buka seperti di hari minggu seperti Extra di Sandness sentrum, dan Joker di sampik Vaktapotek Stavanger sentrum. Selebihnya, sebaiknya belanja sebelum di hari minggu.\n\n\nExtra\n\n\n\n\n\nREMA1000\n\n\n\n\n\nSupermarket Muslim\nSupermarket muslim, yang maksudnya menjual daging-dagingan yang halal, ada beberapa. Utamanya ada IMS Internasjonal Matsenter yang lokasinya ada di beberapa di area Stavanger (bisa cari di google maps). Selain itu ada juga rumah daging Turki di area Pedersgata (Istanbul Import).\n\n\nIMS\n\n\n\n\n\nIstanbul Import"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#online-marketplace",
    "href": "posts/008-guides-norway/index.html#online-marketplace",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Online Marketplace",
    "text": "Online Marketplace\nDi Norway, ada website sapujagat. Isinya dari jualan barang second-hand, baru, lowongan pekerjaan, sampai pada penyewaan apartemen. Namanya Finn.no. Lagi-lagi, website ini tidak berbahasa inggris, sehingga gunakan Edge browser ya, jadi langsung auto translate. Saya pribadi belum pernah beli di sini sampai sekarang, tapi cerita dari teman-teman, cukup OK terutama untuk pakaian anak-anak yang cepat sekali ganti.\n\n\nFinn No"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#berapa-biaya-hidup-di-norway",
    "href": "posts/008-guides-norway/index.html#berapa-biaya-hidup-di-norway",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Berapa Biaya Hidup di Norway?",
    "text": "Berapa Biaya Hidup di Norway?\n\nBiaya hidup sangat bergantung terhadap status anda (single/ family), lokasi tempat tinggal anda (Oslo/Stavanger, Tromso, Trondheim, etc), dan gaya hidup anda (masak/ beli). Di atas adalah tabel yang saya pakai untuk estimasi pengeluaran per bulan. Rincian saya jabarkan di bawah.\n\n1. Tempat tinggal\nDalam kasus saya, tempat tinggal memakan pengeluaran paling besar. Untuk lokasi yg masih dalam jarak tempuh 15-20min dari sentrum (pusat kota stavanger), harga sewa bervariasi antara 12,000 NOK hingga 16,000 NOK. Patut diingat kalau ketika mencari rumah, usahakan kunjungi rumahnya (visiting), dan kalau bisa punya kontak yg bisa rekomendasikan anda pertamakali (biasanya landlord/owner akan telpon nomor tersebut untuk memastikan anda memang legit).\n\n\n\n\n\n\nTip\n\n\n\nJika anda bertempat tinggal di Stavanger, anda akan berhak akan bus gratis senilai 650 NOK per bulan, sehingga, anggap saja jika anda menyewa tempat tinggal di area stavanger ‚Äì maka anda akan mendapatkan diskon minimal 650 NOK.\n\n\n\n\n2. Groceries\nPada umumnya biaya makan bergantung dengan jumlah anggota keluarga anda (single/ family), dan gaya hidup/makan anda. Kalau anda masak, dan mau berkompromi dengan bahan makanan (tidak melulu beli barang impor), maka biaya makan anda akan relatif rendah (5000 NOK jika single, 7000 jika family).\nNamun jika sering beli barang impor (makanan asia misalnya), maka mungkin kisaran 8000-10,000 NOK lebih masuk akal.\n\n\n3. Electricity\nListrik bergantung pemakaian, namun umumnya tinggi di musim dingin (2000-3000 NOK) dan rendah di bulan-bulan lain (1000 NOK), sehingga reratanya anggap saja 1500-2000 NOK per bulan.\n\n\n4. Wifi dan Prepaid\nWifi ini tergantung dengan deal tempat tinggal anda. Pada banyak kasus, wifi sudah termasuk ke dalam uang sewa, namun jika memang tidak, biaya yg diperlukan bergantung pada kecepatan internet, dan provider yg tersedia di lokasi tempat tinggal. Untuk prepaid sudah saya bahas di bab sebelumnya (Nomor HP Lokal dan Paket Data).\n\n\n4. Transportasi\nHampir di semua region di Norway, transportasi publik sangat baik. Di stavanger, terdapat dua transportasi publik, bis dan kereta. Providernya adalah kolumbus (baca bab Transportasi Publik). Bis nya sangat reliable dan menjangkau hampir semua area, bahkan sampai ke area perumahan. Biayanya untuk warga yang bertempat tinggal di Stavanger‚Äì gratis. Namun jika di luar area stavanger, biayanya 650 NOK per bulan per orang. Region lain seperti Trondheim, biaya tiket senilai 950 NOK per bulan per orang.\n\n\n\n\n\n\nNote\n\n\n\nUntuk Bus, bila bertempat di Stavanger, bisa mendapatkannya dengan gratis. Cek Khusus Penduduk Stavanger"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#pajak-pendapatan-di-norway",
    "href": "posts/008-guides-norway/index.html#pajak-pendapatan-di-norway",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Pajak Pendapatan di Norway",
    "text": "Pajak Pendapatan di Norway\nDi Norway, pajak untuk pendapatan yang per tahun tidak lebih besar dari 650,000 NOK adalah sebesar 25%. Sedangkan untuk pendapatan di atas 650,000 NOK akan dihitung berdasarkan bracket tax yang ditentukan, lebih mudahnya bisa menuju link ini: Tax Calculator. Contoh seperti di bawah;\nPatut dipahami, bahwa pajak di Norway dihitung per tahun, namun diambil sebanyak 10.5 bulan, karena pajak di bulan May-Jun adalah 0%, sedangkan di bulan-bulan masuk Winter senilai 50% dari nilai aslinya, sehingga 12-1.5=10.5. Kenapa ini penting? Karena nilai pajak anda per bulan akan menjadi lebih besar dari hitungan per tahun dibagi 12 bulan.\n\nSebagai contoh, jika pajak anda adalah senilai 280,000 NOK per tahun dari total pendapatan senilai 800,000 NOK, maka pajak anda adalah 35% dari total pendapatan. Namun karena pajak hanya ditagih sebanyak 10.5x dalam setahun, maka di bulan-bulan yg ditagih pajak, setiap penagihan, nilainya dalah 35%*12/10.5=40%. Itulah sebabnya pajak di Norway ‚Äúterlihat‚Äù tinggi."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#holiday-pay",
    "href": "posts/008-guides-norway/index.html#holiday-pay",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Holiday Pay",
    "text": "Holiday Pay\nDi Norway, ada satu hal yang tidak ada di banyak negara lain, yakni holiday pay.\nPada banyak perusahaan di Norway, kita biasanya diberikan kurang lebih 21-25 hari libur (vacation), yang kalau dhitung kurang lebih 1 bulan. Sehingga, ada mekanisme, supaya di bulan kita tidak bekerja selama kurang lebih 1 bulan ini, kita tetap menerima gaji ‚Äì inilah Holiday Pay.\nBesarnya kurang lebih 12% dari income tahunan (tergantung umur dan lain-lain, baca lebih lanjut di sini ), namun hanya bisa didapatkan ketika kita sudah bekerja selama 1 tahun di Norway. Holiday pay ini diberikan di bulan-bulan memasuki musim panas (summertime), sekitar bulan May-Jun, sehingga jika anda seperti saya yang datang di bulan April, anda tidak akan mendapatkan gaji di bulan May-Jun, karena belum genap 1 tahun di Norway. Baru di tahun berikutnyalah anda akan mendapatkan gaji full di bulan May-Jun.\n\nDalam beberapa kasus, ada situasi di mana anda bisa MENJUAL jatah cuti anda demi mendapatkan gaji di bulan May-Jun, ketika anda belum genap 1 tahun di Norway."
  },
  {
    "objectID": "posts/008-guides-norway/index.html#penutup",
    "href": "posts/008-guides-norway/index.html#penutup",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Penutup",
    "text": "Penutup\nTerimakasih sudah membaca sampai sini. Jika ada yang ingin ditanyakan, langsung saja DM atau Mention saya di Twitter. Saya sudah rangkumkan pula beberapa hal sehari-hari di thread di bawah. Enjoy!"
  },
  {
    "objectID": "posts/008-guides-norway/index.html#footnotes",
    "href": "posts/008-guides-norway/index.html#footnotes",
    "title": "Petunjuk Hidup dan Menetap di Stavanger, Norway",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDUF-num adalah nomor pendaftaran ketika mendaftarkan diri anda/ anggota keluarga di UDI (website imigrasi Norway). Nomor ini terhubung dengan nomor antrian aplikasi anda di sistem.‚Ü©Ô∏é\nDigital ID atau e-ID ini memiliki tingkat akses yang berbeda. MinID adalah e-ID paling dasar, bisa digunakan untuk membuat akun bank, dan akses pajak. Untuk akses ke kesehatan anda butuh minimal BuypassID. Sedangkan untuk aplikasi keuangan seperti Vipps anda butuh BankID.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/005-webscraping-machinelearning-rent-pricing/index.html",
    "href": "posts/005-webscraping-machinelearning-rent-pricing/index.html",
    "title": "Malaysia Property Pricing - Webscraping & Machine Learning Model",
    "section": "",
    "text": "Pacmann Batch 8 Capstone by Aditya Arie Wijaya (aditya-66kK)\n\n\nThis is a machine learning project to predict unit/property monthly rent price in Kuala Lumpur region, Malaysia. The project uses a dataset from an online ads listing for property mudah.my. This project outlines the process of web-scraping/ data gathering, data cleaning-wrangling, and machine learning modeling.\nThis project aims to answers question about how much a unit monthly rent would be if given information such as location, number of bedrooms, parking, furnished, etc? This would help potential tenant and also the owner to get the best price of their rental unit, comparable to the market value.\nSome previous work about house pricing was listed below, however most of them are targeting a dataset of house pricing or an Airbnb pricing. There are difference such as in Airbnb, the booking rarely took more than 2 weeks, let alone a year. Therefore the pricing may be different. Additionally, in Airbnb, there is text feature coming from the review given by the tenant and the owner.The better the review, the higher the rent prices ‚Äì which was not available in this current project dataset.\nPrevious work by (Madhuri, Anuradha, and Pujitha 2019), (Xu and Nguyen 2022), (Zhao et al. 2022) highlight the importance feature selection, and the choice of machine learning model. Based on the previous works, the most consistently performed machine learning model are Random Forest and Gradient boosting, and the MAE and R2 score usually used in evaluating the performance of the model. Although the above work are all not about apartment rent pricing, similar method can be applied to this project.\n\n\n\nThe data will use a scraped data from the website mentioned before, focusing on property-to-rent surrounding Kuala Lumpur, Malaysia. \n\n\nCode\n#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('max_colwidth', 200)\nfrom bs4 import BeautifulSoup as bs\nimport requests\nimport re\nimport time\nimport datetime\nimport smtplib\nimport json\nfrom tqdm.notebook import tqdm, trange\nimport time    # to be used in loop iterations\n\n!jupyter nbextension enable --py widgetsnbextension #enabling progress bar\n\n\nEnabling notebook extension jupyter-js-widgets/extension...\n      - Validating: problems found:\n        - require?  X jupyter-js-widgets/extension\n\n\n\n\nThe process started out by gathering data from the website. We are using python library for web-scraping: BeautifulSoup as depicted below. The first process is generating a list of webpage address for a given page number.\n\n\nCode\n#generate list address of n_page\ndef page_number(start, end):\n    \"\"\"\n    Description:\n        Function to generate a list of webpage address for a given page number\n\n    Parameters:\n        start (int) : starting page number\n        end (int)   : ending page number\n    Returns:\n        a list of listing web address \n    \n    \"\"\"\n    \n    page_url = 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o='\n    list_page = []\n    for i in range(start,end+1):\n        list_page.append(page_url+str(i))\n    return list_page\npage_number(2,4)\n\n\n['https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=2',\n 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=3',\n 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=4']\n\n\nThen generate a list of ads listing on a single page.\n\n\nCode\n#setting up list of page from \ndef get_list_html(page_url):\n    \"\"\"\n    Description:\n        Function to get every listing ads in a given url (page_url)\n\n    Parameters:\n        page_url (str): website url\n        \n    Returns:\n        a list of listing ads\n    \n    \"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n    page = requests.get(url=page_url, headers=headers)\n    soup = bs(page.text, \"html.parser\")\n\n    script_tag = soup.find('script', type='application/ld+json')\n    data = json.loads(script_tag.text)\n    dict_query = data[2]['itemListElement']\n\n    n_query = data[2]['numberOfItems']\n    list_html = []\n\n    for i in range(n_query):\n        link = data[2]['itemListElement'][i]['item']['url']\n        list_html.append(link)\n        \n    return list_html\n\n\n#getting listing property from the 1st-5th in the list\nget_list_html('https://www.mudah.my/neighbouring-kuala-lumpur/apartment-for-rent?o=2')[0:5]\n\n\n['https://www.mudah.my/Suria+Court+Mahkota+Cheras+-100450215.htm',\n 'https://www.mudah.my/Cemara+Apartment+Bandar+Sri+Permaisuri+Cheras+actual+pic-98750544.htm',\n 'https://www.mudah.my/Seasons+Garden+PV21+850sf+3R2B+Fully+Furnished+Actual+Pics+-100449939.htm',\n 'https://www.mudah.my/Pangsapuri+palma+puteri+with+fully+furnished+seksyen+6-92222541.htm',\n 'https://www.mudah.my/STUDENT+Danau+Perintis+Fully+Furnish+WIFI+PUNCAK+ALAM-100143070.htm']\n\n\nCombining the previous two functions, generate a list of url for all pages.\n\n\nCode\n#generate listing property from each page of n_page\ndef get_list_url(n_page):\n    \"\"\"\n    Description:\n        Function to get every listing ads in every page (n_page)\n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a list of listing ads\n    \n    \"\"\"\n    list_html=[]\n    for i in tqdm(range(n_page)):\n        list_html.extend(get_list_html(page_number(1, n_page)[i]))\n    return list_html\n\nget_list_url(2)[:10]\n\n\n\n\n\n['https://www.mudah.my/Pertiwi+Indah+1285sf+fully+furnished-98377671.htm',\n 'https://www.mudah.my/PPA1M+Kepong+2+Parking+FULLY+FURNISHED+-95301490.htm',\n 'https://www.mudah.my/PPA1M+Kepong+MARCH+2023+FULLY+FURNISHED+-98845611.htm',\n 'https://www.mudah.my/Ixora+Apartment+Kepong+yang+lengkap+dengan+Time+Internet+Access-100451938.htm',\n 'https://www.mudah.my/Pangsapuri+Permai+Sungai+Besi+3R2B+near+TBS+Bandar+Tasik+Selatan-98987533.htm',\n 'https://www.mudah.my/Inspirasi+Mont+Kiara+BRAND+NEW+RENOVATED+MID+FLOOR+UNIT+NICE+VIEW+-99491862.htm',\n 'https://www.mudah.my/Maxim+Residence+3+Room+3+Aircond+2+Car+Park+Walking+Distance+MRT-100451825.htm',\n 'https://www.mudah.my/Residensi+Harmoni+2+Cheapest+Rental+Near+Mont+Kiara+Sri+Hartamas+KL-100451718.htm',\n 'https://www.mudah.my/Angkasa+Condo+Taman+Connaught+Cheras+Walking+Distance+To+UCSI-100213869.htm',\n 'https://www.mudah.my/Maxim+Residence+Condo+Taman+Len+Cheras-99852286.htm']\n\n\nThen extract the attributes inside the listing ads in a form of nested dictionary.\n\n\nCode\n#extract data from url\ndef get_list_dict(n_page):\n    \"\"\"\n    Description:\n        Function to get dataset (atribut of units) in a form of dictionary. \n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a dictionaries of attributes inside a list\n    \n    \"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n    list_html = get_list_url(n_page)\n    list_dict = []\n    for url in tqdm(list_html):\n        try:\n            page = requests.get(url=url, headers=headers)\n            soup1 = bs(page.text, \"html.parser\")\n            soup2 = bs(soup1.prettify(), 'html.parser')\n\n            id_html = re.search(r'(\\d+).htm', url).group(1)\n            title = soup2.find(itemprop='name').text.strip()\n\n            script_tag = soup2.find(\"script\", id=\"__NEXT_DATA__\")\n            script_content = script_tag.text\n            data = json.loads(script_content)\n            props = data.get(\"props\", {})\n            id_listing = re.search(r'-(\\d+)\\.htm', url).group(1)\n\n            dict_id = [{'realValue': '', 'id': 'ads_id', 'value': id_listing, 'label': 'id ads'}]\n            dict_building = props[\"initialState\"][\"adDetails\"][\"byID\"][id_listing][\"attributes\"]['propertyParams'][2]['params']\n            dict_prop = props[\"initialState\"][\"adDetails\"][\"byID\"][id_listing][\"attributes\"]['categoryParams']\n            dict_unit = dict_id + dict_building + dict_prop\n        except:\n            None\n        \n        list_dict.append(dict_unit)\n        \n    return list_dict\n\n#sanity check\nget_list_dict(1)[1]\n\n\n\n\n\n\n\n\n[{'realValue': '', 'id': 'ads_id', 'value': '95301490', 'label': 'id ads'},\n {'realValue': 'PPA1M METROPOLITAN KEPONG',\n  'id': 'prop_name',\n  'value': 'PPA1M METROPOLITAN KEPONG',\n  'label': 'Building Name'},\n {'realValue': '', 'id': 'developer_name', 'value': '', 'label': 'Developer'},\n {'realValue': 'BLOK A PPA1M METROPOLITAN KEPONG MRR2, Kuala Lumpur, Kepong',\n  'id': 'full_address',\n  'value': 'BLOK A PPA1M METROPOLITAN KEPONG MRR2, Kuala Lumpur, Kepong',\n  'label': 'Address'},\n {'realValue': '',\n  'id': 'completion_year',\n  'value': '',\n  'label': 'Completion Year'},\n {'realValue': '', 'id': 'num_floors', 'value': '', 'label': '# of Floors'},\n {'realValue': '', 'id': 'num_units', 'value': '', 'label': 'Total Units'},\n {'realValue': 'RM 1 600 per month',\n  'id': 'monthly_rent',\n  'value': 'RM 1 600 per month',\n  'label': 'Monthly Rent'},\n {'realValue': '2020',\n  'id': 'category_id',\n  'value': 'Apartment / Condominium, For rent',\n  'label': 'Category'},\n {'realValue': '9',\n  'id': 'location',\n  'value': 'Kuala Lumpur - Kepong',\n  'label': 'Location'},\n {'realValue': '1',\n  'id': 'property_type',\n  'value': 'Condominium',\n  'label': 'Property Type'},\n {'realValue': '1',\n  'id': 'floor_range',\n  'value': 'High',\n  'label': 'Floor Range'},\n {'realValue': '5', 'id': 'rooms', 'value': '5', 'label': 'Bedrooms'},\n {'realValue': '2', 'id': 'bathroom', 'value': '2', 'label': 'Bathroom'},\n {'realValue': '1500', 'id': 'size', 'value': '1500 sq.ft.', 'label': 'Size'},\n {'realValue': '1',\n  'id': 'furnished',\n  'value': 'Fully Furnished',\n  'label': 'Furnished'},\n {'realValue': '13,9,12,7,5,16',\n  'id': 'facilities',\n  'value': 'Parking, Security, Lift, Playground, Minimart, Multipurpose hall',\n  'label': 'Facilities'},\n {'realValue': '6200',\n  'id': 'rendepo',\n  'value': 'RM 6200',\n  'label': 'Rental Deposit'},\n {'realValue': '1',\n  'id': 'additional_facilities',\n  'value': 'Air-Cond',\n  'label': 'Other Facilities'},\n {'realValue': 'e', 'id': 'firm_type', 'value': 'E', 'label': 'Firm Type'},\n {'realValue': '10091',\n  'id': 'estate_agent',\n  'value': '10091',\n  'label': 'Firm Number'}]\n\n\nExtracting the values inside the dictionary for each attributes.\n\n\nCode\n#getting values out of dictionary\ndef get_values(list_dict):\n    \"\"\"\n    Description:\n        Function to values of the previous dictionary.\n\n    Parameters:\n        list_dict (list): list of dictionary where attributes stored\n        \n    Returns:\n        a list of values (unit/property) attributes\n    \n    \"\"\"\n    keys = [\n        'ads_id',\n        'prop_name',\n        # 'developer_name', \n        # 'address', \n        'completion_year', \n        # 'num_floors', \n        # 'num_units',\n        'monthly_rent', \n        # 'category_id', \n        'location', \n        'property_type', \n        # 'floor_range', \n        'rooms', \n        'parking',\n        'bathroom', \n        'size', \n        'furnished',\n        'facilities', \n        'additional_facilities', \n       ]\n\n    values = {}\n    for key in keys:\n        try:\n            values[key] = next(item['value'] for item in list_dict if item[\"id\"] == key)\n        except StopIteration:\n            values[key] = None\n    return values\n\n#sanity check\nget_values(get_list_dict(1)[1])\n\n\n\n\n\n\n\n\n{'ads_id': '95301490',\n 'prop_name': 'PPA1M METROPOLITAN KEPONG',\n 'completion_year': '',\n 'monthly_rent': 'RM 1 600 per month',\n 'location': 'Kuala Lumpur - Kepong',\n 'property_type': 'Condominium',\n 'rooms': '5',\n 'parking': None,\n 'bathroom': '2',\n 'size': '1500 sq.ft.',\n 'furnished': 'Fully Furnished',\n 'facilities': 'Parking, Security, Lift, Playground, Minimart, Multipurpose hall',\n 'additional_facilities': 'Air-Cond'}\n\n\n\n\nCode\n#get df from list\ndef get_df_final(n_page):\n    \"\"\"\n    Description:\n        Function to generate dataframe from the list.\n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a dataframe of the list of attributes on each listings.\n    \n    \"\"\"\n    list_data = get_list_dict(n_page)\n    list_new = []\n    for i in range(0,len(list_data)):\n            dic = get_values(list_data[i])\n            list_new.append(dic)\n    \n    df = pd.DataFrame(list_new)\n    return df\n\n\nOf course we won‚Äôt scrape 250 pages at first, let‚Äôs extract 1 page only:\n\n\nCode\n#sanity check\nget_df_final(1).head(2).T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nads_id\n98377671\n95301490\n\n\nprop_name\n\nPPA1M METROPOLITAN KEPONG\n\n\ncompletion_year\n\n\n\n\nmonthly_rent\nRM 2 000 per month\nRM 1 600 per month\n\n\nlocation\nKuala Lumpur - Cheras\nKuala Lumpur - Kepong\n\n\nproperty_type\nCondominium\nCondominium\n\n\nrooms\n3\n5\n\n\nparking\n1\nNone\n\n\nbathroom\n2\n2\n\n\nsize\n1285 sq.ft.\n1500 sq.ft.\n\n\nfurnished\nFully Furnished\nFully Furnished\n\n\nfacilities\nParking, Security, Playground, Lift, Swimming Pool, Minimart\nParking, Security, Lift, Playground, Minimart, Multipurpose hall\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed\nAir-Cond\n\n\n\n\n\n\n\nFinally, let‚Äôs extract dataset from 250 pages. File is then saved into a csv, to be reloaded again.\n\n\nCode\n# df_=get_df_final(250)\n# df_.to_csv('mudah-apartment-raw.csv', index=False)\n## already run, and file is saved\n\n\n\n\n\n\n\nReload the original dataset.\n\n\nCode\n#read it back\ndf = pd.read_csv('./mudah-apartment-raw.csv')\ndf.head(3).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n\n\nmonthly_rent\nRM 4 200 per month\nRM 2 300 per month\nRM 1 000 per month\n\n\nlocation\nKuala Lumpur - Taman Desa\nKuala Lumpur - Cheras\nKuala Lumpur - Taman Desa\n\n\nproperty_type\nCondominium\nCondominium\nApartment\n\n\nrooms\n5\n3\n3\n\n\nparking\n2.0\n1.0\nNaN\n\n\nbathroom\n6.0\n2.0\n2.0\n\n\nsize\n1842 sq.ft.\n1170 sq.ft.\n650 sq.ft.\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 13 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   ads_id                 10000 non-null  int64  \n 1   prop_name              9492 non-null   object \n 2   completion_year        5623 non-null   float64\n 3   monthly_rent           10000 non-null  object \n 4   location               10000 non-null  object \n 5   property_type          10000 non-null  object \n 6   rooms                  9998 non-null   object \n 7   parking                7368 non-null   float64\n 8   bathroom               9998 non-null   float64\n 9   size                   10000 non-null  object \n 10  furnished              9999 non-null   object \n 11  facilities             9104 non-null   object \n 12  additional_facilities  7167 non-null   object \ndtypes: float64(3), int64(1), object(9)\nmemory usage: 1015.8+ KB\n\n\nThe following feature is available in the dataset:\n\nads_id: ads listing ID, unique to each ads\nprop_name: the building name of the property\ncompletion_year: year of the building/property completed\nmonthly_rent: monthly rent price in Malaysian Ringgit (RM)\nlocation: the location (region) of the property\nproperty_type: property type, such as flat, apartment, etc\nrooms: number of rooms\nparking: number of parking spot\nbathroom: number of bathroom\nsize: total area of the unit in sq.ft\nfurnished: furnishin status of the unit, fully-partial-non\nfacilities: main facilities within the unit\nadditional_facilities: additional facilities\n\n\n\n\n\nCode\n#cek duplikat\ndf.duplicated().sum()\n\n#drop duplikat\ndf1 = df.drop_duplicates()\n\n\nSaving the file to csv after remove duplicated values.\n\n\nCode\n# #saving to csv\n# df1.to_csv(\"mudah-apartment-clean.csv\", index=False)\n# #saved already\n\n\nReload the data after drop duplicates\n\n\nCode\n#reload the data\ndf = pd.read_csv(\"./mudah-apartment-clean.csv\")\n\n\n\n\nCode\n#sanity check\ndf.duplicated().sum()\n\n\n0\n\n\n\n\n\n\n\nCode\n#removing RM from monthly rent\ndf['monthly_rent'] = df['monthly_rent'].apply(lambda x: int(re.search(r'RM (.*?) per', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'monthly_rent': 'monthly_rent_rm'})\n\n#dropping sq.ft from size\ndf['size'] = df['size'].apply(lambda x: int(re.search(r'(.*?) sq', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'size': 'size_sqft'})\n\n#dropping kuala lumpur from the location\ndf['location'] = df['location'].apply(lambda x: re.findall(\"\\w+$\", x)[0])\ndf.head(4).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n100191767\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\nSentul Point Suite Apartment\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n2020.0\n\n\nmonthly_rent_rm\n4200\n2300\n1000\n1700\n\n\nlocation\nDesa\nCheras\nDesa\nSentul\n\n\nproperty_type\nCondominium\nCondominium\nApartment\nApartment\n\n\nrooms\n5\n3\n3\n2\n\n\nparking\n2.0\n1.0\nNaN\n1.0\n\n\nbathroom\n6.0\n2.0\n2.0\n2.0\n\n\nsize_sqft\n1842\n1170\n650\n743\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\nParking, Playground, Swimming Pool, Squash Court, Security, Minimart, Gymnasium, Lift\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\nCooking Allowed, Near KTM/LRT, Washing Machine\n\n\n\n\n\n\n\n\n\n\nHypotheses: closer access to KTM/LRT = higher monthly rent\n\n\nCode\n#extracting near KTM/LRT from the additional facilities\ndef extract_near_ktm_lrt(text):\n    pattern = re.compile(r'\\bNear KTM/LRT\\b')\n    try:\n        match = pattern.search(text)\n        if match:\n            return 'yes'\n        return 'no'\n    except TypeError:\n        return text\n\n\nExtracting ‚Äúnear KTM/LRT‚Äù into its own column.\n\n\nCode\ndf['nearby_railways'] = df.additional_facilities.apply(lambda x: extract_near_ktm_lrt(x))\ndf.head(4).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n100191767\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\nSentul Point Suite Apartment\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n2020.0\n\n\nmonthly_rent_rm\n4200\n2300\n1000\n1700\n\n\nlocation\nDesa\nCheras\nDesa\nSentul\n\n\nproperty_type\nCondominium\nCondominium\nApartment\nApartment\n\n\nrooms\n5\n3\n3\n2\n\n\nparking\n2.0\n1.0\nNaN\n1.0\n\n\nbathroom\n6.0\n2.0\n2.0\n2.0\n\n\nsize_sqft\n1842\n1170\n650\n743\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\nParking, Playground, Swimming Pool, Squash Court, Security, Minimart, Gymnasium, Lift\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\nCooking Allowed, Near KTM/LRT, Washing Machine\n\n\nnearby_railways\nno\nyes\nNaN\nyes\n\n\n\n\n\n\n\nPlotting the difference between nearby KTM/LRT or not:\n\n\nCode\nsns.boxplot(data=df, x='monthly_rent_rm', y='nearby_railways')\nplt.xlim(0,4000);\n\nnear_ktmlrt = df.query(\" nearby_railways == 'yes' \")\nnot_near_ktmlrt = df.query(\" nearby_railways == 'no' \")\n\nprint(f\"\"\" \nMedian:\nNearby KTM/LRT: {near_ktmlrt.monthly_rent_rm.median():.0f}RM\nNot nearby KTM/LRT: {not_near_ktmlrt.monthly_rent_rm.median():.0f}RM\n      \"\"\")\n\n\n \nMedian:\nNearby KTM/LRT: 1650RM\nNot nearby KTM/LRT: 1600RM\n      \n\n\n\n\nFigure¬†1: Boxplot between Nearby KTM/LRT or Not\n\n\n\n\n\nSanity check:\n\n\nCode\ndf[df['prop_name'] == 'Majestic Maxim'][['nearby_railways']].value_counts()\n\n\nnearby_railways\nyes                166\nno                  24\ndtype: int64\n\n\nAs seen above, Figure¬†1 shows that it sligthly increases the median monthly rent by 50RM. However, near KTM/LRT is not appearing in all row even though the property is the same\nConclusion: Near KTM/LRT may be used, but it can be improved as the listing is inconsistent\n\n\n\n\n\nCode\ndf.isna().sum()\n\n\nads_id                      0\nprop_name                 508\ncompletion_year          4373\nmonthly_rent_rm             0\nlocation                    0\nproperty_type               0\nrooms                       2\nparking                  2630\nbathroom                    2\nsize_sqft                   0\nfurnished                   1\nfacilities                895\nadditional_facilities    2831\nnearby_railways          2831\ndtype: int64\n\n\n\n\nCode\n#dropping some columns\ndf = df.drop(columns=[\n    'ads_id', \n    'prop_name', \n    'facilities', \n    'additional_facilities',\n    # 'nearby_railways',\n    # 'completion_year'\n])\ndf\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9991 rows √ó 10 columns\n\n\n\n\n\nCode\n#checking dtypes from all columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9991 entries, 0 to 9990\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   completion_year  5618 non-null   float64\n 1   monthly_rent_rm  9991 non-null   int64  \n 2   location         9991 non-null   object \n 3   property_type    9991 non-null   object \n 4   rooms            9989 non-null   object \n 5   parking          7361 non-null   float64\n 6   bathroom         9989 non-null   float64\n 7   size_sqft        9991 non-null   int64  \n 8   furnished        9990 non-null   object \n 9   nearby_railways  7160 non-null   object \ndtypes: float64(3), int64(2), object(5)\nmemory usage: 780.7+ KB\n\n\n\n\nCode\n#converting rooms from object to int64\ndf['rooms'] = pd.to_numeric(df['rooms'], downcast='integer', errors='coerce')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9991 entries, 0 to 9990\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   completion_year  5618 non-null   float64\n 1   monthly_rent_rm  9991 non-null   int64  \n 2   location         9991 non-null   object \n 3   property_type    9991 non-null   object \n 4   rooms            9987 non-null   float64\n 5   parking          7361 non-null   float64\n 6   bathroom         9989 non-null   float64\n 7   size_sqft        9991 non-null   int64  \n 8   furnished        9990 non-null   object \n 9   nearby_railways  7160 non-null   object \ndtypes: float64(4), int64(2), object(4)\nmemory usage: 780.7+ KB\n\n\n\n\n\nTo remove some unexplainable data such as 0 monthly rent, 0 size, the rent that is way too old (1970), including the monthly rent that is way too high and/or size too big.\n\n\nCode\ndf[['size_sqft', 'monthly_rent_rm']].plot(kind='scatter', x='size_sqft', y='monthly_rent_rm');\nplt.ylim(100,5500) #batas harga rent\nplt.xlim(50,3000)  #batas size\nplt.show()\n\n\n\n\nFigure¬†2: Monthly Rent\n\n\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1,2)\naxs[0].boxplot(data=df, x='monthly_rent_rm')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=df, x='monthly_rent_rm')\naxs[1].set_ylim(0,5000)\naxs[1].set_title('croped at 5,000 RM')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†3: Comparison between Different Scale\n\n\n\n\n\nBased on EDA on Figure¬†2 and Figure¬†3, author decided to filter the data between 100-5500 RM as follows:\n\n\nCode\n#removing all rows with monthly rent above 5500 RM and below 100RM\ndfx = df.query(\" monthly_rent_rm &gt; 100 & monthly_rent_rm &lt; 5500 \")\ndfx.describe()\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nrooms\nparking\nbathroom\nsize_sqft\n\n\n\n\ncount\n5530.000000\n9841.000000\n9838.000000\n7245.000000\n9840.000000\n9.841000e+03\n\n\nmean\n2014.863110\n1786.840260\n2.742427\n1.339268\n1.928760\n1.111279e+04\n\n\nstd\n7.436904\n768.813626\n0.763700\n0.517512\n0.517118\n1.008037e+06\n\n\nmin\n1980.000000\n110.000000\n1.000000\n1.000000\n1.000000\n1.000000e+00\n\n\n25%\n2011.000000\n1300.000000\n2.000000\n1.000000\n2.000000\n8.000000e+02\n\n\n50%\n2017.000000\n1600.000000\n3.000000\n1.000000\n2.000000\n9.080000e+02\n\n\n75%\n2020.000000\n2100.000000\n3.000000\n2.000000\n2.000000\n1.087000e+03\n\n\nmax\n2025.000000\n5300.000000\n9.000000\n10.000000\n6.000000\n1.000000e+08\n\n\n\n\n\n\n\nSanity check after removal as shown in Figure¬†4 belo:\n\n\nCode\ndfx.monthly_rent_rm.plot(kind='box', x='monthly_rent_rm');\n\n\n\n\nFigure¬†4: Data after Outlier Removal\n\n\n\n\n\n\n\n\n\nChecking the dataset in terms of size.\n\n\nCode\nfig, axs = plt.subplots(1,2)\naxs[0].boxplot(data=dfx, x='size_sqft')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=dfx, x='size_sqft')\naxs[1].set_ylim(0,2000)\naxs[1].set_title('croped at 0-2,000 square feet')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†5: Raw Data Size sq.ft\n\n\n\n\n\nStill based on Figure¬†2, outliers are removed.\n\n\nCode\n#removing outliers below 500, and higher than 3000 sqft and below 50 sqft\ndfx = \\\n(dfx.query(\" size_sqft &gt; 50 & size_sqft &lt; 3000 \")\n # .size_sqft\n # .plot(kind='box')\n)\ndfx\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 10 columns\n\n\n\nSanity check:\n\n\nCode\ndfx.size_sqft.plot(kind='box');\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1,5, figsize=(12,4))\naxs[0].boxplot(data=dfx.dropna(), x='size_sqft')\naxs[1].boxplot(data=dfx.dropna(), x='rooms')\naxs[2].boxplot(data=dfx.dropna(), x='parking')\naxs[3].boxplot(data=dfx.dropna(), x='bathroom')\n# axs[4].boxplot(data=dfx.dropna(), x='completion_year')\n\naxs[0].set_title('Size')\naxs[1].set_title('Rooms')\naxs[2].set_title('Parking')\naxs[3].set_title('Bathrooms')\n# axs[4].set_title('Completion Year')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†6: Final Data after Outlier Removal\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef extractInputOutput(data,\n                       output_column_name):\n    \"\"\"\n    Fungsi untuk memisahkan data input dan output\n    :param data: &lt;pandas dataframe&gt; data seluruh sample\n    :param output_column_name: &lt;string&gt; nama kolom output\n    :return input_data: &lt;pandas dataframe&gt; data input\n    :return output_data: &lt;pandas series&gt; data output\n    \"\"\"\n    output_data = data[output_column_name]\n    input_data = data.drop(output_column_name,\n                           axis = 1)\n    \n    return input_data, output_data\n\n\n\n\nCode\nX, y = extractInputOutput(data=dfx, output_column_name='monthly_rent_rm')\n\n\n\n\nCode\nX\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 9 columns\n\n\n\n\n\nCode\ny\n\n\n0       4200\n1       2300\n2       1000\n3       1700\n4       1299\n        ... \n9986    1400\n9987    1000\n9988    1488\n9989    2000\n9990    2000\nName: monthly_rent_rm, Length: 9822, dtype: int64\n\n\n\n\n\n\n\nCode\n#import libraries\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state = 123)\n\n\n\n\nCode\n#sanity check\nlen(X_test)/len(X)\n\n\n0.20006108735491754\n\n\n\n\nCode\n#sanity check\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n5978\n2020.0\nSentul\nApartment\n3.0\n1.0\n2.0\n876\nPartially Furnished\nyes\n\n\n2151\n2022.0\nSetapak\nCondominium\n3.0\n1.0\n2.0\n850\nFully Furnished\nyes\n\n\n9714\n2002.0\nCheras\nCondominium\n3.0\nNaN\n2.0\n1000\nFully Furnished\nyes\n\n\n8556\n2021.0\nCheras\nService Residence\n2.0\n1.0\n2.0\n680\nPartially Furnished\nyes\n\n\n2809\n2005.0\nCheras\nCondominium\n3.0\n1.0\n2.0\n920\nPartially Furnished\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9954\n2016.0\nBesi\nService Residence\n3.0\n2.0\n2.0\n1121\nPartially Furnished\nyes\n\n\n7901\nNaN\nJalil\nFlat\n3.0\nNaN\n2.0\n650\nNot Furnished\nyes\n\n\n5322\n2013.0\nKepong\nCondominium\n3.0\n2.0\n2.0\n1378\nFully Furnished\nno\n\n\n1363\nNaN\nDesa\nCondominium\n3.0\n1.0\n2.0\n950\nPartially Furnished\nNaN\n\n\n3648\n2019.0\nKLCC\nService Residence\n2.0\n1.0\n2.0\n796\nFully Furnished\nNaN\n\n\n\n\n7857 rows √ó 9 columns\n\n\n\nPreprocessing Original Data for Categorical Dtypes\nOne must paying attention to the number of categorical observation in the original data, with respect to the sampling train-test value. If, the test_size = 0.3, that means any categorical observation with a total of 3 and less, would not be distributed evenly among train and test data.\n\n\nCode\nprint(dfx.location.nunique())\nprint(X_train.location.nunique())\nprint(X_test.location.nunique())\n\n\n53\n53\n50\n\n\n\n\nCode\nprint(dfx.property_type.nunique())\nprint(X_train.property_type.nunique())\nprint(X_test.property_type.nunique())\n\n\n9\n9\n8\n\n\n\n\nCode\nprint(set(X_train.furnished.to_list()) - set(X_test.furnished.to_list()))\nprint(set(X_train.location.to_list()) - set(X_test.location.to_list()))\nprint(set(X_train.property_type.to_list()) - set(X_test.property_type.to_list()))\nprint(set(X_train.nearby_railways.to_list()) - set(X_test.nearby_railways.to_list()))\n\n\nset()\n{'Sentral', 'Lin', 'Penchala'}\n{'Condo / Services residence / Penthouse / Townhouse'}\nset()\n\n\n\nDropping Data\n\n\n\nCode\ndfx\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 10 columns\n\n\n\n\n\nCode\ndfx.location.value_counts()\n\n\nCheras         1614\nSetapak         965\nSentul          776\nKepong          662\nJalil           577\nMaju            456\nAmpang          327\nKeramat         300\nRoad            298\nDesa            295\nCity            268\nKiara           264\nKLCC            239\nIpoh            224\nLama            193\nSegambut        178\nPetaling        174\nPandan          173\nBesi            173\nKuching         168\nSouth           143\nPantai          113\nBintang          99\nMelawati         91\nTitiwangsa       83\nHilir            78\nHartamas         75\nDamansara        73\nOUG              63\nIsmail           59\nDutamas          57\nGombak           56\nPerdana          53\nSetiawangsa      50\nParkCity         50\nBangsar          47\nMenjalara        45\nSeputeh          35\nPuchong          33\nIndah            29\nCentre           25\nJaya             24\nBrickfields      24\nPudu             24\nSelatan          19\nHeights          18\nJinjang           9\nSerdang           9\nSentral           5\nOthers            5\nTunku             2\nPenchala          1\nLin               1\nName: location, dtype: int64\n\n\n\n\nCode\ndfx.property_type.value_counts()\n\n\nCondominium                                           4698\nService Residence                                     2647\nApartment                                             2123\nFlat                                                   265\nStudio                                                  27\nDuplex                                                  27\nOthers                                                  27\nTownhouse Condo                                          7\nCondo / Services residence / Penthouse / Townhouse       1\nName: property_type, dtype: int64\n\n\n\n\nCode\ndfx_new = dfx[\n    (dfx.location != 'Jinjang') \n    & (dfx.location != 'Serdang') & \n    (dfx.location != 'Sentral') & \n    (dfx.location != 'Others') & \n    (dfx.location != 'Tunku') & \n    (dfx.location != 'Penchala') & \n    (dfx.location != 'Lin') &\n    # (dfx.property_type != 'Others') &\n    (dfx.property_type != 'Condo / Services residence / Penthouse / Townhouse') &\n    (dfx.property_type != 'Townhouse Condo')\n]\n\n\n\n\nCode\ndfx_new.property_type.value_counts()\n\n\nCondominium          4683\nService Residence    2642\nApartment            2115\nFlat                  263\nDuplex                 27\nStudio                 26\nOthers                 26\nName: property_type, dtype: int64\n\n\n\nRe-split Training-Test\n\n\n\nCode\nX, y = extractInputOutput(data=dfx_new, output_column_name='monthly_rent_rm')\n\n\n\n\nCode\n#import libraries\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state = 123)\n\n\n\n\nCode\n#sanity check\nlen(X_test)/len(X)\n\n\n0.2000613371498671\n\n\n\n\nCode\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n8649\nNaN\nMaju\nCondominium\n2.0\n1.0\n1.0\n800\nFully Furnished\nyes\n\n\n9112\n1993.0\nBangsar\nCondominium\n2.0\n1.0\n1.0\n890\nFully Furnished\nyes\n\n\n1472\nNaN\nJalil\nCondominium\n1.0\nNaN\n1.0\n1200\nFully Furnished\nyes\n\n\n5536\nNaN\nCheras\nCondominium\n3.0\n2.0\n2.0\n893\nFully Furnished\nno\n\n\n8152\nNaN\nPudu\nApartment\n3.0\nNaN\n2.0\n980\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\nAmpang\nApartment\n3.0\nNaN\n2.0\n828\nPartially Furnished\nNaN\n\n\n7927\nNaN\nSouth\nFlat\n3.0\nNaN\n2.0\n750\nNot Furnished\nNaN\n\n\n5340\n2023.0\nCheras\nCondominium\n4.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n1369\nNaN\nKLCC\nCondominium\n1.0\nNaN\n1.0\n473\nFully Furnished\nyes\n\n\n3659\n2017.0\nRoad\nService Residence\n3.0\nNaN\n2.0\n953\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 9 columns\n\n\n\n\n\nCode\nprint(set(X_train.furnished.to_list()) - set(X_test.furnished.to_list()))\nprint(set(X_train.location.to_list()) - set(X_test.location.to_list()))\nprint(set(X_train.property_type.to_list()) - set(X_test.property_type.to_list()))\n# print(set(X_train.nearby_railways.to_list()) - set(X_test.nearby_railways.to_list()))\n\n\nset()\nset()\nset()\n\n\n\n\nCode\nprint(dfx_new.location.nunique())\nprint(X_train.location.nunique())\nprint(X_test.location.nunique())\n\n\n46\n46\n46\n\n\n\n\nCode\n#sanity check\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n8649\nNaN\nMaju\nCondominium\n2.0\n1.0\n1.0\n800\nFully Furnished\nyes\n\n\n9112\n1993.0\nBangsar\nCondominium\n2.0\n1.0\n1.0\n890\nFully Furnished\nyes\n\n\n1472\nNaN\nJalil\nCondominium\n1.0\nNaN\n1.0\n1200\nFully Furnished\nyes\n\n\n5536\nNaN\nCheras\nCondominium\n3.0\n2.0\n2.0\n893\nFully Furnished\nno\n\n\n8152\nNaN\nPudu\nApartment\n3.0\nNaN\n2.0\n980\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\nAmpang\nApartment\n3.0\nNaN\n2.0\n828\nPartially Furnished\nNaN\n\n\n7927\nNaN\nSouth\nFlat\n3.0\nNaN\n2.0\n750\nNot Furnished\nNaN\n\n\n5340\n2023.0\nCheras\nCondominium\n4.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n1369\nNaN\nKLCC\nCondominium\n1.0\nNaN\n1.0\n473\nFully Furnished\nyes\n\n\n3659\n2017.0\nRoad\nService Residence\n3.0\nNaN\n2.0\n953\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 9 columns\n\n\n\n\n\nCode\n#export data training\nX_train.to_csv('X_train.csv', index=False)\ny_train.to_csv('y_train.csv', index=False)\n\n\n\n\nCode\n#export data testing\nX_test.to_csv('X_test.csv', index=False)\ny_test.to_csv('y_test.csv', index=False)\n\n\n\n\n\n\n\n\nCode\n#checking null data\nX_train.isna().sum()\n\n\ncompletion_year    3438\nlocation              0\nproperty_type         0\nrooms                 2\nparking            2074\nbathroom              0\nsize_sqft             0\nfurnished             0\nnearby_railways    2206\ndtype: int64\n\n\n\n\n\n\nCode\nX_train_num =  X_train.select_dtypes(exclude='object')\nX_train_num\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\n\n\n\n\n8649\nNaN\n2.0\n1.0\n1.0\n800\n\n\n9112\n1993.0\n2.0\n1.0\n1.0\n890\n\n\n1472\nNaN\n1.0\nNaN\n1.0\n1200\n\n\n5536\nNaN\n3.0\n2.0\n2.0\n893\n\n\n8152\nNaN\n3.0\nNaN\n2.0\n980\n\n\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\n3.0\nNaN\n2.0\n828\n\n\n7927\nNaN\n3.0\nNaN\n2.0\n750\n\n\n5340\n2023.0\n4.0\n2.0\n2.0\n1000\n\n\n1369\nNaN\n1.0\nNaN\n1.0\n473\n\n\n3659\n2017.0\n3.0\nNaN\n2.0\n953\n\n\n\n\n7825 rows √ó 5 columns\n\n\n\n\n\nCode\nX_train_num.isna().sum()\n\n\ncompletion_year    3438\nrooms                 2\nparking            2074\nbathroom              0\nsize_sqft             0\ndtype: int64\n\n\n\nWe can fill completion year, rooms, parking and bathroom with mode\n\n\n\nCode\nfrom sklearn.impute import SimpleImputer\n\ndef numericalImputation(X_train_num, strategy = 'most_frequent'):\n    \"\"\"\n    Fungsi untuk melakukan imputasi data numerik NaN\n    :param data: &lt;pandas dataframe&gt; sample data input\n\n    :return X_train_numerical: &lt;pandas dataframe&gt; data numerik\n    :return imputer_numerical: numerical imputer method\n    \"\"\"\n    #buat imputer\n    imputer_num = SimpleImputer(missing_values = np.nan, strategy = strategy)\n    \n    #fitting\n    imputer_num.fit(X_train_num)\n\n    # transform\n    imputed_data = imputer_num.transform(X_train_num)\n    X_train_num_imputed = pd.DataFrame(imputed_data)\n\n    #pastikan index dan nama kolom antara imputed dan non-imputed SAMA\n    X_train_num_imputed.columns = X_train_num.columns\n    X_train_num_imputed.index = X_train_num.index\n\n    return X_train_num_imputed, imputer_num\n\n\n\n\nCode\nX_train_num, imputer_num = numericalImputation(X_train_num, strategy='most_frequent')\nX_train_num.isna().sum()\n\n\ncompletion_year    0\nrooms              0\nparking            0\nbathroom           0\nsize_sqft          0\ndtype: int64\n\n\n\n\nCode\nimputer_num\n\n\nSimpleImputer(strategy='most_frequent')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer(strategy='most_frequent')\n\n\n\n\n\n\n\nCode\nX_train_cat = X_train.select_dtypes(include='object')\nX_train_cat\n\n\n\n\n\n\n\n\n\nlocation\nproperty_type\nfurnished\nnearby_railways\n\n\n\n\n8649\nMaju\nCondominium\nFully Furnished\nyes\n\n\n9112\nBangsar\nCondominium\nFully Furnished\nyes\n\n\n1472\nJalil\nCondominium\nFully Furnished\nyes\n\n\n5536\nCheras\nCondominium\nFully Furnished\nno\n\n\n8152\nPudu\nApartment\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n\n\n7541\nAmpang\nApartment\nPartially Furnished\nNaN\n\n\n7927\nSouth\nFlat\nNot Furnished\nNaN\n\n\n5340\nCheras\nCondominium\nPartially Furnished\nyes\n\n\n1369\nKLCC\nCondominium\nFully Furnished\nyes\n\n\n3659\nRoad\nService Residence\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 4 columns\n\n\n\n\n\nCode\nX_train_cat.isna().sum()\n\n\nlocation              0\nproperty_type         0\nfurnished             0\nnearby_railways    2206\ndtype: int64\n\n\n\nImpute with mode\n\n\n\nCode\nX_train_cat, imputer_num = numericalImputation(X_train_cat, strategy='most_frequent')\nX_train_cat.isna().sum()\n\n\nlocation           0\nproperty_type      0\nfurnished          0\nnearby_railways    0\ndtype: int64\n\n\n\n\n\n\n\nCode\nX_train_cat_ohe =  pd.get_dummies(X_train_cat)\nX_train_cat_ohe.head(2)\n\n\n\n\n\n\n\n\n\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\nlocation_Centre\nlocation_Cheras\nlocation_City\nlocation_Damansara\nlocation_Desa\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n9112\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 58 columns\n\n\n\n\n\nCode\nohe_columns = X_train_cat_ohe.columns\nohe_columns\n\n\nIndex(['location_Ampang', 'location_Bangsar', 'location_Besi',\n       'location_Bintang', 'location_Brickfields', 'location_Centre',\n       'location_Cheras', 'location_City', 'location_Damansara',\n       'location_Desa', 'location_Dutamas', 'location_Gombak',\n       'location_Hartamas', 'location_Heights', 'location_Hilir',\n       'location_Indah', 'location_Ipoh', 'location_Ismail', 'location_Jalil',\n       'location_Jaya', 'location_KLCC', 'location_Kepong', 'location_Keramat',\n       'location_Kiara', 'location_Kuching', 'location_Lama', 'location_Maju',\n       'location_Melawati', 'location_Menjalara', 'location_OUG',\n       'location_Pandan', 'location_Pantai', 'location_ParkCity',\n       'location_Perdana', 'location_Petaling', 'location_Puchong',\n       'location_Pudu', 'location_Road', 'location_Segambut',\n       'location_Selatan', 'location_Sentul', 'location_Seputeh',\n       'location_Setapak', 'location_Setiawangsa', 'location_South',\n       'location_Titiwangsa', 'property_type_Apartment',\n       'property_type_Condominium', 'property_type_Duplex',\n       'property_type_Flat', 'property_type_Others',\n       'property_type_Service Residence', 'property_type_Studio',\n       'furnished_Fully Furnished', 'furnished_Not Furnished',\n       'furnished_Partially Furnished', 'nearby_railways_no',\n       'nearby_railways_yes'],\n      dtype='object')\n\n\n\n\nCode\nX_train_cat_ohe.isna().sum()\n\n\nlocation_Ampang                    0\nlocation_Bangsar                   0\nlocation_Besi                      0\nlocation_Bintang                   0\nlocation_Brickfields               0\nlocation_Centre                    0\nlocation_Cheras                    0\nlocation_City                      0\nlocation_Damansara                 0\nlocation_Desa                      0\nlocation_Dutamas                   0\nlocation_Gombak                    0\nlocation_Hartamas                  0\nlocation_Heights                   0\nlocation_Hilir                     0\nlocation_Indah                     0\nlocation_Ipoh                      0\nlocation_Ismail                    0\nlocation_Jalil                     0\nlocation_Jaya                      0\nlocation_KLCC                      0\nlocation_Kepong                    0\nlocation_Keramat                   0\nlocation_Kiara                     0\nlocation_Kuching                   0\nlocation_Lama                      0\nlocation_Maju                      0\nlocation_Melawati                  0\nlocation_Menjalara                 0\nlocation_OUG                       0\nlocation_Pandan                    0\nlocation_Pantai                    0\nlocation_ParkCity                  0\nlocation_Perdana                   0\nlocation_Petaling                  0\nlocation_Puchong                   0\nlocation_Pudu                      0\nlocation_Road                      0\nlocation_Segambut                  0\nlocation_Selatan                   0\nlocation_Sentul                    0\nlocation_Seputeh                   0\nlocation_Setapak                   0\nlocation_Setiawangsa               0\nlocation_South                     0\nlocation_Titiwangsa                0\nproperty_type_Apartment            0\nproperty_type_Condominium          0\nproperty_type_Duplex               0\nproperty_type_Flat                 0\nproperty_type_Others               0\nproperty_type_Service Residence    0\nproperty_type_Studio               0\nfurnished_Fully Furnished          0\nfurnished_Not Furnished            0\nfurnished_Partially Furnished      0\nnearby_railways_no                 0\nnearby_railways_yes                0\ndtype: int64\n\n\n\n\nCode\nX_train_num.isna().sum()\n\n\ncompletion_year    0\nrooms              0\nparking            0\nbathroom           0\nsize_sqft          0\ndtype: int64\n\n\n\n\n\n\n\nCode\nX_train_concat = pd.concat([X_train_num,\n                            X_train_cat_ohe],\n                           axis = 1)\n\n\n\n\nCode\nX_train_concat.head(2)\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n2021.0\n2.0\n1.0\n1.0\n800.0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n9112\n1993.0\n2.0\n1.0\n1.0\n890.0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 63 columns\n\n\n\n\n\nCode\n#sanity check\nX_train_concat.isnull().sum()\n\n\ncompletion_year                  0\nrooms                            0\nparking                          0\nbathroom                         0\nsize_sqft                        0\n                                ..\nfurnished_Fully Furnished        0\nfurnished_Not Furnished          0\nfurnished_Partially Furnished    0\nnearby_railways_no               0\nnearby_railways_yes              0\nLength: 63, dtype: int64\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Buat fungsi\ndef standardizerData(data):\n    \"\"\"\n    Fungsi untuk melakukan standarisasi data\n    :param data: &lt;pandas dataframe&gt; sampel data\n    :return standardized_data: &lt;pandas dataframe&gt; sampel data standard\n    :return standardizer: method untuk standardisasi data\n    \"\"\"\n    data_columns = data.columns  # agar nama kolom tidak hilang\n    data_index = data.index  # agar index tidak hilang\n\n    # buat (fit) standardizer\n    standardizer = StandardScaler()\n    standardizer.fit(data)\n\n    # transform data\n    standardized_data_raw = standardizer.transform(data)\n    standardized_data = pd.DataFrame(standardized_data_raw)\n    standardized_data.columns = data_columns\n    standardized_data.index = data_index\n\n    return standardized_data, standardizer\n\n\n\n\nCode\nX_train_clean, standardizer = standardizerData(data = X_train_concat)\n\n\n\n\nCode\nX_train_clean.head()\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n0.541336\n-0.971807\n-0.542258\n-1.813058\n-0.530030\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n9112\n-3.913712\n-0.971807\n-0.542258\n-1.813058\n-0.190258\n-0.189407\n14.508152\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n1472\n0.541336\n-2.288585\n-0.542258\n-1.813058\n0.980069\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n5536\n0.541336\n0.344970\n1.685126\n0.150837\n-0.178932\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n1.614308\n-1.614308\n\n\n8152\n0.541336\n0.344970\n-0.542258\n0.150837\n0.149515\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n-0.932137\n-0.406146\n1.241535\n-0.619460\n0.619460\n\n\n\n\n5 rows √ó 63 columns\n\n\n\n\n\n\n\nSince this is a regression model, R2 score and mean absolute error (MAE) will be used as a performance metrics.\nThe machine learning model will use baseline from average value of the target columns (monthly rent) and also result from linear regression model. After that, author used some of the recommended model based on previous works, which are random forest and gradient boosting to better improve the performance of the model.\n\n\nThe concept here is to use average value of the target as the easiest way to predict the monhtly rent of a unit.\n\n\nCode\ny_baseline = np.ones(len(y_train)) * y_train.mean()\ny_baseline\n\n\narray([1780.0086901, 1780.0086901, 1780.0086901, ..., 1780.0086901,\n       1780.0086901, 1780.0086901])\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Train the linear regression model\nlin_reg = LinearRegression().fit(X_train_clean, y_train)\n\n# Predict using the train data\ny_pred_train = y_baseline\n\n# Calculate R-squared\nr2_baseline = r2_score(y_train, y_pred_train)\n\n#calculate MAE\nmae_baseline = mean_absolute_error(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_baseline:.4f} and MAE score: {mae_baseline:.4f}\")\n\n\nR2-score: 0.0000 and MAE score: 562.3710\n\n\n\n\nCode\nplt.scatter(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe second method is using linear regression, which simply put is finding the minum total error (distance) between predicted value and the target value, using linear equation.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Train the linear regression model\nlin_reg = LinearRegression().fit(X_train_clean, y_train)\n\n# Predict using the train data\n# y_pred = y_baseline\ny_pred_train = lin_reg.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_linreg = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_linreg = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_linreg:.4f} and MAE score: {mae_linreg:.4f}\")\n\n\nR2-score: 0.6468 and MAE score: 319.2229\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe gradient boosting, is one of the recommendation from previous works, is a model where each sample would be given a different weights (boosts) depending on its performance in predicting the value/ target.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Build random forest\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\n\n\n\nCode\n# Fit random forest\ngrad_tree.fit(X_train_clean, y_train)\n\n\nGradientBoostingRegressor(random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = grad_tree.predict(X_train_clean)\n# y_pred_test = grad_tree.predict(X_test_clean)\n\n# Calculate mean absolute error\nmae_gb = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_gb = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_gb:.4f} and MAE score: {mae_gb:.4f}\")\n\n\nR2-score: 0.7246 and MAE score: 281.6835\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\n#gridsearch\n\nfrom sklearn.model_selection import GridSearchCV \n\n\nparams = {'n_estimators': [100, 200, 300, 400, 500],\n              'learning_rate': [0.1, 0.05, 0.01]}\n\n# Buat gridsearch\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\ngrad_tree_cv = GridSearchCV(estimator = grad_tree,\n                           param_grid = params,\n                           cv = 5,\n                           scoring = \"neg_mean_absolute_error\")\n\n\n\n\nCode\n# Fit grid search cv\ngrad_tree_cv.fit(X_train_clean, y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=123),\n             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             scoring='neg_mean_absolute_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=123),\n             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             scoring='neg_mean_absolute_error')estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=123)GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\n\nCode\n# Best params\ngrad_tree_cv.best_params_\n\n\n{'learning_rate': 0.1, 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the Adaboost\ngrad_tree = GradientBoostingRegressor(n_estimators = grad_tree_cv.best_params_[\"n_estimators\"],\n                                      random_state = 123)\n\ngrad_tree.fit(X_train_clean, y_train)\n\n\nGradientBoostingRegressor(n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = grad_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_gb_cv = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_gb_cv = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_gb_cv:.4f} and MAE score: {mae_gb_cv:.4f}\")\n\n\nR2-score: 0.8194 and MAE score: 228.0225\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe last model, which was also recommended by previous works, is a model where not only it has weights based on its performance, but the feature selection in which the sample is measured was done at random. Therefore, reduces not only the variance, but also the bias.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n\nCode\n# Build random forest\nrf_tree = RandomForestRegressor(n_estimators = 100,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n\n\n\nCode\n# Fit random forest\nrf_tree.fit(X_train_clean, y_train)\n\n\nRandomForestRegressor(max_features='sqrt', random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_rf = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf:.4f} and MAE score: {mae_rf:.4f}\")\n\n\nR2-score: 0.9577 and MAE score: 100.8408\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\nparams = {\"n_estimators\": [100, 200, 300, 500 ],\n          \"max_features\": [\"sqrt\", \"log2\"]}\n\n# Buat gridsearch\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                random_state = 123)\n\nrf_tree_cv = GridSearchCV(estimator = rf_tree,\n                          param_grid = params,\n                          cv = 5,\n                          scoring = \"neg_mean_absolute_error\")\n\n\n\n\nCode\n# Fit grid search cv\nrf_tree_cv.fit(X_train_clean, y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=123),\n             param_grid={'max_features': ['sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 500]},\n             scoring='neg_mean_absolute_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=123),\n             param_grid={'max_features': ['sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 500]},\n             scoring='neg_mean_absolute_error')estimator: RandomForestRegressorRandomForestRegressor(random_state=123)RandomForestRegressorRandomForestRegressor(random_state=123)\n\n\n\n\nCode\n# Best params\nrf_tree_cv.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the Random Forest\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                max_features = rf_tree_cv.best_params_[\"max_features\"],\n                                n_estimators = rf_tree_cv.best_params_[\"n_estimators\"],\n                                random_state = 123)\n\nrf_tree.fit(X_train_clean, y_train)\n\n\nRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf_cv = mean_absolute_error(y_train, y_pred_train)\n\n# # Calculate R-squared\nr2_rf_cv = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf_cv:.4f} and MAE score: {mae_rf_cv:.4f}\")\n\n\nR2-score: 0.9585 and MAE score: 99.7989\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\nmae_score = [mae_baseline, mae_linreg, mae_gb, mae_gb_cv, mae_rf, mae_rf_cv]\nr2_score = [r2_baseline, r2_linreg, r2_gb, r2_gb_cv, r2_rf, r2_rf_cv]\nindexes = [\"baseline\", \"linear regression\", \"gradient boosting\", \"gradient boosting with CV\", \"random forest\",  \"random forest with CV\"]\n\nsummary_df = pd.DataFrame({\n    \"MAE Train\": mae_score,\n    \"R2-Score\": r2_score,\n},index = indexes)\n\nsummary_df.sort_values(by='R2-Score', ascending=False)\n\n\n\n\n\n\n\n\n\nMAE Train\nR2-Score\n\n\n\n\nrandom forest with CV\n99.798879\n0.958519\n\n\nrandom forest\n100.840759\n0.957661\n\n\ngradient boosting with CV\n228.022510\n0.819416\n\n\ngradient boosting\n281.683477\n0.724601\n\n\nlinear regression\n319.222873\n0.646780\n\n\nbaseline\n562.370983\n0.000000\n\n\n\n\n\n\n\nFrom the above table, it can be seen that Random Forest model performs the best, and Gradient Boosting at the second place. This is similar to the previous work done by others, on house pricing.\n\n\n\n\n\n\nCode\n# libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n#setting up\nrf_tree = RandomForestRegressor(n_estimators = 500,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n#fit model train\nrf_tree.fit(X_train_clean, y_train)\n\n# Predict model train\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf_cv_train = mean_absolute_error(y_train, y_pred_train)\n\n# # Calculate R-squared\nr2_rf_cv_train = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf_cv_train:.3f} and MAE score: +/-{mae_rf_cv_train:.2f} RM\")\n\nsns.scatterplot(x=y_train, y=y_pred_train )\nplt.plot([0, 5500], [0,5500], \"--r\")\nplt.xlim(0, 5500)\nplt.xlabel(\"Actual Monthly Rent\")\nplt.ylim(0,5500)\nplt.ylabel(\"Predicted Monthly Rent\")\nplt.suptitle(\"Random Forest - Best Regression Model\")\nplt.show()\n\n\nR2-score: 0.959 and MAE score: +/-99.80 RM"
  },
  {
    "objectID": "posts/005-webscraping-machinelearning-rent-pricing/index.html#machine-learning-project",
    "href": "posts/005-webscraping-machinelearning-rent-pricing/index.html#machine-learning-project",
    "title": "Malaysia Property Pricing - Webscraping & Machine Learning Model",
    "section": "",
    "text": "Pacmann Batch 8 Capstone by Aditya Arie Wijaya (aditya-66kK)\n\n\nThis is a machine learning project to predict unit/property monthly rent price in Kuala Lumpur region, Malaysia. The project uses a dataset from an online ads listing for property mudah.my. This project outlines the process of web-scraping/ data gathering, data cleaning-wrangling, and machine learning modeling.\nThis project aims to answers question about how much a unit monthly rent would be if given information such as location, number of bedrooms, parking, furnished, etc? This would help potential tenant and also the owner to get the best price of their rental unit, comparable to the market value.\nSome previous work about house pricing was listed below, however most of them are targeting a dataset of house pricing or an Airbnb pricing. There are difference such as in Airbnb, the booking rarely took more than 2 weeks, let alone a year. Therefore the pricing may be different. Additionally, in Airbnb, there is text feature coming from the review given by the tenant and the owner.The better the review, the higher the rent prices ‚Äì which was not available in this current project dataset.\nPrevious work by (Madhuri, Anuradha, and Pujitha 2019), (Xu and Nguyen 2022), (Zhao et al. 2022) highlight the importance feature selection, and the choice of machine learning model. Based on the previous works, the most consistently performed machine learning model are Random Forest and Gradient boosting, and the MAE and R2 score usually used in evaluating the performance of the model. Although the above work are all not about apartment rent pricing, similar method can be applied to this project.\n\n\n\nThe data will use a scraped data from the website mentioned before, focusing on property-to-rent surrounding Kuala Lumpur, Malaysia. \n\n\nCode\n#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('max_colwidth', 200)\nfrom bs4 import BeautifulSoup as bs\nimport requests\nimport re\nimport time\nimport datetime\nimport smtplib\nimport json\nfrom tqdm.notebook import tqdm, trange\nimport time    # to be used in loop iterations\n\n!jupyter nbextension enable --py widgetsnbextension #enabling progress bar\n\n\nEnabling notebook extension jupyter-js-widgets/extension...\n      - Validating: problems found:\n        - require?  X jupyter-js-widgets/extension\n\n\n\n\nThe process started out by gathering data from the website. We are using python library for web-scraping: BeautifulSoup as depicted below. The first process is generating a list of webpage address for a given page number.\n\n\nCode\n#generate list address of n_page\ndef page_number(start, end):\n    \"\"\"\n    Description:\n        Function to generate a list of webpage address for a given page number\n\n    Parameters:\n        start (int) : starting page number\n        end (int)   : ending page number\n    Returns:\n        a list of listing web address \n    \n    \"\"\"\n    \n    page_url = 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o='\n    list_page = []\n    for i in range(start,end+1):\n        list_page.append(page_url+str(i))\n    return list_page\npage_number(2,4)\n\n\n['https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=2',\n 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=3',\n 'https://www.mudah.my/kuala-lumpur/apartment-condominium-for-rent?o=4']\n\n\nThen generate a list of ads listing on a single page.\n\n\nCode\n#setting up list of page from \ndef get_list_html(page_url):\n    \"\"\"\n    Description:\n        Function to get every listing ads in a given url (page_url)\n\n    Parameters:\n        page_url (str): website url\n        \n    Returns:\n        a list of listing ads\n    \n    \"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n    page = requests.get(url=page_url, headers=headers)\n    soup = bs(page.text, \"html.parser\")\n\n    script_tag = soup.find('script', type='application/ld+json')\n    data = json.loads(script_tag.text)\n    dict_query = data[2]['itemListElement']\n\n    n_query = data[2]['numberOfItems']\n    list_html = []\n\n    for i in range(n_query):\n        link = data[2]['itemListElement'][i]['item']['url']\n        list_html.append(link)\n        \n    return list_html\n\n\n#getting listing property from the 1st-5th in the list\nget_list_html('https://www.mudah.my/neighbouring-kuala-lumpur/apartment-for-rent?o=2')[0:5]\n\n\n['https://www.mudah.my/Suria+Court+Mahkota+Cheras+-100450215.htm',\n 'https://www.mudah.my/Cemara+Apartment+Bandar+Sri+Permaisuri+Cheras+actual+pic-98750544.htm',\n 'https://www.mudah.my/Seasons+Garden+PV21+850sf+3R2B+Fully+Furnished+Actual+Pics+-100449939.htm',\n 'https://www.mudah.my/Pangsapuri+palma+puteri+with+fully+furnished+seksyen+6-92222541.htm',\n 'https://www.mudah.my/STUDENT+Danau+Perintis+Fully+Furnish+WIFI+PUNCAK+ALAM-100143070.htm']\n\n\nCombining the previous two functions, generate a list of url for all pages.\n\n\nCode\n#generate listing property from each page of n_page\ndef get_list_url(n_page):\n    \"\"\"\n    Description:\n        Function to get every listing ads in every page (n_page)\n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a list of listing ads\n    \n    \"\"\"\n    list_html=[]\n    for i in tqdm(range(n_page)):\n        list_html.extend(get_list_html(page_number(1, n_page)[i]))\n    return list_html\n\nget_list_url(2)[:10]\n\n\n\n\n\n['https://www.mudah.my/Pertiwi+Indah+1285sf+fully+furnished-98377671.htm',\n 'https://www.mudah.my/PPA1M+Kepong+2+Parking+FULLY+FURNISHED+-95301490.htm',\n 'https://www.mudah.my/PPA1M+Kepong+MARCH+2023+FULLY+FURNISHED+-98845611.htm',\n 'https://www.mudah.my/Ixora+Apartment+Kepong+yang+lengkap+dengan+Time+Internet+Access-100451938.htm',\n 'https://www.mudah.my/Pangsapuri+Permai+Sungai+Besi+3R2B+near+TBS+Bandar+Tasik+Selatan-98987533.htm',\n 'https://www.mudah.my/Inspirasi+Mont+Kiara+BRAND+NEW+RENOVATED+MID+FLOOR+UNIT+NICE+VIEW+-99491862.htm',\n 'https://www.mudah.my/Maxim+Residence+3+Room+3+Aircond+2+Car+Park+Walking+Distance+MRT-100451825.htm',\n 'https://www.mudah.my/Residensi+Harmoni+2+Cheapest+Rental+Near+Mont+Kiara+Sri+Hartamas+KL-100451718.htm',\n 'https://www.mudah.my/Angkasa+Condo+Taman+Connaught+Cheras+Walking+Distance+To+UCSI-100213869.htm',\n 'https://www.mudah.my/Maxim+Residence+Condo+Taman+Len+Cheras-99852286.htm']\n\n\nThen extract the attributes inside the listing ads in a form of nested dictionary.\n\n\nCode\n#extract data from url\ndef get_list_dict(n_page):\n    \"\"\"\n    Description:\n        Function to get dataset (atribut of units) in a form of dictionary. \n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a dictionaries of attributes inside a list\n    \n    \"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n    list_html = get_list_url(n_page)\n    list_dict = []\n    for url in tqdm(list_html):\n        try:\n            page = requests.get(url=url, headers=headers)\n            soup1 = bs(page.text, \"html.parser\")\n            soup2 = bs(soup1.prettify(), 'html.parser')\n\n            id_html = re.search(r'(\\d+).htm', url).group(1)\n            title = soup2.find(itemprop='name').text.strip()\n\n            script_tag = soup2.find(\"script\", id=\"__NEXT_DATA__\")\n            script_content = script_tag.text\n            data = json.loads(script_content)\n            props = data.get(\"props\", {})\n            id_listing = re.search(r'-(\\d+)\\.htm', url).group(1)\n\n            dict_id = [{'realValue': '', 'id': 'ads_id', 'value': id_listing, 'label': 'id ads'}]\n            dict_building = props[\"initialState\"][\"adDetails\"][\"byID\"][id_listing][\"attributes\"]['propertyParams'][2]['params']\n            dict_prop = props[\"initialState\"][\"adDetails\"][\"byID\"][id_listing][\"attributes\"]['categoryParams']\n            dict_unit = dict_id + dict_building + dict_prop\n        except:\n            None\n        \n        list_dict.append(dict_unit)\n        \n    return list_dict\n\n#sanity check\nget_list_dict(1)[1]\n\n\n\n\n\n\n\n\n[{'realValue': '', 'id': 'ads_id', 'value': '95301490', 'label': 'id ads'},\n {'realValue': 'PPA1M METROPOLITAN KEPONG',\n  'id': 'prop_name',\n  'value': 'PPA1M METROPOLITAN KEPONG',\n  'label': 'Building Name'},\n {'realValue': '', 'id': 'developer_name', 'value': '', 'label': 'Developer'},\n {'realValue': 'BLOK A PPA1M METROPOLITAN KEPONG MRR2, Kuala Lumpur, Kepong',\n  'id': 'full_address',\n  'value': 'BLOK A PPA1M METROPOLITAN KEPONG MRR2, Kuala Lumpur, Kepong',\n  'label': 'Address'},\n {'realValue': '',\n  'id': 'completion_year',\n  'value': '',\n  'label': 'Completion Year'},\n {'realValue': '', 'id': 'num_floors', 'value': '', 'label': '# of Floors'},\n {'realValue': '', 'id': 'num_units', 'value': '', 'label': 'Total Units'},\n {'realValue': 'RM 1 600 per month',\n  'id': 'monthly_rent',\n  'value': 'RM 1 600 per month',\n  'label': 'Monthly Rent'},\n {'realValue': '2020',\n  'id': 'category_id',\n  'value': 'Apartment / Condominium, For rent',\n  'label': 'Category'},\n {'realValue': '9',\n  'id': 'location',\n  'value': 'Kuala Lumpur - Kepong',\n  'label': 'Location'},\n {'realValue': '1',\n  'id': 'property_type',\n  'value': 'Condominium',\n  'label': 'Property Type'},\n {'realValue': '1',\n  'id': 'floor_range',\n  'value': 'High',\n  'label': 'Floor Range'},\n {'realValue': '5', 'id': 'rooms', 'value': '5', 'label': 'Bedrooms'},\n {'realValue': '2', 'id': 'bathroom', 'value': '2', 'label': 'Bathroom'},\n {'realValue': '1500', 'id': 'size', 'value': '1500 sq.ft.', 'label': 'Size'},\n {'realValue': '1',\n  'id': 'furnished',\n  'value': 'Fully Furnished',\n  'label': 'Furnished'},\n {'realValue': '13,9,12,7,5,16',\n  'id': 'facilities',\n  'value': 'Parking, Security, Lift, Playground, Minimart, Multipurpose hall',\n  'label': 'Facilities'},\n {'realValue': '6200',\n  'id': 'rendepo',\n  'value': 'RM 6200',\n  'label': 'Rental Deposit'},\n {'realValue': '1',\n  'id': 'additional_facilities',\n  'value': 'Air-Cond',\n  'label': 'Other Facilities'},\n {'realValue': 'e', 'id': 'firm_type', 'value': 'E', 'label': 'Firm Type'},\n {'realValue': '10091',\n  'id': 'estate_agent',\n  'value': '10091',\n  'label': 'Firm Number'}]\n\n\nExtracting the values inside the dictionary for each attributes.\n\n\nCode\n#getting values out of dictionary\ndef get_values(list_dict):\n    \"\"\"\n    Description:\n        Function to values of the previous dictionary.\n\n    Parameters:\n        list_dict (list): list of dictionary where attributes stored\n        \n    Returns:\n        a list of values (unit/property) attributes\n    \n    \"\"\"\n    keys = [\n        'ads_id',\n        'prop_name',\n        # 'developer_name', \n        # 'address', \n        'completion_year', \n        # 'num_floors', \n        # 'num_units',\n        'monthly_rent', \n        # 'category_id', \n        'location', \n        'property_type', \n        # 'floor_range', \n        'rooms', \n        'parking',\n        'bathroom', \n        'size', \n        'furnished',\n        'facilities', \n        'additional_facilities', \n       ]\n\n    values = {}\n    for key in keys:\n        try:\n            values[key] = next(item['value'] for item in list_dict if item[\"id\"] == key)\n        except StopIteration:\n            values[key] = None\n    return values\n\n#sanity check\nget_values(get_list_dict(1)[1])\n\n\n\n\n\n\n\n\n{'ads_id': '95301490',\n 'prop_name': 'PPA1M METROPOLITAN KEPONG',\n 'completion_year': '',\n 'monthly_rent': 'RM 1 600 per month',\n 'location': 'Kuala Lumpur - Kepong',\n 'property_type': 'Condominium',\n 'rooms': '5',\n 'parking': None,\n 'bathroom': '2',\n 'size': '1500 sq.ft.',\n 'furnished': 'Fully Furnished',\n 'facilities': 'Parking, Security, Lift, Playground, Minimart, Multipurpose hall',\n 'additional_facilities': 'Air-Cond'}\n\n\n\n\nCode\n#get df from list\ndef get_df_final(n_page):\n    \"\"\"\n    Description:\n        Function to generate dataframe from the list.\n\n    Parameters:\n        n_page (int): number of page\n        \n    Returns:\n        a dataframe of the list of attributes on each listings.\n    \n    \"\"\"\n    list_data = get_list_dict(n_page)\n    list_new = []\n    for i in range(0,len(list_data)):\n            dic = get_values(list_data[i])\n            list_new.append(dic)\n    \n    df = pd.DataFrame(list_new)\n    return df\n\n\nOf course we won‚Äôt scrape 250 pages at first, let‚Äôs extract 1 page only:\n\n\nCode\n#sanity check\nget_df_final(1).head(2).T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nads_id\n98377671\n95301490\n\n\nprop_name\n\nPPA1M METROPOLITAN KEPONG\n\n\ncompletion_year\n\n\n\n\nmonthly_rent\nRM 2 000 per month\nRM 1 600 per month\n\n\nlocation\nKuala Lumpur - Cheras\nKuala Lumpur - Kepong\n\n\nproperty_type\nCondominium\nCondominium\n\n\nrooms\n3\n5\n\n\nparking\n1\nNone\n\n\nbathroom\n2\n2\n\n\nsize\n1285 sq.ft.\n1500 sq.ft.\n\n\nfurnished\nFully Furnished\nFully Furnished\n\n\nfacilities\nParking, Security, Playground, Lift, Swimming Pool, Minimart\nParking, Security, Lift, Playground, Minimart, Multipurpose hall\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed\nAir-Cond\n\n\n\n\n\n\n\nFinally, let‚Äôs extract dataset from 250 pages. File is then saved into a csv, to be reloaded again.\n\n\nCode\n# df_=get_df_final(250)\n# df_.to_csv('mudah-apartment-raw.csv', index=False)\n## already run, and file is saved\n\n\n\n\n\n\n\nReload the original dataset.\n\n\nCode\n#read it back\ndf = pd.read_csv('./mudah-apartment-raw.csv')\ndf.head(3).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n\n\nmonthly_rent\nRM 4 200 per month\nRM 2 300 per month\nRM 1 000 per month\n\n\nlocation\nKuala Lumpur - Taman Desa\nKuala Lumpur - Cheras\nKuala Lumpur - Taman Desa\n\n\nproperty_type\nCondominium\nCondominium\nApartment\n\n\nrooms\n5\n3\n3\n\n\nparking\n2.0\n1.0\nNaN\n\n\nbathroom\n6.0\n2.0\n2.0\n\n\nsize\n1842 sq.ft.\n1170 sq.ft.\n650 sq.ft.\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 13 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   ads_id                 10000 non-null  int64  \n 1   prop_name              9492 non-null   object \n 2   completion_year        5623 non-null   float64\n 3   monthly_rent           10000 non-null  object \n 4   location               10000 non-null  object \n 5   property_type          10000 non-null  object \n 6   rooms                  9998 non-null   object \n 7   parking                7368 non-null   float64\n 8   bathroom               9998 non-null   float64\n 9   size                   10000 non-null  object \n 10  furnished              9999 non-null   object \n 11  facilities             9104 non-null   object \n 12  additional_facilities  7167 non-null   object \ndtypes: float64(3), int64(1), object(9)\nmemory usage: 1015.8+ KB\n\n\nThe following feature is available in the dataset:\n\nads_id: ads listing ID, unique to each ads\nprop_name: the building name of the property\ncompletion_year: year of the building/property completed\nmonthly_rent: monthly rent price in Malaysian Ringgit (RM)\nlocation: the location (region) of the property\nproperty_type: property type, such as flat, apartment, etc\nrooms: number of rooms\nparking: number of parking spot\nbathroom: number of bathroom\nsize: total area of the unit in sq.ft\nfurnished: furnishin status of the unit, fully-partial-non\nfacilities: main facilities within the unit\nadditional_facilities: additional facilities\n\n\n\n\n\nCode\n#cek duplikat\ndf.duplicated().sum()\n\n#drop duplikat\ndf1 = df.drop_duplicates()\n\n\nSaving the file to csv after remove duplicated values.\n\n\nCode\n# #saving to csv\n# df1.to_csv(\"mudah-apartment-clean.csv\", index=False)\n# #saved already\n\n\nReload the data after drop duplicates\n\n\nCode\n#reload the data\ndf = pd.read_csv(\"./mudah-apartment-clean.csv\")\n\n\n\n\nCode\n#sanity check\ndf.duplicated().sum()\n\n\n0\n\n\n\n\n\n\n\nCode\n#removing RM from monthly rent\ndf['monthly_rent'] = df['monthly_rent'].apply(lambda x: int(re.search(r'RM (.*?) per', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'monthly_rent': 'monthly_rent_rm'})\n\n#dropping sq.ft from size\ndf['size'] = df['size'].apply(lambda x: int(re.search(r'(.*?) sq', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'size': 'size_sqft'})\n\n#dropping kuala lumpur from the location\ndf['location'] = df['location'].apply(lambda x: re.findall(\"\\w+$\", x)[0])\ndf.head(4).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n100191767\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\nSentul Point Suite Apartment\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n2020.0\n\n\nmonthly_rent_rm\n4200\n2300\n1000\n1700\n\n\nlocation\nDesa\nCheras\nDesa\nSentul\n\n\nproperty_type\nCondominium\nCondominium\nApartment\nApartment\n\n\nrooms\n5\n3\n3\n2\n\n\nparking\n2.0\n1.0\nNaN\n1.0\n\n\nbathroom\n6.0\n2.0\n2.0\n2.0\n\n\nsize_sqft\n1842\n1170\n650\n743\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\nParking, Playground, Swimming Pool, Squash Court, Security, Minimart, Gymnasium, Lift\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\nCooking Allowed, Near KTM/LRT, Washing Machine\n\n\n\n\n\n\n\n\n\n\nHypotheses: closer access to KTM/LRT = higher monthly rent\n\n\nCode\n#extracting near KTM/LRT from the additional facilities\ndef extract_near_ktm_lrt(text):\n    pattern = re.compile(r'\\bNear KTM/LRT\\b')\n    try:\n        match = pattern.search(text)\n        if match:\n            return 'yes'\n        return 'no'\n    except TypeError:\n        return text\n\n\nExtracting ‚Äúnear KTM/LRT‚Äù into its own column.\n\n\nCode\ndf['nearby_railways'] = df.additional_facilities.apply(lambda x: extract_near_ktm_lrt(x))\ndf.head(4).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nads_id\n100323185\n100203973\n100323128\n100191767\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\nPangsapuri Teratak Muhibbah 2\nSentul Point Suite Apartment\n\n\ncompletion_year\n2022.0\nNaN\nNaN\n2020.0\n\n\nmonthly_rent_rm\n4200\n2300\n1000\n1700\n\n\nlocation\nDesa\nCheras\nDesa\nSentul\n\n\nproperty_type\nCondominium\nCondominium\nApartment\nApartment\n\n\nrooms\n5\n3\n3\n2\n\n\nparking\n2.0\n1.0\nNaN\n1.0\n\n\nbathroom\n6.0\n2.0\n2.0\n2.0\n\n\nsize_sqft\n1842\n1170\n650\n743\n\n\nfurnished\nFully Furnished\nPartially Furnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\nMinimart, Jogging Track, Lift, Swimming Pool\nParking, Playground, Swimming Pool, Squash Court, Security, Minimart, Gymnasium, Lift\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\nNaN\nCooking Allowed, Near KTM/LRT, Washing Machine\n\n\nnearby_railways\nno\nyes\nNaN\nyes\n\n\n\n\n\n\n\nPlotting the difference between nearby KTM/LRT or not:\n\n\nCode\nsns.boxplot(data=df, x='monthly_rent_rm', y='nearby_railways')\nplt.xlim(0,4000);\n\nnear_ktmlrt = df.query(\" nearby_railways == 'yes' \")\nnot_near_ktmlrt = df.query(\" nearby_railways == 'no' \")\n\nprint(f\"\"\" \nMedian:\nNearby KTM/LRT: {near_ktmlrt.monthly_rent_rm.median():.0f}RM\nNot nearby KTM/LRT: {not_near_ktmlrt.monthly_rent_rm.median():.0f}RM\n      \"\"\")\n\n\n \nMedian:\nNearby KTM/LRT: 1650RM\nNot nearby KTM/LRT: 1600RM\n      \n\n\n\n\nFigure¬†1: Boxplot between Nearby KTM/LRT or Not\n\n\n\n\n\nSanity check:\n\n\nCode\ndf[df['prop_name'] == 'Majestic Maxim'][['nearby_railways']].value_counts()\n\n\nnearby_railways\nyes                166\nno                  24\ndtype: int64\n\n\nAs seen above, Figure¬†1 shows that it sligthly increases the median monthly rent by 50RM. However, near KTM/LRT is not appearing in all row even though the property is the same\nConclusion: Near KTM/LRT may be used, but it can be improved as the listing is inconsistent\n\n\n\n\n\nCode\ndf.isna().sum()\n\n\nads_id                      0\nprop_name                 508\ncompletion_year          4373\nmonthly_rent_rm             0\nlocation                    0\nproperty_type               0\nrooms                       2\nparking                  2630\nbathroom                    2\nsize_sqft                   0\nfurnished                   1\nfacilities                895\nadditional_facilities    2831\nnearby_railways          2831\ndtype: int64\n\n\n\n\nCode\n#dropping some columns\ndf = df.drop(columns=[\n    'ads_id', \n    'prop_name', \n    'facilities', \n    'additional_facilities',\n    # 'nearby_railways',\n    # 'completion_year'\n])\ndf\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9991 rows √ó 10 columns\n\n\n\n\n\nCode\n#checking dtypes from all columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9991 entries, 0 to 9990\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   completion_year  5618 non-null   float64\n 1   monthly_rent_rm  9991 non-null   int64  \n 2   location         9991 non-null   object \n 3   property_type    9991 non-null   object \n 4   rooms            9989 non-null   object \n 5   parking          7361 non-null   float64\n 6   bathroom         9989 non-null   float64\n 7   size_sqft        9991 non-null   int64  \n 8   furnished        9990 non-null   object \n 9   nearby_railways  7160 non-null   object \ndtypes: float64(3), int64(2), object(5)\nmemory usage: 780.7+ KB\n\n\n\n\nCode\n#converting rooms from object to int64\ndf['rooms'] = pd.to_numeric(df['rooms'], downcast='integer', errors='coerce')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9991 entries, 0 to 9990\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   completion_year  5618 non-null   float64\n 1   monthly_rent_rm  9991 non-null   int64  \n 2   location         9991 non-null   object \n 3   property_type    9991 non-null   object \n 4   rooms            9987 non-null   float64\n 5   parking          7361 non-null   float64\n 6   bathroom         9989 non-null   float64\n 7   size_sqft        9991 non-null   int64  \n 8   furnished        9990 non-null   object \n 9   nearby_railways  7160 non-null   object \ndtypes: float64(4), int64(2), object(4)\nmemory usage: 780.7+ KB\n\n\n\n\n\nTo remove some unexplainable data such as 0 monthly rent, 0 size, the rent that is way too old (1970), including the monthly rent that is way too high and/or size too big.\n\n\nCode\ndf[['size_sqft', 'monthly_rent_rm']].plot(kind='scatter', x='size_sqft', y='monthly_rent_rm');\nplt.ylim(100,5500) #batas harga rent\nplt.xlim(50,3000)  #batas size\nplt.show()\n\n\n\n\nFigure¬†2: Monthly Rent\n\n\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1,2)\naxs[0].boxplot(data=df, x='monthly_rent_rm')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=df, x='monthly_rent_rm')\naxs[1].set_ylim(0,5000)\naxs[1].set_title('croped at 5,000 RM')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†3: Comparison between Different Scale\n\n\n\n\n\nBased on EDA on Figure¬†2 and Figure¬†3, author decided to filter the data between 100-5500 RM as follows:\n\n\nCode\n#removing all rows with monthly rent above 5500 RM and below 100RM\ndfx = df.query(\" monthly_rent_rm &gt; 100 & monthly_rent_rm &lt; 5500 \")\ndfx.describe()\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nrooms\nparking\nbathroom\nsize_sqft\n\n\n\n\ncount\n5530.000000\n9841.000000\n9838.000000\n7245.000000\n9840.000000\n9.841000e+03\n\n\nmean\n2014.863110\n1786.840260\n2.742427\n1.339268\n1.928760\n1.111279e+04\n\n\nstd\n7.436904\n768.813626\n0.763700\n0.517512\n0.517118\n1.008037e+06\n\n\nmin\n1980.000000\n110.000000\n1.000000\n1.000000\n1.000000\n1.000000e+00\n\n\n25%\n2011.000000\n1300.000000\n2.000000\n1.000000\n2.000000\n8.000000e+02\n\n\n50%\n2017.000000\n1600.000000\n3.000000\n1.000000\n2.000000\n9.080000e+02\n\n\n75%\n2020.000000\n2100.000000\n3.000000\n2.000000\n2.000000\n1.087000e+03\n\n\nmax\n2025.000000\n5300.000000\n9.000000\n10.000000\n6.000000\n1.000000e+08\n\n\n\n\n\n\n\nSanity check after removal as shown in Figure¬†4 belo:\n\n\nCode\ndfx.monthly_rent_rm.plot(kind='box', x='monthly_rent_rm');\n\n\n\n\nFigure¬†4: Data after Outlier Removal\n\n\n\n\n\n\n\n\n\nChecking the dataset in terms of size.\n\n\nCode\nfig, axs = plt.subplots(1,2)\naxs[0].boxplot(data=dfx, x='size_sqft')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=dfx, x='size_sqft')\naxs[1].set_ylim(0,2000)\naxs[1].set_title('croped at 0-2,000 square feet')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†5: Raw Data Size sq.ft\n\n\n\n\n\nStill based on Figure¬†2, outliers are removed.\n\n\nCode\n#removing outliers below 500, and higher than 3000 sqft and below 50 sqft\ndfx = \\\n(dfx.query(\" size_sqft &gt; 50 & size_sqft &lt; 3000 \")\n # .size_sqft\n # .plot(kind='box')\n)\ndfx\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 10 columns\n\n\n\nSanity check:\n\n\nCode\ndfx.size_sqft.plot(kind='box');\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1,5, figsize=(12,4))\naxs[0].boxplot(data=dfx.dropna(), x='size_sqft')\naxs[1].boxplot(data=dfx.dropna(), x='rooms')\naxs[2].boxplot(data=dfx.dropna(), x='parking')\naxs[3].boxplot(data=dfx.dropna(), x='bathroom')\n# axs[4].boxplot(data=dfx.dropna(), x='completion_year')\n\naxs[0].set_title('Size')\naxs[1].set_title('Rooms')\naxs[2].set_title('Parking')\naxs[3].set_title('Bathrooms')\n# axs[4].set_title('Completion Year')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure¬†6: Final Data after Outlier Removal\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef extractInputOutput(data,\n                       output_column_name):\n    \"\"\"\n    Fungsi untuk memisahkan data input dan output\n    :param data: &lt;pandas dataframe&gt; data seluruh sample\n    :param output_column_name: &lt;string&gt; nama kolom output\n    :return input_data: &lt;pandas dataframe&gt; data input\n    :return output_data: &lt;pandas series&gt; data output\n    \"\"\"\n    output_data = data[output_column_name]\n    input_data = data.drop(output_column_name,\n                           axis = 1)\n    \n    return input_data, output_data\n\n\n\n\nCode\nX, y = extractInputOutput(data=dfx, output_column_name='monthly_rent_rm')\n\n\n\n\nCode\nX\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 9 columns\n\n\n\n\n\nCode\ny\n\n\n0       4200\n1       2300\n2       1000\n3       1700\n4       1299\n        ... \n9986    1400\n9987    1000\n9988    1488\n9989    2000\n9990    2000\nName: monthly_rent_rm, Length: 9822, dtype: int64\n\n\n\n\n\n\n\nCode\n#import libraries\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state = 123)\n\n\n\n\nCode\n#sanity check\nlen(X_test)/len(X)\n\n\n0.20006108735491754\n\n\n\n\nCode\n#sanity check\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n5978\n2020.0\nSentul\nApartment\n3.0\n1.0\n2.0\n876\nPartially Furnished\nyes\n\n\n2151\n2022.0\nSetapak\nCondominium\n3.0\n1.0\n2.0\n850\nFully Furnished\nyes\n\n\n9714\n2002.0\nCheras\nCondominium\n3.0\nNaN\n2.0\n1000\nFully Furnished\nyes\n\n\n8556\n2021.0\nCheras\nService Residence\n2.0\n1.0\n2.0\n680\nPartially Furnished\nyes\n\n\n2809\n2005.0\nCheras\nCondominium\n3.0\n1.0\n2.0\n920\nPartially Furnished\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9954\n2016.0\nBesi\nService Residence\n3.0\n2.0\n2.0\n1121\nPartially Furnished\nyes\n\n\n7901\nNaN\nJalil\nFlat\n3.0\nNaN\n2.0\n650\nNot Furnished\nyes\n\n\n5322\n2013.0\nKepong\nCondominium\n3.0\n2.0\n2.0\n1378\nFully Furnished\nno\n\n\n1363\nNaN\nDesa\nCondominium\n3.0\n1.0\n2.0\n950\nPartially Furnished\nNaN\n\n\n3648\n2019.0\nKLCC\nService Residence\n2.0\n1.0\n2.0\n796\nFully Furnished\nNaN\n\n\n\n\n7857 rows √ó 9 columns\n\n\n\nPreprocessing Original Data for Categorical Dtypes\nOne must paying attention to the number of categorical observation in the original data, with respect to the sampling train-test value. If, the test_size = 0.3, that means any categorical observation with a total of 3 and less, would not be distributed evenly among train and test data.\n\n\nCode\nprint(dfx.location.nunique())\nprint(X_train.location.nunique())\nprint(X_test.location.nunique())\n\n\n53\n53\n50\n\n\n\n\nCode\nprint(dfx.property_type.nunique())\nprint(X_train.property_type.nunique())\nprint(X_test.property_type.nunique())\n\n\n9\n9\n8\n\n\n\n\nCode\nprint(set(X_train.furnished.to_list()) - set(X_test.furnished.to_list()))\nprint(set(X_train.location.to_list()) - set(X_test.location.to_list()))\nprint(set(X_train.property_type.to_list()) - set(X_test.property_type.to_list()))\nprint(set(X_train.nearby_railways.to_list()) - set(X_test.nearby_railways.to_list()))\n\n\nset()\n{'Sentral', 'Lin', 'Penchala'}\n{'Condo / Services residence / Penthouse / Townhouse'}\nset()\n\n\n\nDropping Data\n\n\n\nCode\ndfx\n\n\n\n\n\n\n\n\n\ncompletion_year\nmonthly_rent_rm\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n0\n2022.0\n4200\nDesa\nCondominium\n5.0\n2.0\n6.0\n1842\nFully Furnished\nno\n\n\n1\nNaN\n2300\nCheras\nCondominium\n3.0\n1.0\n2.0\n1170\nPartially Furnished\nyes\n\n\n2\nNaN\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n650\nFully Furnished\nNaN\n\n\n3\n2020.0\n1700\nSentul\nApartment\n2.0\n1.0\n2.0\n743\nPartially Furnished\nyes\n\n\n4\nNaN\n1299\nKiara\nService Residence\n1.0\n1.0\n1.0\n494\nNot Furnished\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9986\n2017.0\n1400\nSentul\nService Residence\n3.0\n1.0\n2.0\n900\nNot Furnished\nno\n\n\n9987\n1998.0\n1000\nDesa\nApartment\n3.0\nNaN\n2.0\n657\nFully Furnished\nno\n\n\n9988\n2021.0\n1488\nCheras\nCondominium\n3.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n9989\n1988.0\n2000\nDesa\nCondominium\n3.0\n2.0\n2.0\n1200\nFully Furnished\nno\n\n\n9990\nNaN\n2000\nCheras\nCondominium\n3.0\n2.0\n3.0\n1010\nFully Furnished\nyes\n\n\n\n\n9822 rows √ó 10 columns\n\n\n\n\n\nCode\ndfx.location.value_counts()\n\n\nCheras         1614\nSetapak         965\nSentul          776\nKepong          662\nJalil           577\nMaju            456\nAmpang          327\nKeramat         300\nRoad            298\nDesa            295\nCity            268\nKiara           264\nKLCC            239\nIpoh            224\nLama            193\nSegambut        178\nPetaling        174\nPandan          173\nBesi            173\nKuching         168\nSouth           143\nPantai          113\nBintang          99\nMelawati         91\nTitiwangsa       83\nHilir            78\nHartamas         75\nDamansara        73\nOUG              63\nIsmail           59\nDutamas          57\nGombak           56\nPerdana          53\nSetiawangsa      50\nParkCity         50\nBangsar          47\nMenjalara        45\nSeputeh          35\nPuchong          33\nIndah            29\nCentre           25\nJaya             24\nBrickfields      24\nPudu             24\nSelatan          19\nHeights          18\nJinjang           9\nSerdang           9\nSentral           5\nOthers            5\nTunku             2\nPenchala          1\nLin               1\nName: location, dtype: int64\n\n\n\n\nCode\ndfx.property_type.value_counts()\n\n\nCondominium                                           4698\nService Residence                                     2647\nApartment                                             2123\nFlat                                                   265\nStudio                                                  27\nDuplex                                                  27\nOthers                                                  27\nTownhouse Condo                                          7\nCondo / Services residence / Penthouse / Townhouse       1\nName: property_type, dtype: int64\n\n\n\n\nCode\ndfx_new = dfx[\n    (dfx.location != 'Jinjang') \n    & (dfx.location != 'Serdang') & \n    (dfx.location != 'Sentral') & \n    (dfx.location != 'Others') & \n    (dfx.location != 'Tunku') & \n    (dfx.location != 'Penchala') & \n    (dfx.location != 'Lin') &\n    # (dfx.property_type != 'Others') &\n    (dfx.property_type != 'Condo / Services residence / Penthouse / Townhouse') &\n    (dfx.property_type != 'Townhouse Condo')\n]\n\n\n\n\nCode\ndfx_new.property_type.value_counts()\n\n\nCondominium          4683\nService Residence    2642\nApartment            2115\nFlat                  263\nDuplex                 27\nStudio                 26\nOthers                 26\nName: property_type, dtype: int64\n\n\n\nRe-split Training-Test\n\n\n\nCode\nX, y = extractInputOutput(data=dfx_new, output_column_name='monthly_rent_rm')\n\n\n\n\nCode\n#import libraries\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state = 123)\n\n\n\n\nCode\n#sanity check\nlen(X_test)/len(X)\n\n\n0.2000613371498671\n\n\n\n\nCode\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n8649\nNaN\nMaju\nCondominium\n2.0\n1.0\n1.0\n800\nFully Furnished\nyes\n\n\n9112\n1993.0\nBangsar\nCondominium\n2.0\n1.0\n1.0\n890\nFully Furnished\nyes\n\n\n1472\nNaN\nJalil\nCondominium\n1.0\nNaN\n1.0\n1200\nFully Furnished\nyes\n\n\n5536\nNaN\nCheras\nCondominium\n3.0\n2.0\n2.0\n893\nFully Furnished\nno\n\n\n8152\nNaN\nPudu\nApartment\n3.0\nNaN\n2.0\n980\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\nAmpang\nApartment\n3.0\nNaN\n2.0\n828\nPartially Furnished\nNaN\n\n\n7927\nNaN\nSouth\nFlat\n3.0\nNaN\n2.0\n750\nNot Furnished\nNaN\n\n\n5340\n2023.0\nCheras\nCondominium\n4.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n1369\nNaN\nKLCC\nCondominium\n1.0\nNaN\n1.0\n473\nFully Furnished\nyes\n\n\n3659\n2017.0\nRoad\nService Residence\n3.0\nNaN\n2.0\n953\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 9 columns\n\n\n\n\n\nCode\nprint(set(X_train.furnished.to_list()) - set(X_test.furnished.to_list()))\nprint(set(X_train.location.to_list()) - set(X_test.location.to_list()))\nprint(set(X_train.property_type.to_list()) - set(X_test.property_type.to_list()))\n# print(set(X_train.nearby_railways.to_list()) - set(X_test.nearby_railways.to_list()))\n\n\nset()\nset()\nset()\n\n\n\n\nCode\nprint(dfx_new.location.nunique())\nprint(X_train.location.nunique())\nprint(X_test.location.nunique())\n\n\n46\n46\n46\n\n\n\n\nCode\n#sanity check\nX_train\n\n\n\n\n\n\n\n\n\ncompletion_year\nlocation\nproperty_type\nrooms\nparking\nbathroom\nsize_sqft\nfurnished\nnearby_railways\n\n\n\n\n8649\nNaN\nMaju\nCondominium\n2.0\n1.0\n1.0\n800\nFully Furnished\nyes\n\n\n9112\n1993.0\nBangsar\nCondominium\n2.0\n1.0\n1.0\n890\nFully Furnished\nyes\n\n\n1472\nNaN\nJalil\nCondominium\n1.0\nNaN\n1.0\n1200\nFully Furnished\nyes\n\n\n5536\nNaN\nCheras\nCondominium\n3.0\n2.0\n2.0\n893\nFully Furnished\nno\n\n\n8152\nNaN\nPudu\nApartment\n3.0\nNaN\n2.0\n980\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\nAmpang\nApartment\n3.0\nNaN\n2.0\n828\nPartially Furnished\nNaN\n\n\n7927\nNaN\nSouth\nFlat\n3.0\nNaN\n2.0\n750\nNot Furnished\nNaN\n\n\n5340\n2023.0\nCheras\nCondominium\n4.0\n2.0\n2.0\n1000\nPartially Furnished\nyes\n\n\n1369\nNaN\nKLCC\nCondominium\n1.0\nNaN\n1.0\n473\nFully Furnished\nyes\n\n\n3659\n2017.0\nRoad\nService Residence\n3.0\nNaN\n2.0\n953\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 9 columns\n\n\n\n\n\nCode\n#export data training\nX_train.to_csv('X_train.csv', index=False)\ny_train.to_csv('y_train.csv', index=False)\n\n\n\n\nCode\n#export data testing\nX_test.to_csv('X_test.csv', index=False)\ny_test.to_csv('y_test.csv', index=False)\n\n\n\n\n\n\n\n\nCode\n#checking null data\nX_train.isna().sum()\n\n\ncompletion_year    3438\nlocation              0\nproperty_type         0\nrooms                 2\nparking            2074\nbathroom              0\nsize_sqft             0\nfurnished             0\nnearby_railways    2206\ndtype: int64\n\n\n\n\n\n\nCode\nX_train_num =  X_train.select_dtypes(exclude='object')\nX_train_num\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\n\n\n\n\n8649\nNaN\n2.0\n1.0\n1.0\n800\n\n\n9112\n1993.0\n2.0\n1.0\n1.0\n890\n\n\n1472\nNaN\n1.0\nNaN\n1.0\n1200\n\n\n5536\nNaN\n3.0\n2.0\n2.0\n893\n\n\n8152\nNaN\n3.0\nNaN\n2.0\n980\n\n\n...\n...\n...\n...\n...\n...\n\n\n7541\n2001.0\n3.0\nNaN\n2.0\n828\n\n\n7927\nNaN\n3.0\nNaN\n2.0\n750\n\n\n5340\n2023.0\n4.0\n2.0\n2.0\n1000\n\n\n1369\nNaN\n1.0\nNaN\n1.0\n473\n\n\n3659\n2017.0\n3.0\nNaN\n2.0\n953\n\n\n\n\n7825 rows √ó 5 columns\n\n\n\n\n\nCode\nX_train_num.isna().sum()\n\n\ncompletion_year    3438\nrooms                 2\nparking            2074\nbathroom              0\nsize_sqft             0\ndtype: int64\n\n\n\nWe can fill completion year, rooms, parking and bathroom with mode\n\n\n\nCode\nfrom sklearn.impute import SimpleImputer\n\ndef numericalImputation(X_train_num, strategy = 'most_frequent'):\n    \"\"\"\n    Fungsi untuk melakukan imputasi data numerik NaN\n    :param data: &lt;pandas dataframe&gt; sample data input\n\n    :return X_train_numerical: &lt;pandas dataframe&gt; data numerik\n    :return imputer_numerical: numerical imputer method\n    \"\"\"\n    #buat imputer\n    imputer_num = SimpleImputer(missing_values = np.nan, strategy = strategy)\n    \n    #fitting\n    imputer_num.fit(X_train_num)\n\n    # transform\n    imputed_data = imputer_num.transform(X_train_num)\n    X_train_num_imputed = pd.DataFrame(imputed_data)\n\n    #pastikan index dan nama kolom antara imputed dan non-imputed SAMA\n    X_train_num_imputed.columns = X_train_num.columns\n    X_train_num_imputed.index = X_train_num.index\n\n    return X_train_num_imputed, imputer_num\n\n\n\n\nCode\nX_train_num, imputer_num = numericalImputation(X_train_num, strategy='most_frequent')\nX_train_num.isna().sum()\n\n\ncompletion_year    0\nrooms              0\nparking            0\nbathroom           0\nsize_sqft          0\ndtype: int64\n\n\n\n\nCode\nimputer_num\n\n\nSimpleImputer(strategy='most_frequent')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer(strategy='most_frequent')\n\n\n\n\n\n\n\nCode\nX_train_cat = X_train.select_dtypes(include='object')\nX_train_cat\n\n\n\n\n\n\n\n\n\nlocation\nproperty_type\nfurnished\nnearby_railways\n\n\n\n\n8649\nMaju\nCondominium\nFully Furnished\nyes\n\n\n9112\nBangsar\nCondominium\nFully Furnished\nyes\n\n\n1472\nJalil\nCondominium\nFully Furnished\nyes\n\n\n5536\nCheras\nCondominium\nFully Furnished\nno\n\n\n8152\nPudu\nApartment\nPartially Furnished\nyes\n\n\n...\n...\n...\n...\n...\n\n\n7541\nAmpang\nApartment\nPartially Furnished\nNaN\n\n\n7927\nSouth\nFlat\nNot Furnished\nNaN\n\n\n5340\nCheras\nCondominium\nPartially Furnished\nyes\n\n\n1369\nKLCC\nCondominium\nFully Furnished\nyes\n\n\n3659\nRoad\nService Residence\nPartially Furnished\nNaN\n\n\n\n\n7825 rows √ó 4 columns\n\n\n\n\n\nCode\nX_train_cat.isna().sum()\n\n\nlocation              0\nproperty_type         0\nfurnished             0\nnearby_railways    2206\ndtype: int64\n\n\n\nImpute with mode\n\n\n\nCode\nX_train_cat, imputer_num = numericalImputation(X_train_cat, strategy='most_frequent')\nX_train_cat.isna().sum()\n\n\nlocation           0\nproperty_type      0\nfurnished          0\nnearby_railways    0\ndtype: int64\n\n\n\n\n\n\n\nCode\nX_train_cat_ohe =  pd.get_dummies(X_train_cat)\nX_train_cat_ohe.head(2)\n\n\n\n\n\n\n\n\n\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\nlocation_Centre\nlocation_Cheras\nlocation_City\nlocation_Damansara\nlocation_Desa\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n9112\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 58 columns\n\n\n\n\n\nCode\nohe_columns = X_train_cat_ohe.columns\nohe_columns\n\n\nIndex(['location_Ampang', 'location_Bangsar', 'location_Besi',\n       'location_Bintang', 'location_Brickfields', 'location_Centre',\n       'location_Cheras', 'location_City', 'location_Damansara',\n       'location_Desa', 'location_Dutamas', 'location_Gombak',\n       'location_Hartamas', 'location_Heights', 'location_Hilir',\n       'location_Indah', 'location_Ipoh', 'location_Ismail', 'location_Jalil',\n       'location_Jaya', 'location_KLCC', 'location_Kepong', 'location_Keramat',\n       'location_Kiara', 'location_Kuching', 'location_Lama', 'location_Maju',\n       'location_Melawati', 'location_Menjalara', 'location_OUG',\n       'location_Pandan', 'location_Pantai', 'location_ParkCity',\n       'location_Perdana', 'location_Petaling', 'location_Puchong',\n       'location_Pudu', 'location_Road', 'location_Segambut',\n       'location_Selatan', 'location_Sentul', 'location_Seputeh',\n       'location_Setapak', 'location_Setiawangsa', 'location_South',\n       'location_Titiwangsa', 'property_type_Apartment',\n       'property_type_Condominium', 'property_type_Duplex',\n       'property_type_Flat', 'property_type_Others',\n       'property_type_Service Residence', 'property_type_Studio',\n       'furnished_Fully Furnished', 'furnished_Not Furnished',\n       'furnished_Partially Furnished', 'nearby_railways_no',\n       'nearby_railways_yes'],\n      dtype='object')\n\n\n\n\nCode\nX_train_cat_ohe.isna().sum()\n\n\nlocation_Ampang                    0\nlocation_Bangsar                   0\nlocation_Besi                      0\nlocation_Bintang                   0\nlocation_Brickfields               0\nlocation_Centre                    0\nlocation_Cheras                    0\nlocation_City                      0\nlocation_Damansara                 0\nlocation_Desa                      0\nlocation_Dutamas                   0\nlocation_Gombak                    0\nlocation_Hartamas                  0\nlocation_Heights                   0\nlocation_Hilir                     0\nlocation_Indah                     0\nlocation_Ipoh                      0\nlocation_Ismail                    0\nlocation_Jalil                     0\nlocation_Jaya                      0\nlocation_KLCC                      0\nlocation_Kepong                    0\nlocation_Keramat                   0\nlocation_Kiara                     0\nlocation_Kuching                   0\nlocation_Lama                      0\nlocation_Maju                      0\nlocation_Melawati                  0\nlocation_Menjalara                 0\nlocation_OUG                       0\nlocation_Pandan                    0\nlocation_Pantai                    0\nlocation_ParkCity                  0\nlocation_Perdana                   0\nlocation_Petaling                  0\nlocation_Puchong                   0\nlocation_Pudu                      0\nlocation_Road                      0\nlocation_Segambut                  0\nlocation_Selatan                   0\nlocation_Sentul                    0\nlocation_Seputeh                   0\nlocation_Setapak                   0\nlocation_Setiawangsa               0\nlocation_South                     0\nlocation_Titiwangsa                0\nproperty_type_Apartment            0\nproperty_type_Condominium          0\nproperty_type_Duplex               0\nproperty_type_Flat                 0\nproperty_type_Others               0\nproperty_type_Service Residence    0\nproperty_type_Studio               0\nfurnished_Fully Furnished          0\nfurnished_Not Furnished            0\nfurnished_Partially Furnished      0\nnearby_railways_no                 0\nnearby_railways_yes                0\ndtype: int64\n\n\n\n\nCode\nX_train_num.isna().sum()\n\n\ncompletion_year    0\nrooms              0\nparking            0\nbathroom           0\nsize_sqft          0\ndtype: int64\n\n\n\n\n\n\n\nCode\nX_train_concat = pd.concat([X_train_num,\n                            X_train_cat_ohe],\n                           axis = 1)\n\n\n\n\nCode\nX_train_concat.head(2)\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n2021.0\n2.0\n1.0\n1.0\n800.0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n9112\n1993.0\n2.0\n1.0\n1.0\n890.0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 63 columns\n\n\n\n\n\nCode\n#sanity check\nX_train_concat.isnull().sum()\n\n\ncompletion_year                  0\nrooms                            0\nparking                          0\nbathroom                         0\nsize_sqft                        0\n                                ..\nfurnished_Fully Furnished        0\nfurnished_Not Furnished          0\nfurnished_Partially Furnished    0\nnearby_railways_no               0\nnearby_railways_yes              0\nLength: 63, dtype: int64\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Buat fungsi\ndef standardizerData(data):\n    \"\"\"\n    Fungsi untuk melakukan standarisasi data\n    :param data: &lt;pandas dataframe&gt; sampel data\n    :return standardized_data: &lt;pandas dataframe&gt; sampel data standard\n    :return standardizer: method untuk standardisasi data\n    \"\"\"\n    data_columns = data.columns  # agar nama kolom tidak hilang\n    data_index = data.index  # agar index tidak hilang\n\n    # buat (fit) standardizer\n    standardizer = StandardScaler()\n    standardizer.fit(data)\n\n    # transform data\n    standardized_data_raw = standardizer.transform(data)\n    standardized_data = pd.DataFrame(standardized_data_raw)\n    standardized_data.columns = data_columns\n    standardized_data.index = data_index\n\n    return standardized_data, standardizer\n\n\n\n\nCode\nX_train_clean, standardizer = standardizerData(data = X_train_concat)\n\n\n\n\nCode\nX_train_clean.head()\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n8649\n0.541336\n-0.971807\n-0.542258\n-1.813058\n-0.530030\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n9112\n-3.913712\n-0.971807\n-0.542258\n-1.813058\n-0.190258\n-0.189407\n14.508152\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n1472\n0.541336\n-2.288585\n-0.542258\n-1.813058\n0.980069\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n-0.619460\n0.619460\n\n\n5536\n0.541336\n0.344970\n1.685126\n0.150837\n-0.178932\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n1.072803\n-0.406146\n-0.805454\n1.614308\n-1.614308\n\n\n8152\n0.541336\n0.344970\n-0.542258\n0.150837\n0.149515\n-0.189407\n-0.068927\n-0.137405\n-0.102909\n-0.048017\n...\n-0.050621\n-0.167279\n-0.049336\n-0.60979\n-0.053098\n-0.932137\n-0.406146\n1.241535\n-0.619460\n0.619460\n\n\n\n\n5 rows √ó 63 columns\n\n\n\n\n\n\n\nSince this is a regression model, R2 score and mean absolute error (MAE) will be used as a performance metrics.\nThe machine learning model will use baseline from average value of the target columns (monthly rent) and also result from linear regression model. After that, author used some of the recommended model based on previous works, which are random forest and gradient boosting to better improve the performance of the model.\n\n\nThe concept here is to use average value of the target as the easiest way to predict the monhtly rent of a unit.\n\n\nCode\ny_baseline = np.ones(len(y_train)) * y_train.mean()\ny_baseline\n\n\narray([1780.0086901, 1780.0086901, 1780.0086901, ..., 1780.0086901,\n       1780.0086901, 1780.0086901])\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Train the linear regression model\nlin_reg = LinearRegression().fit(X_train_clean, y_train)\n\n# Predict using the train data\ny_pred_train = y_baseline\n\n# Calculate R-squared\nr2_baseline = r2_score(y_train, y_pred_train)\n\n#calculate MAE\nmae_baseline = mean_absolute_error(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_baseline:.4f} and MAE score: {mae_baseline:.4f}\")\n\n\nR2-score: 0.0000 and MAE score: 562.3710\n\n\n\n\nCode\nplt.scatter(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe second method is using linear regression, which simply put is finding the minum total error (distance) between predicted value and the target value, using linear equation.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Train the linear regression model\nlin_reg = LinearRegression().fit(X_train_clean, y_train)\n\n# Predict using the train data\n# y_pred = y_baseline\ny_pred_train = lin_reg.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_linreg = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_linreg = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_linreg:.4f} and MAE score: {mae_linreg:.4f}\")\n\n\nR2-score: 0.6468 and MAE score: 319.2229\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe gradient boosting, is one of the recommendation from previous works, is a model where each sample would be given a different weights (boosts) depending on its performance in predicting the value/ target.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Build random forest\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\n\n\n\nCode\n# Fit random forest\ngrad_tree.fit(X_train_clean, y_train)\n\n\nGradientBoostingRegressor(random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = grad_tree.predict(X_train_clean)\n# y_pred_test = grad_tree.predict(X_test_clean)\n\n# Calculate mean absolute error\nmae_gb = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_gb = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_gb:.4f} and MAE score: {mae_gb:.4f}\")\n\n\nR2-score: 0.7246 and MAE score: 281.6835\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\n#gridsearch\n\nfrom sklearn.model_selection import GridSearchCV \n\n\nparams = {'n_estimators': [100, 200, 300, 400, 500],\n              'learning_rate': [0.1, 0.05, 0.01]}\n\n# Buat gridsearch\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\ngrad_tree_cv = GridSearchCV(estimator = grad_tree,\n                           param_grid = params,\n                           cv = 5,\n                           scoring = \"neg_mean_absolute_error\")\n\n\n\n\nCode\n# Fit grid search cv\ngrad_tree_cv.fit(X_train_clean, y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=123),\n             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             scoring='neg_mean_absolute_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=123),\n             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n                         'n_estimators': [100, 200, 300, 400, 500]},\n             scoring='neg_mean_absolute_error')estimator: GradientBoostingRegressorGradientBoostingRegressor(random_state=123)GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\n\nCode\n# Best params\ngrad_tree_cv.best_params_\n\n\n{'learning_rate': 0.1, 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the Adaboost\ngrad_tree = GradientBoostingRegressor(n_estimators = grad_tree_cv.best_params_[\"n_estimators\"],\n                                      random_state = 123)\n\ngrad_tree.fit(X_train_clean, y_train)\n\n\nGradientBoostingRegressor(n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = grad_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_gb_cv = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_gb_cv = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_gb_cv:.4f} and MAE score: {mae_gb_cv:.4f}\")\n\n\nR2-score: 0.8194 and MAE score: 228.0225\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\n\nThe last model, which was also recommended by previous works, is a model where not only it has weights based on its performance, but the feature selection in which the sample is measured was done at random. Therefore, reduces not only the variance, but also the bias.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n\nCode\n# Build random forest\nrf_tree = RandomForestRegressor(n_estimators = 100,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n\n\n\nCode\n# Fit random forest\nrf_tree.fit(X_train_clean, y_train)\n\n\nRandomForestRegressor(max_features='sqrt', random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf = mean_absolute_error(y_train, y_pred_train)\n\n# Calculate R-squared\nr2_rf = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf:.4f} and MAE score: {mae_rf:.4f}\")\n\n\nR2-score: 0.9577 and MAE score: 100.8408\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\nparams = {\"n_estimators\": [100, 200, 300, 500 ],\n          \"max_features\": [\"sqrt\", \"log2\"]}\n\n# Buat gridsearch\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                random_state = 123)\n\nrf_tree_cv = GridSearchCV(estimator = rf_tree,\n                          param_grid = params,\n                          cv = 5,\n                          scoring = \"neg_mean_absolute_error\")\n\n\n\n\nCode\n# Fit grid search cv\nrf_tree_cv.fit(X_train_clean, y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=123),\n             param_grid={'max_features': ['sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 500]},\n             scoring='neg_mean_absolute_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=123),\n             param_grid={'max_features': ['sqrt', 'log2'],\n                         'n_estimators': [100, 200, 300, 500]},\n             scoring='neg_mean_absolute_error')estimator: RandomForestRegressorRandomForestRegressor(random_state=123)RandomForestRegressorRandomForestRegressor(random_state=123)\n\n\n\n\nCode\n# Best params\nrf_tree_cv.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the Random Forest\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                max_features = rf_tree_cv.best_params_[\"max_features\"],\n                                n_estimators = rf_tree_cv.best_params_[\"n_estimators\"],\n                                random_state = 123)\n\nrf_tree.fit(X_train_clean, y_train)\n\n\nRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf_cv = mean_absolute_error(y_train, y_pred_train)\n\n# # Calculate R-squared\nr2_rf_cv = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf_cv:.4f} and MAE score: {mae_rf_cv:.4f}\")\n\n\nR2-score: 0.9585 and MAE score: 99.7989\n\n\n\n\nCode\nsns.jointplot(x=y_train, y=y_pred_train);\n\n\n\n\n\n\n\nCode\nmae_score = [mae_baseline, mae_linreg, mae_gb, mae_gb_cv, mae_rf, mae_rf_cv]\nr2_score = [r2_baseline, r2_linreg, r2_gb, r2_gb_cv, r2_rf, r2_rf_cv]\nindexes = [\"baseline\", \"linear regression\", \"gradient boosting\", \"gradient boosting with CV\", \"random forest\",  \"random forest with CV\"]\n\nsummary_df = pd.DataFrame({\n    \"MAE Train\": mae_score,\n    \"R2-Score\": r2_score,\n},index = indexes)\n\nsummary_df.sort_values(by='R2-Score', ascending=False)\n\n\n\n\n\n\n\n\n\nMAE Train\nR2-Score\n\n\n\n\nrandom forest with CV\n99.798879\n0.958519\n\n\nrandom forest\n100.840759\n0.957661\n\n\ngradient boosting with CV\n228.022510\n0.819416\n\n\ngradient boosting\n281.683477\n0.724601\n\n\nlinear regression\n319.222873\n0.646780\n\n\nbaseline\n562.370983\n0.000000\n\n\n\n\n\n\n\nFrom the above table, it can be seen that Random Forest model performs the best, and Gradient Boosting at the second place. This is similar to the previous work done by others, on house pricing.\n\n\n\n\n\n\nCode\n# libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n#setting up\nrf_tree = RandomForestRegressor(n_estimators = 500,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n#fit model train\nrf_tree.fit(X_train_clean, y_train)\n\n# Predict model train\ny_pred_train = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf_cv_train = mean_absolute_error(y_train, y_pred_train)\n\n# # Calculate R-squared\nr2_rf_cv_train = r2_score(y_train, y_pred_train)\n\nprint(f\"R2-score: {r2_rf_cv_train:.3f} and MAE score: +/-{mae_rf_cv_train:.2f} RM\")\n\nsns.scatterplot(x=y_train, y=y_pred_train )\nplt.plot([0, 5500], [0,5500], \"--r\")\nplt.xlim(0, 5500)\nplt.xlabel(\"Actual Monthly Rent\")\nplt.ylim(0,5500)\nplt.ylabel(\"Predicted Monthly Rent\")\nplt.suptitle(\"Random Forest - Best Regression Model\")\nplt.show()\n\n\nR2-score: 0.959 and MAE score: +/-99.80 RM"
  },
  {
    "objectID": "posts/005-webscraping-machinelearning-rent-pricing/index.html#data-prediction",
    "href": "posts/005-webscraping-machinelearning-rent-pricing/index.html#data-prediction",
    "title": "Malaysia Property Pricing - Webscraping & Machine Learning Model",
    "section": "Data Prediction",
    "text": "Data Prediction\n\nTest Data Preprocessing\nSimlar process done in train dataset need to be repeated on test dataset.\n\n\nCode\n#checking null data\nX_test.isna().sum()\n\n\ncompletion_year    834\nlocation             0\nproperty_type        0\nrooms                0\nparking            506\nbathroom             0\nsize_sqft            0\nfurnished            0\nnearby_railways    552\ndtype: int64\n\n\n\nNumerical Data\n\n\nCode\nX_test_num =  X_test.select_dtypes(exclude='object')\nX_test_num\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\n\n\n\n\n324\nNaN\n3.0\nNaN\n2.0\n1097\n\n\n7209\n2011.0\n4.0\n2.0\n3.0\n1200\n\n\n1863\nNaN\n2.0\nNaN\n1.0\n560\n\n\n2443\n2021.0\n3.0\n1.0\n2.0\n1200\n\n\n9218\n2023.0\n1.0\n2.0\n2.0\n300\n\n\n...\n...\n...\n...\n...\n...\n\n\n7510\n2008.0\n3.0\n1.0\n2.0\n1204\n\n\n928\n2013.0\n1.0\nNaN\n1.0\n350\n\n\n2181\n2019.0\n1.0\n1.0\n1.0\n653\n\n\n4065\nNaN\n2.0\nNaN\n2.0\n600\n\n\n9041\n2021.0\n3.0\nNaN\n2.0\n800\n\n\n\n\n1957 rows √ó 5 columns\n\n\n\n\n\nCode\nX_test_num.isna().sum()\n\n\ncompletion_year    834\nrooms                0\nparking            506\nbathroom             0\nsize_sqft            0\ndtype: int64\n\n\n\n\nCode\nX_test_num, imputer_num = numericalImputation(X_test_num, strategy='most_frequent')\nX_test_num.isna().sum()\n\n\ncompletion_year    0\nrooms              0\nparking            0\nbathroom           0\nsize_sqft          0\ndtype: int64\n\n\n\n\nCategorical Data\n\n\nCode\nX_test_cat = X_test.select_dtypes(include='object')\nX_test_cat\n\n\n\n\n\n\n\n\n\nlocation\nproperty_type\nfurnished\nnearby_railways\n\n\n\n\n324\nSouth\nCondominium\nPartially Furnished\nno\n\n\n7209\nKLCC\nCondominium\nFully Furnished\nNaN\n\n\n1863\nMaju\nFlat\nNot Furnished\nNaN\n\n\n2443\nLama\nCondominium\nFully Furnished\nyes\n\n\n9218\nCheras\nCondominium\nFully Furnished\nyes\n\n\n...\n...\n...\n...\n...\n\n\n7510\nSetiawangsa\nCondominium\nFully Furnished\nyes\n\n\n928\nRoad\nCondominium\nFully Furnished\nNaN\n\n\n2181\nSegambut\nService Residence\nFully Furnished\nno\n\n\n4065\nLama\nApartment\nNot Furnished\nno\n\n\n9041\nSetapak\nCondominium\nPartially Furnished\nyes\n\n\n\n\n1957 rows √ó 4 columns\n\n\n\n\n\nCode\nX_test_cat.isna().sum()\n\n\nlocation             0\nproperty_type        0\nfurnished            0\nnearby_railways    552\ndtype: int64\n\n\n\n\nCode\nX_test_cat, imputer_num = numericalImputation(X_test_cat, strategy='most_frequent')\nX_test_cat.isna().sum()\n\n\nlocation           0\nproperty_type      0\nfurnished          0\nnearby_railways    0\ndtype: int64\n\n\n\n\nCategorical OHE\n\n\nCode\nX_test_cat_ohe =  pd.get_dummies(X_test_cat)\nX_test_cat_ohe.head(2)\n\n\n\n\n\n\n\n\n\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\nlocation_Centre\nlocation_Cheras\nlocation_City\nlocation_Damansara\nlocation_Desa\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n324\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n7209\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 58 columns\n\n\n\n\n\nCode\nohe_columns = X_test_cat_ohe.columns\nohe_columns\n\n\nIndex(['location_Ampang', 'location_Bangsar', 'location_Besi',\n       'location_Bintang', 'location_Brickfields', 'location_Centre',\n       'location_Cheras', 'location_City', 'location_Damansara',\n       'location_Desa', 'location_Dutamas', 'location_Gombak',\n       'location_Hartamas', 'location_Heights', 'location_Hilir',\n       'location_Indah', 'location_Ipoh', 'location_Ismail', 'location_Jalil',\n       'location_Jaya', 'location_KLCC', 'location_Kepong', 'location_Keramat',\n       'location_Kiara', 'location_Kuching', 'location_Lama', 'location_Maju',\n       'location_Melawati', 'location_Menjalara', 'location_OUG',\n       'location_Pandan', 'location_Pantai', 'location_ParkCity',\n       'location_Perdana', 'location_Petaling', 'location_Puchong',\n       'location_Pudu', 'location_Road', 'location_Segambut',\n       'location_Selatan', 'location_Sentul', 'location_Seputeh',\n       'location_Setapak', 'location_Setiawangsa', 'location_South',\n       'location_Titiwangsa', 'property_type_Apartment',\n       'property_type_Condominium', 'property_type_Duplex',\n       'property_type_Flat', 'property_type_Others',\n       'property_type_Service Residence', 'property_type_Studio',\n       'furnished_Fully Furnished', 'furnished_Not Furnished',\n       'furnished_Partially Furnished', 'nearby_railways_no',\n       'nearby_railways_yes'],\n      dtype='object')\n\n\n\n\n\nPenggabungan Numerical dan Categorical data\n\n\nCode\nX_test_concat = pd.concat([X_test_num,\n                            X_test_cat_ohe],\n                           axis = 1)\n\n\n\n\nCode\nX_test_concat.head(2)\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n324\n2021.0\n3.0\n1.0\n2.0\n1097.0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n7209\n2011.0\n4.0\n2.0\n3.0\n1200.0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n2 rows √ó 63 columns\n\n\n\n\n\nCode\n#sanity check\nX_test_concat.isnull().sum()\n\n\ncompletion_year                  0\nrooms                            0\nparking                          0\nbathroom                         0\nsize_sqft                        0\n                                ..\nfurnished_Fully Furnished        0\nfurnished_Not Furnished          0\nfurnished_Partially Furnished    0\nnearby_railways_no               0\nnearby_railways_yes              0\nLength: 63, dtype: int64\n\n\n\nStandarisasi\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Buat fungsi\ndef standardizerData(data):\n    \"\"\"\n    Fungsi untuk melakukan standarisasi data\n    :param data: &lt;pandas dataframe&gt; sampel data\n    :return standardized_data: &lt;pandas dataframe&gt; sampel data standard\n    :return standardizer: method untuk standardisasi data\n    \"\"\"\n    data_columns = data.columns  # agar nama kolom tidak hilang\n    data_index = data.index  # agar index tidak hilang\n\n    # buat (fit) standardizer\n    standardizer = StandardScaler()\n    standardizer.fit(data)\n\n    # transform data\n    standardized_data_raw = standardizer.transform(data)\n    standardized_data = pd.DataFrame(standardized_data_raw)\n    standardized_data.columns = data_columns\n    standardized_data.index = data_index\n\n    return standardized_data, standardizer\n\n\n\n\nCode\nX_test_clean, standardizer = standardizerData(data = X_test_concat)\n\n\n\n\nCode\nX_test_clean.head()\n\n\n\n\n\n\n\n\n\ncompletion_year\nrooms\nparking\nbathroom\nsize_sqft\nlocation_Ampang\nlocation_Bangsar\nlocation_Besi\nlocation_Bintang\nlocation_Brickfields\n...\nproperty_type_Duplex\nproperty_type_Flat\nproperty_type_Others\nproperty_type_Service Residence\nproperty_type_Studio\nfurnished_Fully Furnished\nfurnished_Not Furnished\nfurnished_Partially Furnished\nnearby_railways_no\nnearby_railways_yes\n\n\n\n\n324\n0.552642\n0.320875\n-0.541910\n0.124936\n0.555078\n-0.168453\n-0.071667\n-0.120479\n-0.09361\n-0.055456\n...\n-0.059914\n-0.161923\n-0.059914\n-0.60234\n-0.045256\n-0.949656\n-0.368524\n1.208981\n1.563785\n-1.563785\n\n\n7209\n-0.958283\n1.620985\n1.505421\n2.112746\n0.933643\n-0.168453\n-0.071667\n-0.120479\n-0.09361\n-0.055456\n...\n-0.059914\n-0.161923\n-0.059914\n-0.60234\n-0.045256\n1.053013\n-0.368524\n-0.827143\n-0.639474\n0.639474\n\n\n1863\n0.552642\n-0.979234\n-0.541910\n-1.862873\n-1.418602\n-0.168453\n-0.071667\n-0.120479\n-0.09361\n-0.055456\n...\n-0.059914\n6.175759\n-0.059914\n-0.60234\n-0.045256\n-0.949656\n2.713531\n-0.827143\n-0.639474\n0.639474\n\n\n2443\n0.552642\n0.320875\n-0.541910\n0.124936\n0.933643\n-0.168453\n-0.071667\n-0.120479\n-0.09361\n-0.055456\n...\n-0.059914\n-0.161923\n-0.059914\n-0.60234\n-0.045256\n1.053013\n-0.368524\n-0.827143\n-0.639474\n0.639474\n\n\n9218\n0.854826\n-2.279344\n1.505421\n0.124936\n-2.374201\n-0.168453\n-0.071667\n-0.120479\n-0.09361\n-0.055456\n...\n-0.059914\n-0.161923\n-0.059914\n-0.60234\n-0.045256\n1.053013\n-0.368524\n-0.827143\n-0.639474\n0.639474\n\n\n\n\n5 rows √ó 63 columns\n\n\n\n\n\n\nTest Data Result\n\n\nCode\n# libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n#setting up\nrf_tree = RandomForestRegressor(n_estimators = 500,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n#fit model train\nrf_tree.fit(X_train_clean, y_train)\n\n# Predict model\ny_pred_test = rf_tree.predict(X_test_clean)\n\n# Calculate mean absolute error\nmae_rf_cv_test = mean_absolute_error(y_test, y_pred_test)\n\n# # Calculate R-squared\nr2_rf_cv_test = r2_score(y_test, y_pred_test)\n\nprint(f\"R2-score: {r2_rf_cv_test:.3f} and MAE score: +/-{mae_rf_cv_test:.2f} RM\")\n\nsns.scatterplot(x=y_test, y=y_pred_test )\nplt.plot([0, 5500], [0,5500], \"--r\")\nplt.xlim(0, 5500)\nplt.xlabel(\"Actual Monthly Rent\")\nplt.ylim(0,5500)\nplt.ylabel(\"Predicted Monthly Rent\")\nplt.suptitle(\"Random Forest - Best Regression Model\")\nplt.show()\n\n\nR2-score: 0.803 and MAE score: +/-214.08 RM\n\n\n\n\n\n\n\nCode\nmae_score = [mae_rf_cv_train, mae_rf_cv_test]\nr2_score = [r2_rf_cv_train, r2_rf_cv_test]\nindexes = [\"train\", \"test\"]\n\nsummary_df_train_test = pd.DataFrame({\n    \"MAE Train\": mae_score,\n    \"R2-Score\": r2_score,\n},index = indexes)\n\nsummary_df_train_test\n\n\n\n\n\n\n\n\n\nMAE Train\nR2-Score\n\n\n\n\ntrain\n99.798879\n0.958519\n\n\ntest\n214.084111\n0.802556\n\n\n\n\n\n\n\n\nFeature Importance\n\n\nCode\n# calculate the feature importances\nimportances = rf_tree.feature_importances_\n\n# rescale the importances back to the original scale of the features\nimportances = importances * X_train_clean.std()\n\n# sort the feature importances in descending order\nsorted_index = importances.argsort()[::-1]\n\n# print the feature importances\ndict_feature_importance = {}\nfor i in sorted_index:\n    # print(\"{}: {}\".format(X_train_clean.columns[i], importances[i]))\n    dict_feature_importance.update({X_train_clean.columns[i]: importances[i]})\n    \n# Create a DataFrame from the dictionary\ndf = pd.DataFrame.from_dict(dict_feature_importance, orient='index', columns=['values'])\n\n# Reset the index to become a column\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['feature', 'importance_value']\n\ndf.sort_values(by='importance_value', ascending=False).head(10)\n\n\n\n\n\n\n\n\n\nfeature\nimportance_value\n\n\n\n\n0\nsize_sqft\n0.227595\n\n\n1\nfurnished_Fully Furnished\n0.106722\n\n\n2\ncompletion_year\n0.073598\n\n\n3\nbathroom\n0.059699\n\n\n4\nrooms\n0.046285\n\n\n5\nparking\n0.045606\n\n\n6\nlocation_Kiara\n0.042962\n\n\n7\nfurnished_Not Furnished\n0.040320\n\n\n8\nlocation_KLCC\n0.037287\n\n\n9\nfurnished_Partially Furnished\n0.036137\n\n\n\n\n\n\n\n\n\n\nResults\n\nResult indicates that the best model for prediction is Random Forest with hyperparameter tuning, scoring 95% on R2-score, and a shy 100 RM on MAE. This proves to be a good model since the test dataset gives a scoring of 80% on R2, and 240 RM on MAE.\nThere are some factors that author believed to be affecting the result/ performance of the model:\n\nDropping missing value reduces the performance! Initial model uses half of the data (4-5k rows) and gives poorer performance on R2 and MAE. Imputation and keeping the number of rows close to the original dataset (9k rows) proves to be improving the model. Especially on test dataset.\nFeature selection importance can be seen on the last table, but initially the selection was based on paper and intuition of the author (author lives and work in KL, Malaysia for 5 years). Feature such as completion_year and nearby_railways are important in improving the model.\nLast but not least is the outlier identification. The best practice for me is using jointplot to see not only the distribution of the data in 2-dimension, but also in the third dimension (the density) of the data.\n\nSome of the feature that were believed to be quite important even before doing the modeling is size, furnished and location. All three is available within the 10-most features affecting the modeling. As a context, location in KLCC is like Pondok Indah in South Jakarta. Location in Kiara is like BSD in South Tangerang.\n\n\n\nDiscussions\n\nOne of the feature that author thinks is significant but not appearing on the 10-best important feature is nearby_railways. This column is showing if a certain property has a close proximity to a railways (KTM/LRT). The issue is, half of the data is missing, hence the imputation. Author belives, the proximity to nearby railways line can be approximated using manhanttan distance of railways line to each property unit."
  },
  {
    "objectID": "posts/002-migrating-to-quarto/index.html",
    "href": "posts/002-migrating-to-quarto/index.html",
    "title": "Migrating My Personal Blog to Quarto",
    "section": "",
    "text": "Photo by Kelly Sikkema on Unsplash"
  },
  {
    "objectID": "posts/002-migrating-to-quarto/index.html#footnotes",
    "href": "posts/002-migrating-to-quarto/index.html#footnotes",
    "title": "Migrating My Personal Blog to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is one of the thing I am struggling when trying wordpress.‚Ü©Ô∏é\nPreviously R Studio. Read the announcement‚Ü©Ô∏é\nQuarto 1.2 pre-released version supports embedding video from Youtube.‚Ü©Ô∏é\nIssue in Quarto 1.1‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/001-my-first-gig/index.html",
    "href": "posts/001-my-first-gig/index.html",
    "title": "My First Interview in 500 Fortune Company",
    "section": "",
    "text": "Image by Unsplash\n\n\n\nIn the early 2013, around March, I was getting an interview on the 500 Fortune company, where my interviewer asked me a difficult question and I said I don‚Äôt know, but somehow I got the Job.\nMy career started even before that. In 2011-2012, during my last year of studies and close to graduation day I spent around a year to do side projects with my lecturers, from geological mapping, seismic interpretation, joint-study field evaluation, basin modeling, to petrophysical analysis.\nNot until 2013-2014, where I got my first industrial role in Jakarta, the capital city of Indonesia, moving away from freelance project in Yogyakarta. Here I mostly worked on petrophysical analysis, evaluating more than 100 wells in clastics and carbonate reefal reservoir, helping the team to update their static model. We planned to do EOR (waterflooding) at the time, so simulation is very critical before economical decision was made.\nFast forward to March 2014, through some luck and intentional effort, I managed to secure an interview with my current employer for an entry level position as petrophysicist/ geoscientist. The interview was a bit ‚Äúdrama‚Äù at first, cause the interviewer was changed at the last minute. Nerveousness was at its peak.\nHowever, somehow I got to meet the interviewer, answer few tehnical questions and non-technical questions. Right before we end the interview, she asked me the final technical questions. The question is about intutitive meaning of when density and neutron log is overlayed each other. She said she just need a straightforward answer, two words not more.\nI knew at that time that I have no idea what was supposed to be the answer to the question, but I was too afraid that I would mess my chance to get the gig. I was not esactly remembered how long I take the time to answer, but eventually I come to a conclusion ‚Äì I should be honest.\nI said:\n\n‚ÄúI Do Not Know‚Äù.\n\nIt took her 5 seconds, and as she smiled she said:\n\n‚ÄúWell, I Do Not Expect you to be able to Answer the question given the limited experience you have with well logs‚Äù\n\nShe explained to me that the question is to test me. She knew that I would probably did not know the answer to the question, her target was to know whether I will spit out my honest truth (that I dont know), or try to be smart by spitting some BS. I was glad I choose the first.\nTo me this was one of the biggest moment in my career, where you should always try to be honest with yourself in an interview. At the end of the day, you would not want to be in the same team with a person who you cannot trust.\nCredibility, given how ‚Äúsmall‚Äù the world of energy industry, should not be treated lightly. Furthermore, connection, was built upon trusts. You will be surprised, how many times I have seen ‚Äúa guy/girl‚Äù got the gig just by ‚Äúknowing someone‚Äù.\n\n\n\nCitationBibTeX citation:@online{arie wijaya2022,\n  author = {Arie Wijaya, Aditya},\n  title = {My {First} {Interview} in 500 {Fortune} {Company}},\n  date = {2022-08-29},\n  url = {https://adtarie.net/posts/001-my-first-gig},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nArie Wijaya, Aditya. 2022. ‚ÄúMy First Interview in 500 Fortune\nCompany.‚Äù August 29, 2022. https://adtarie.net/posts/001-my-first-gig."
  },
  {
    "objectID": "posts/003-medical-insurance/index.html",
    "href": "posts/003-medical-insurance/index.html",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "",
    "text": "Photo by Towfiqu barbhuiya on Unsplash"
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#introduction-objectives",
    "href": "posts/003-medical-insurance/index.html#introduction-objectives",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Introduction & Objectives",
    "text": "Introduction & Objectives\nHealth insurance plays an important role in future financial planning. Insurance members are required to pay a routine payment (insurance rates) to the insurance company. This rate will be used to pay medical bill of the insurance members. therefore, determination of insurance rate becomes a critical component to ensure the sustainability of the insurance.\nIn this project, the author wanted to do an exploratory analysis based on known variable that may correlate with the medical bill of the said members. This project will be using personal medical bills dataset (insurance.zip) as the main source1, along with the included metadata below:\n\nage: age of primary beneficiary\nsex: insurance contractor gender, female, male\nbmi: body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg/m2) using the ratio of height to weight, ideally 18.5 to 24.9\nchildren: number of children covered by health insurance / number of dependents\nsmoker: smoking\nregion: the beneficiary‚Äôs residential area in the US, northeast, southeast, southwest, northwest.\ncharges: individual medical costs billed by health insurance\n\nAt glance, bmi and smoker would likely to induce a high medical cost of a person, while age, sex, children and region may contribute in some senses or the others.\nObjectives:\n\nAnalyze the relationship between multiple variables to medical charges\nCharacterize the risk profile of members, based on the said analysis\nDetermine if the insurance rate can be optimized for each risk profile"
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#setting-up",
    "href": "posts/003-medical-insurance/index.html#setting-up",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Setting-up",
    "text": "Setting-up\n\n#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\n# import hvplot.pandas\n\n#setting default theme\nsns.set_theme(style='white', palette='tab20')"
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#importing-dataset",
    "href": "posts/003-medical-insurance/index.html#importing-dataset",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Importing Dataset",
    "text": "Importing Dataset\n\ninsurance = pd.read_csv('insurance.zip')\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520"
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#feature-engineering",
    "href": "posts/003-medical-insurance/index.html#feature-engineering",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nIn this dataset, the BMI is a numeric data. In order to better analyze the dataset, the bmi data can be grouped into different class/group. The classification in this project will be using BMI classification below:\n\n\n\nBMI\nWeight Status\n\n\n\n\nBelow 18.5\nUnderweight\n\n\n18.5 ‚Äì 24.9\nHealthy Weight\n\n\n25.0 ‚Äì 29.9\nOverweight\n\n\n30.0 and Above\nObesity\n\n\n\n\nWe can use pandas.cut method to create a quick binning over bmi column.\n\nbins= [0,18.49,24.9,30,100] #setting up the group based on bmi bins \nlabels = [\n         'underweight',\n         'healthy',\n         'overweight',\n         'obese'\n         ] #setting up the label on each group\n\ninsurance['bmi_class']= pd.cut(\n   insurance['bmi'], \n   bins=bins, \n   labels=labels,\n   include_lowest=False\n   ) #making the new column called bmi_class\n\n#sanity check on bmi_class\n(insurance\n .groupby('bmi_class')\n [['bmi', 'bmi_class']]\n .agg(['min', 'max', 'count'])\n # .style.background_gradient()\n # .style.text_gradient()\n#  .T\n)\n\n\n\n\n\n\nTable¬†1: BMI Class\n\n\n\nbmi\nbmi_class\n\n\n\nmin\nmax\ncount\nmin\nmax\ncount\n\n\nbmi_class\n\n\n\n\n\n\n\n\n\n\nunderweight\n15.96\n18.335\n20\nunderweight\nunderweight\n20\n\n\nhealthy\n18.50\n24.890\n222\nhealthy\nhealthy\n222\n\n\noverweight\n24.97\n30.000\n391\noverweight\noverweight\n391\n\n\nobese\n30.02\n53.130\n705\nobese\nobese\n705\n\n\n\n\n\n\n\n\nTable¬†1 shows a new column bmi_class as the result of grouping the bmi data into different categories."
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#quicklook",
    "href": "posts/003-medical-insurance/index.html#quicklook",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Quicklook",
    "text": "Quicklook\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 8 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   age        1338 non-null   int64   \n 1   sex        1338 non-null   object  \n 2   bmi        1338 non-null   float64 \n 3   children   1338 non-null   int64   \n 4   smoker     1338 non-null   object  \n 5   region     1338 non-null   object  \n 6   charges    1338 non-null   float64 \n 7   bmi_class  1338 non-null   category\ndtypes: category(1), float64(2), int64(2), object(3)\nmemory usage: 74.8+ KB\n\n\nThe data appears to be clean, with no null row, and dftypes appear to be correct. However, the format appears to be a non-tidy format.\n\n(insurance\n .select_dtypes(include=object) #includes all column with object dtypes\n .value_counts() #counting unique value\n)\n\nsex     smoker  region   \nfemale  no      southwest    141\n                southeast    139\n                northwest    135\nmale    no      southeast    134\nfemale  no      northeast    132\nmale    no      northwest    132\n                southwest    126\n                northeast    125\n        yes     southeast     55\n                northeast     38\n                southwest     37\nfemale  yes     southeast     36\n                northeast     29\n                northwest     29\nmale    yes     northwest     29\nfemale  yes     southwest     21\nName: count, dtype: int64\n\n\nSome observations:\n\n2 categorical data in sex column: female and male\n2 categorical data in the smoker column, yes or no\n4 categorical data in the region column: southwest, northwest, southeast, and northeast\n\n\n\n\n\n\n\nImportant\n\n\n\nThe first attempt is to see the distribution on each variable relative to each other, depending on different categories. For example, comparing mean age between smoker and non-smoker group, age between low and high bmi class, etc.\nThen trying to understand the relationship of each variable with respect to the medical charges"
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#exploratory-data-analysis",
    "href": "posts/003-medical-insurance/index.html#exploratory-data-analysis",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nAs many people know, smoking is highly linked to clinical disease such as TBC, lung cancer, hypertension, etc. People with smoking history, may be considered a high risk profile, and as the likely outcome the medical charges may be higher than a non-smoker people.\n\nOverall Mean age of insurance member\n\n\nCode\n(sns\n .displot(\n     data=insurance, \n     x='age', \n     hue='smoker',\n     kind='hist',\n     height=3,\n     aspect=1.2,\n )\n);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†1: Overall Mean Age\n\n\n\n\n\nBased on distribution at Figure¬†1, the mean age for all insurance members is around 39 years old. There is also higher number of non-smoker compared to the total data (2 times higher) compared to smoker. There is an anomaly frequency around age of 20 that has up to 4 times higher counts. May need further check.\n\n\nMean age, bmi and charges of smoker at different sex\n\n\nCode\n(insurance\n .groupby([\n     'smoker',\n     'sex'\n ])\n \n [[\n     'age',\n     'bmi',\n     'charges'\n  ]]\n \n .agg([\n     'mean', \n ])\n#  .style.background_gradient(\n#      axis=0,\n#      cmap=\"Blues\"\n#  )\n)\n\n\n\n\n\n\n\nTable¬†2: Statistic of Age, BMI, and Charges by Smoker and Sex\n\n\n\n\nage\nbmi\ncharges\n\n\n\n\nmean\nmean\nmean\n\n\nsmoker\nsex\n\n\n\n\n\n\n\nno\nfemale\n39.691042\n30.539525\n8762.297300\n\n\nmale\n39.061896\n30.770580\n8087.204731\n\n\nyes\nfemale\n38.608696\n29.608261\n30678.996276\n\n\nmale\n38.446541\n31.504182\n33042.005975\n\n\n\n\n\n\n\n\nCalculating the ratio of smoker to non-smoker group\n\n\nCode\n(insurance\n .groupby('smoker')\n [['charges']]\n .agg([np.mean])/8434.268298 #to calculate how high the smoker medical charges\n)\n\n\n\n\n\n\n\nTable¬†3: Ratio of Charges based on Smoker profile\n\n\n\ncharges\n\n\n\nmean\n\n\nsmoker\n\n\n\n\n\nno\n1.000000\n\n\nyes\n3.800001\n\n\n\n\n\n\n\n\nBased on the above Table¬†2, the male bmi is always higher than the female counterpart, irrespective of its sex. Furthermore, the average bmi for smoker is slightly higher than non-smoker group.\nOn the other hand, the average age for female is always higher than its male counterpart, regardless of smoker or non-smoker.\nAs indicated by Table¬†3, the medical charges for is much higher in the smoker member, compared to non-smoker member, with up to 4 times higher for smoker\n\n\nCode\n(insurance\n .groupby('sex')\n [['charges']]\n .agg([np.mean])/12569.578844 #to calculate how high the smoker medical charges\n)\n\n\n\n\n\n\n\nTable¬†4: Ratio of Charges based on Sex profile\n\n\n\ncharges\n\n\n\nmean\n\n\nsex\n\n\n\n\n\nfemale\n1.000000\n\n\nmale\n1.110359\n\n\n\n\n\n\n\n\nFurthermore, Table¬†4 shows that male has 10% higher medical charges compared to female counterpart.\n\nDistribution of age categorized based on sex, smoker, and bmi_class\n\n\nCode\n(sns\n .catplot\n (data=insurance,\n  kind='box',\n  x='age', \n  y='smoker',\n  hue='bmi_class',\n  col='sex',\n  # col_wrap=1,\n  height=4,\n  aspect=0.7,\n  # showmeans=True,\n  palette='Blues',\n )\n);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†2: Age distribution based on Categorical values\n\n\n\n\n\nThe above Figure¬†2 age shows a the distribution of age between smoker, bmi_class and different sex in the data. As can be seen, there is a clear trend of non-smoker where as the age increases, the bmi increases also, in both male and female group.\nWhereas in the smoker group, there is no clear trend of age vs bmi. This can be further checked when using scatterplot between age and bmi vs charges.\n\n\n\nDoes region affecting the age distribution?\n\n\nCode\n(sns\n .catplot\n (data=insurance,\n  kind='box',\n  x='age', \n  y='smoker',\n  hue='bmi_class',\n  col='region',\n  col_wrap=2,\n  height=4,\n  aspect=0.7,\n  showmeans=True,\n  palette='Blues',\n )\n);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†3: Age distribution based on region\n\n\n\n\n\nAs can be seen in Figure¬†3, the region category does not seem to bring any value to the analysis, as the pattern with/ without region data is unclear. May need further check.\n\nInspecting the age vs charges based on smoker, sex, and bmi_class profile\n\n\nCode\ng=(sns\n.relplot\n (data=insurance,\n  x='age',\n  y='charges',\n  hue='smoker',\n  size='bmi_class',\n  style='sex',\n  # legend='full',\n  # col='bmi_class',\n  # col_wrap=1,\n  height=3.5, \n  aspect=1.2,\n  markers=[\"8\",\"P\"],\n  palette='tab10',\n  size_order=['obese', 'overweight', 'healthy', 'underweight']\n )\n);\n\n#setting up annotations\n\ng.fig.text(0.6, 0.25, \"I\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\ng.fig.text(0.6, 0.45, \"II\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\ng.fig.text(0.6, 0.7, \"III\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\nplt.suptitle('age vs charges', y = 1.05);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†4: Age vs Charges\n\n\n\n\n\nFigure¬†4 shows at least three groups of trend with a strong relationship between medical charges and age. As the age increases, the medical charges increases.\nThe three group of medical charges are as follows:\n\ngroup I: 16000 and below\ngroup II: 16000-30000\ngroup III: above 30000\n\nThe three group of trends were heavily affected by the smoker/ non-smoker group, as the highest group III appears to have more points with obese bmi_class. This can be further checked if we exclude non-smoker group and hue it by bmi_class, and we can put sex as the column category.\n\n\n\nDoes sex affects age vs charges distribution/trend?\n\n\nCode\n(sns\n.relplot\n (data=insurance\n  # .query(\"smoker=='yes'\")\n  ,\n  x='age',\n  y='charges',\n  hue='bmi_class',\n  size='bmi_class',\n  style='smoker',\n  # legend='full',\n  col='sex',\n#   col_wrap=1,\n  # row='smoker',\n  height=4, \n  aspect=0.7,\n  markers=[\"8\",\"P\"],\n  s=300,\n  palette='tab10',\n  alpha=0.7,\n  size_order=['obese', 'overweight', 'healthy', 'underweight'],\n  \n )\n)\n\nplt.suptitle('age vs charges | separated by male vs female', y = 1.05);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†5: Age vs Charges based on Sex\n\n\n\n\n\nCouple conclusions can be drawn from these Figure¬†4 and Figure¬†5:\n\nThat the male is likely to have higher medical charges compared to female, with relatively small difference (Table¬†4).\nThere are three groups of strong trend between age vs charges, where as the age increases in all trends, the medical charges is likely to increases as well.\nThe three groups can be characterized from low-high charges as follows:\n\nGroup I: medical charges between 0-16,000, predominantly non-smoker,and a mix between all bmi_class.\nGroup II: medical charges between 12,000-30,000, a mix between smoker and non-smoker group, and bmi_class of healthy and overweight.\nGroup III: medical charges above 30,000, predominantly obese bmi_class and smoker.\n\n\n\n\nSome observed outlier (group I-a) between group I and II?\n\n\nCode\ng = (sns\n.relplot\n (data=insurance,\n  x='age',\n  y='charges',\n  hue='bmi_class',\n  size='bmi_class',\n  style='smoker',\n  # legend='full',\n  col='smoker',\n  # col_wrap=1,\n  # row='smoker',\n  height=4, \n  aspect=0.7,\n  markers=[\"8\",\"P\"],\n  s=300,\n  palette='tab10',\n  alpha=0.7,\n  size_order=['obese', 'overweight', 'healthy', 'underweight'],\n )\n);\n\n#annotations\n\ng.fig.text(0.6, 0.22, \"I\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\ng.fig.text(0.3, 0.4, \"II\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\ng.fig.text(0.3, 0.63, \"III\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\ng.fig.text(0.65, 0.4, \"I-a\",\n   color=\"black\", fontdict=dict(size=20), fontweight='bold'\n          )\n\n\nplt.suptitle('age vs charges | separated by smoker vs non-smoker', y = 1.05);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†6: Outliers?\n\n\n\n\n\nIf we look at the above Figure¬†6, in the non-smoker group, there is a cloud of data below the group II is. This needs further check, as what would affect the scattered data across this category, as it looks like there is another factor (aside from what was plotted already) that affects the data ‚Äúmoves up‚Äù (increased medical charges)\n\n\nWhat affects the outlier/ I-a group?\n\n\nCode\n(sns\n.relplot\n (data=insurance.query('smoker == \"no\"'),\n  x='age',\n  y='charges',\n  hue='bmi_class',\n  # size='children',\n  style='sex',\n  legend='full',\n  col='children',\n  col_wrap=2,\n  # row='region',\n  height=3, \n  aspect=1,\n#   markers=[\"8\",\"P\"],\n  # s=400,\n  palette='tab10',\n  alpha=0.7,\n  # size_order=['obese', 'overweight', 'healthy', 'underweight'],\n )\n);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†7: Outliers vs number of Children\n\n\n\n\n\nFigure¬†7 shows just group I and I-a where we see through zero to six number of childrens, colored by bmi_class and styled by sex. As can be seen, there is no clear differentiator between group I and I-a, as the number of children increases, it affects both group I and I-a also.\n\n\n\n\n\n\nImportant\n\n\n\nIt is unclear as to why this is happening. Perhaps other factors plays a role. At the time of this writing, author decided to categorized the group I-a as the outlier.\n\n\n\n\nCode\ng = (sns\n.relplot\n (data=insurance,\n  x='bmi',\n  y='charges',\n  hue='smoker',\n  size='children',\n  # style='sex',\n  # legend='full',\n  # col='bmi_class',\n  # col_wrap=1,\n  height=3.5, \n  aspect=1,\n  markers=[\"8\",\"P\"],\n  palette='tab10',\n  size_order=[1,2,3,4,5,6]\n )\n)\n\n#annotations\ng.fig.text(0.35, 0.25, \"I\",\n   color=\"black\", fontdict=dict(size=18), fontweight='bold'\n          )\n\ng.fig.text(0.35, 0.45, \"II\",\n   color=\"black\", fontdict=dict(size=18), fontweight='bold'\n          )\n\ng.fig.text(0.6, 0.65, \"III\",\n   color=\"black\", fontdict=dict(size=18), fontweight='bold'\n          )\n\ng.fig.text(0.5, 0.45, \"I-a\",\n   color=\"black\", fontdict=dict(size=18), fontweight='bold'\n          )\n\nplt.suptitle('bmi vs charges', y = 1.05);\n\n\n/Users/ariewijaya/miniconda3/envs/minids/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\nFigure¬†8: BMI vs Charges\n\n\n\n\n\nSimilar to the previous Figure¬†5, in Figure¬†8 we can see that the non-smoker group overlaps with the smoker group at around 16,000-30,000 medical charge range."
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#conclusions-and-outcomes",
    "href": "posts/003-medical-insurance/index.html#conclusions-and-outcomes",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Conclusions and Outcomes",
    "text": "Conclusions and Outcomes\n\nThe insurance member can be characterzed based on the smoker profile, age, bmi/ bmi_class, and sex profile.\nThe critical factor for a high medical charges is whether member is a smoker or not, followed by age and then bmi2.\nOther variable such as the number of children/ dependant is not playing a role, whereas sex profile affect the charges slightly3.\nThere is a strong relationship between age and medical charges, with 3 strong group categorized by smoker and bmi profile. The big three groups are:\n\nGroup I: medical charges below 16k, related to a group of non-smoker4.\nGroup II: medical charges between 16k-30k, related to a group of smoker and overweight.\nGroup III: medical charges above 30k, related to a group of smoker and obese.\n\nThese three groups have its own trendline which can be later determined (trendline).\nThere is also one outlier or group I-a, where it is unclear as to what affects the medical charges increase to be between group I and II.\n\nThe above groups (I, II, III) can be used for risk profiling, where based on its profile, the associated risk related to medical charges can be determined. Since each group forms a trendline (simple linear regression), and can be categorized based on its smoker, and bmi profile. Based on this approach, one can estimate the optimum pricing for each risk profile (based on its medical charges).\nSee below flowchart for simple ilustration.\n\n\n\n\nflowchart LR\n    A[Member] --&gt; B{Smoking}\n    B --&gt; |No| C[trendline Group I]\n    B --&gt; |Yes| D{BMI Class}\n    D --&gt; |Overweight| E[trendline Group II]\n    D --&gt; |Obese| F[trendline Group III]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe next step would be using some of these knowledge to further investigate the likelihood (probability) of each risk profile based on its numeric (age-bmi) and categorical variable (smoker-bmi-region-sex).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis article was made as a replacement for medium article as part of Pacmann Data Science bootcamp assignment for Linear Algebra batch 8. The EDA notebook can be found in this notebook. Inside there will be a Youtube link as part of the assignment also."
  },
  {
    "objectID": "posts/003-medical-insurance/index.html#footnotes",
    "href": "posts/003-medical-insurance/index.html#footnotes",
    "title": "Medical Insurance Cost - Exploratory Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nalso available at Kaggle.‚Ü©Ô∏é\nthe lowest medical charges (group I) may have obese in their profile, but since it is a non-smoker profile, the medical charges is lower.‚Ü©Ô∏é\naround 10% higher in male, compared to female counterpart (see Table¬†4)‚Ü©Ô∏é\nthe BMI class shows a mix of all classes (from underweight to obese)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html",
    "href": "posts/006-easy-report-machine-learning/index.html",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "",
    "text": "Photo by Esmonde Yong on Unsplash\n\n\n\nThis is a machine learning project to predict unit/property monthly rent price in Kuala Lumpur region, Malaysia. The project uses a dataset from an online ads listing for property mudah.my. This project outlines the process of web-scraping/ data gathering, data cleaning-wrangling, and machine learning modeling.\nThis project aims to answers question about how much a unit monthly rent would be if given information such as location, number of bedrooms, parking, furnished, etc? This would help potential tenant and also the owner to get the best price of their rental unit, comparable to the market value.\nSome previous work about house pricing was listed below, however most of them are targeting a dataset of house pricing or an Airbnb pricing. There are difference such as in Airbnb, the booking rarely took more than 2 weeks, let alone a year. Therefore the pricing may be different. Additionally, in Airbnb, there is text feature coming from the review given by the tenant and the owner.The better the review, the higher the rent prices ‚Äì which was not available in this current project dataset."
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#background",
    "href": "posts/006-easy-report-machine-learning/index.html#background",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "",
    "text": "Photo by Esmonde Yong on Unsplash\n\n\n\nThis is a machine learning project to predict unit/property monthly rent price in Kuala Lumpur region, Malaysia. The project uses a dataset from an online ads listing for property mudah.my. This project outlines the process of web-scraping/ data gathering, data cleaning-wrangling, and machine learning modeling.\nThis project aims to answers question about how much a unit monthly rent would be if given information such as location, number of bedrooms, parking, furnished, etc? This would help potential tenant and also the owner to get the best price of their rental unit, comparable to the market value.\nSome previous work about house pricing was listed below, however most of them are targeting a dataset of house pricing or an Airbnb pricing. There are difference such as in Airbnb, the booking rarely took more than 2 weeks, let alone a year. Therefore the pricing may be different. Additionally, in Airbnb, there is text feature coming from the review given by the tenant and the owner.The better the review, the higher the rent prices ‚Äì which was not available in this current project dataset."
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#who-is-this-for",
    "href": "posts/006-easy-report-machine-learning/index.html#who-is-this-for",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Who is this for?",
    "text": "Who is this for?\n\n\n\n\n\n\nNote\n\n\n\nThis project was the TLDR-version of the complete article where author explained in much more details about the process of webscraping-data cleaning-data wrangling-feature engineering, etc. This was made also as a mandatory terms for me to pass the Pacmann bootcamp intro to machine learning class. Video of me explaining the whole project is also available here or in the video below"
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#related-work",
    "href": "posts/006-easy-report-machine-learning/index.html#related-work",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Related work",
    "text": "Related work\nPrevious work by Madhuri, Anuradha, and Pujitha (2019), Xu and Nguyen (2022), and Zhao et al. (2022) highlight the importance feature selection, and the choice of machine learning model. Based on the previous works, the most consistently performed machine learning model are Random Forest and Gradient boosting, and the MAE and R2 score usually used in evaluating the performance of the model. Although the above work are all not about apartment rent pricing, similar method can be applied to this project."
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#dataset-features",
    "href": "posts/006-easy-report-machine-learning/index.html#dataset-features",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Dataset & features",
    "text": "Dataset & features\nThe dataset is using the scraped dataset from ads listing website, particularly property-to-rent surrounding Kuala Lumpur, Malaysia.\n\n\n\n\n\n\nWhy Webscraping?\n\n\n\nAs 80% of data science process is about data engineering, from collection (gathering) to wrangling/ cleaning, author fells the need to brush up the skill, from available online data, relevant to the author (location: Kuala Lumpur), using webscraping tool such as BeaufifulSoup.\nDetail of the web-scraping process on this project can be found in this article.\n\n\nThere are over 10k ads listed at the time of this project as can be seen below:\n\n\nData Description\n\n\nCode\n#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('max_colwidth', 200)\nimport re\n\n#reload the data\ndf = pd.read_csv(\"./mudah-apartment-clean.csv\")\ndf.head(2).T\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nads_id\n100323185\n100203973\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\n\n\ncompletion_year\n2022.0\nNaN\n\n\nmonthly_rent\nRM 4 200 per month\nRM 2 300 per month\n\n\nlocation\nKuala Lumpur - Taman Desa\nKuala Lumpur - Cheras\n\n\nproperty_type\nCondominium\nCondominium\n\n\nrooms\n5\n3\n\n\nparking\n2.0\n1.0\n\n\nbathroom\n6.0\n2.0\n\n\nsize\n1842 sq.ft.\n1170 sq.ft.\n\n\nfurnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT\n\n\n\n\n\n\n\nThere are 13 features with one unique ids (ads_id) and one target feature (monthly_rent)\n\nads_id: the listing ids (unique)\nprop_name: name of the building/ property\ncompletion_year: completion/ established year of the property\nmonthly_rent: monthly rent in ringgit malaysia (RM)\nlocation: property location in Kuala Lumpur region\nproperty_type:property type such as apartment, condominium, flat, duplex, studio, etc\nrooms: number of rooms in the unit\nparking: number of parking space for the unit\nbathroom: number of bathrooms in the unit\nsize: total area of the unit in square feet\nfurnished: furnishing status of the unit (fully, partial, non-furnished)\nfacilities: main facilities available\nadditional_facilities: additional facilities (proximity to attraction area, mall, school, shopping, railways, etc)\n\n\n\nData Cleaning\nThe cleaning process mainly related to extracting the value out of column. E.g. extracting monthly rent of 1400 from a string of ‚Äú1400 RM‚Äù, etc.\n\n\nCode\n#removing RM from monthly rent\ndf['monthly_rent'] = df['monthly_rent'].apply(lambda x: int(re.search(r'RM (.*?) per', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'monthly_rent': 'monthly_rent_rm'})\n\n#dropping sq.ft from size\ndf['size'] = df['size'].apply(lambda x: int(re.search(r'(.*?) sq', x).group(1).replace(' ', '')))\ndf = df.rename(columns={'size': 'size_sqft'})\n\n#dropping kuala lumpur from the location\ndf['location'] = df['location'].apply(lambda x: re.findall(\"\\w+$\", x)[0])\ndf.head(2).T\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nads_id\n100323185\n100203973\n\n\nprop_name\nThe Hipster @ Taman Desa\nSegar Courts\n\n\ncompletion_year\n2022.0\nNaN\n\n\nmonthly_rent_rm\n4200\n2300\n\n\nlocation\nDesa\nCheras\n\n\nproperty_type\nCondominium\nCondominium\n\n\nrooms\n5\n3\n\n\nparking\n2.0\n1.0\n\n\nbathroom\n6.0\n2.0\n\n\nsize_sqft\n1842\n1170\n\n\nfurnished\nFully Furnished\nPartially Furnished\n\n\nfacilities\nMinimart, Gymnasium, Security, Playground, Swimming Pool, Parking, Lift, Barbeque area, Multipurpose hall, Jogging Track\nPlayground, Parking, Barbeque area, Security, Jogging Track, Swimming Pool, Gymnasium, Lift, Sauna\n\n\nadditional_facilities\nAir-Cond, Cooking Allowed, Washing Machine\nAir-Cond, Cooking Allowed, Near KTM/LRT"
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#methods",
    "href": "posts/006-easy-report-machine-learning/index.html#methods",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Methods",
    "text": "Methods\nFollowing the works from others, author will be focusing on using Random Forest and Gradient Boosting for the two main machine learning model to try to compare to baseline (average, and linear regression).\n\nBaseline using average means that the prediction will be using average value of the train target value. This yield a zero R2-score and the highest MAE value which will not be used as comparison in the following discussion.\n\nThe author will mainly talking about baseline using linear regression. Linear regression is one of the machine learning model, where the model objective is to minimize the total error (distance) of each predicted value against the actual value.\nThe comparable model will be using Random Forest and Gradient Boosting.\nA gradient boosting uses iterative process to assign weights to different sample, until the model predicts the target correctly. Meanwhile, Random Forest use similar concept, but the sampling and feature selection are random, therefore reduces both bias and variance in the model."
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#experiments",
    "href": "posts/006-easy-report-machine-learning/index.html#experiments",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Experiments",
    "text": "Experiments\nThe experiments mostly related to the data preparation before getting into modeling. The most author spent time with is feature selection and outlier removal. One of the insight when doing feature selection is the proximity to nearby railways (KTM/LRT) is likely to affect the increase of rent price. However, the finding is that the listing is inconsistent, the same property may listed to be ‚Äònear KTM/LRT‚Äô, but the other rows were not.\n\nExtracting Near KTM/LRT\n\n\nCode\n#extracting near KTM/LRT from the additional facilities\ndef extract_near_ktm_lrt(text):\n    pattern = re.compile(r'\\bNear KTM/LRT\\b')\n    try:\n        match = pattern.search(text)\n        if match:\n            return 'yes'\n        return 'no'\n    except TypeError:\n        return text\n\n\n\n\nCode\ndf['nearby_railways'] = df.additional_facilities.apply(lambda x: extract_near_ktm_lrt(x))\n\nfig, axs = plt.subplots(figsize=(6,4))\nsns.boxplot(data=df, x='monthly_rent_rm', y='nearby_railways', ax=axs)\nplt.xlim(0,4000);\n\nnear_ktmlrt = df.query(\" nearby_railways == 'yes' \")\nnot_near_ktmlrt = df.query(\" nearby_railways == 'no' \")\n\nprint(f\"\"\" \nMedian:\nNearby KTM/LRT: {near_ktmlrt.monthly_rent_rm.median():.0f}RM\nNot nearby KTM/LRT: {not_near_ktmlrt.monthly_rent_rm.median():.0f}RM\n      \"\"\")\n\n\n \nMedian:\nNearby KTM/LRT: 1650RM\nNot nearby KTM/LRT: 1600RM\n      \n\n\n\n\n\n\n\nCode\ndf[df['prop_name'] == 'Majestic Maxim'][['nearby_railways']].value_counts()\n\n\nnearby_railways\nyes                166\nno                  24\ndtype: int64\n\n\nAs seen above, nearby KTM/LRT is slightly increases the median monthly rent by 50RM, however near KTM/LRT is not appearing in all row even though the unit is the same building.\n\n\nDrop Unnecessary Missing Values\nSome features such as ads_id, prop_name, facilities and additional_facilities would no longer needed after the previous process.\n\n\nCode\ndf = df.drop(columns=[\n    'ads_id', \n    'prop_name', \n    'facilities', \n    'additional_facilities',\n    # 'nearby_railways',\n    # 'completion_year'\n])\ndf.head(2).T\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\ncompletion_year\n2022.0\nNaN\n\n\nmonthly_rent_rm\n4200\n2300\n\n\nlocation\nDesa\nCheras\n\n\nproperty_type\nCondominium\nCondominium\n\n\nrooms\n5\n3\n\n\nparking\n2.0\n1.0\n\n\nbathroom\n6.0\n2.0\n\n\nsize_sqft\n1842\n1170\n\n\nfurnished\nFully Furnished\nPartially Furnished\n\n\nnearby_railways\nno\nyes\n\n\n\n\n\n\n\n\n\nCode\n#converting rooms from object to int64 for plotting\ndf['rooms'] = pd.to_numeric(df['rooms'], downcast='integer', errors='coerce')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9991 entries, 0 to 9990\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   completion_year  5618 non-null   float64\n 1   monthly_rent_rm  9991 non-null   int64  \n 2   location         9991 non-null   object \n 3   property_type    9991 non-null   object \n 4   rooms            9987 non-null   float64\n 5   parking          7361 non-null   float64\n 6   bathroom         9989 non-null   float64\n 7   size_sqft        9991 non-null   int64  \n 8   furnished        9990 non-null   object \n 9   nearby_railways  7160 non-null   object \ndtypes: float64(4), int64(2), object(4)\nmemory usage: 780.7+ KB\n\n\n\n\nOutlier Removal\nRemoving the outlier is extremely important, as some of these observation e.g.¬†monthly rent, have astronomical rent value, far exceeding the median. After multiple iteration, below is the most-ideal limit for size_sqft and monthly_rent_rm.\n\n\nCode\nf, axs = plt.subplots(1,1, figsize=(6,4))\ndf[['size_sqft', 'monthly_rent_rm']].plot(kind='scatter', \n                                          x='size_sqft', \n                                          y='monthly_rent_rm', \n                                          ax=axs);\nplt.ylim(100,5500) #batas harga rent\nplt.xlim(50,3000)  #batas size\nplt.show()\n\n\n\n\n\n\nMonthly Rent\n\n\nCode\nfig, axs = plt.subplots(1,2, figsize=(6,4))\naxs[0].boxplot(data=df, \n               x='monthly_rent_rm')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=df, \n               x='monthly_rent_rm')\naxs[1].set_ylim(100,5500)\naxs[1].set_title('100-5,500 RM')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\n#removing all rows with monthly rent above 5500 RM and below 100RM\ndfx = df.query(\" monthly_rent_rm &gt; 100 & monthly_rent_rm &lt; 5500 \")\n\n\n\n\nSize\n\n\nCode\nfig, axs = plt.subplots(1,2, figsize=(5,4))\naxs[0].boxplot(data=dfx, x='size_sqft')\naxs[0].set_ylim(0,20000)\naxs[0].set_title('all data')\n\naxs[1].boxplot(data=dfx, x='size_sqft')\naxs[1].set_ylim(0,2000)\naxs[1].set_title('50-3,000 square feet')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\n#removing outliers below 500, and higher than 3000 sqft and below 50 sqft\ndfx = dfx.query(\" size_sqft &gt; 50 & size_sqft &lt; 3000 \")\n\n\n\n\n\nData Preparation\n\nPreprocessing Original Data for Categorical Dtypes\nOne must paying attention to the number of categorical observation in the original data, with respect to the sampling train-test value. If, the test_size = 0.3, that means any categorical observation with a total of 3 and less, would not be distributed evenly among train and test data. Below is the process of removing some observation in which appearing only in one of the dataset (train/ test).\n\n\nCode\ndfx_new = dfx[\n    (dfx.location != 'Jinjang') \n    & (dfx.location != 'Serdang') & \n    (dfx.location != 'Sentral') & \n    (dfx.location != 'Others') & \n    (dfx.location != 'Tunku') & \n    (dfx.location != 'Penchala') & \n    (dfx.location != 'Lin') &\n    # (dfx.property_type != 'Others') &\n    (dfx.property_type != 'Condo / Services residence / Penthouse / Townhouse') &\n    (dfx.property_type != 'Townhouse Condo')\n]\n\n\n\n\nInput-Output\n\n\nCode\ndef extractInputOutput(data,\n                       output_column_name):\n    \"\"\"\n    Fungsi untuk memisahkan data input dan output\n    :param data: &lt;pandas dataframe&gt; data seluruh sample\n    :param output_column_name: &lt;string&gt; nama kolom output\n    :return input_data: &lt;pandas dataframe&gt; data input\n    :return output_data: &lt;pandas series&gt; data output\n    \"\"\"\n    output_data = data[output_column_name]\n    input_data = data.drop(output_column_name,\n                           axis = 1)\n    \n    return input_data, output_data\n\n\n\n\nCode\nX, y = extractInputOutput(data=dfx_new, \n                          output_column_name='monthly_rent_rm')\n\n\n\n\nTrain-Test Split Data\n\n\nCode\n#import libraries\nfrom sklearn.model_selection import train_test_split\n\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size = 0.2,\n                                                    random_state = 123)\n\n\n\n\nTraining Data Imputation\n\nNumerical Data\n\n\nCode\nfrom sklearn.impute import SimpleImputer\n\ndef numericalImputation(X_train_num, \n                        strategy = 'most_frequent'):\n    \"\"\"\n    Fungsi untuk melakukan imputasi data numerik NaN\n    :param data: &lt;pandas dataframe&gt; sample data input\n\n    :return X_train_numerical: &lt;pandas dataframe&gt; data numerik\n    :return imputer_numerical: numerical imputer method\n    \"\"\"\n    #buat imputer\n    imputer_num = SimpleImputer(missing_values = np.nan, \n                                strategy = strategy)\n    \n    #fitting\n    imputer_num.fit(X_train_num)\n\n    # transform\n    imputed_data = imputer_num.transform(X_train_num)\n    X_train_num_imputed = pd.DataFrame(imputed_data)\n\n    #pastikan index dan nama kolom antara imputed dan non-imputed SAMA\n    X_train_num_imputed.columns = X_train_num.columns\n    X_train_num_imputed.index = X_train_num.index\n\n    return X_train_num_imputed, imputer_num\n\n\n\n\nCode\nX_train_num =  X_train.select_dtypes(exclude='object')\nX_train_num, imputer_num = numericalImputation(X_train_num, \n                                               strategy='most_frequent')\n\n\n\n\nCategorical Data\n\n\nCode\nX_train_cat = X_train.select_dtypes(include='object')\nX_train_cat, imputer_num = numericalImputation(X_train_cat, \n                                               strategy='most_frequent')\n\n\n\n\nOHE Categorical Data\n\n\nCode\ndef get_dum_n_concat(df_num, df_cat):\n    df_cat_ohe = pd.get_dummies(df_cat)\n    ohe_columns = df_cat_ohe.columns\n    df_concat = pd.concat([df_num, df_cat_ohe], axis=1)\n    print(f\"Number of Cols: {df_concat.shape[1]},\\nNumber of Null Rows: {df_concat.isna().sum()}\")\n    return ohe_columns, df_concat\n\n\n\n\nCode\nohe_col, X_train_concat = get_dum_n_concat(X_train_num, \n                                           X_train_cat)\n\n\nNumber of Cols: 63,\nNumber of Null Rows: completion_year                  0\nrooms                            0\nparking                          0\nbathroom                         0\nsize_sqft                        0\n                                ..\nfurnished_Fully Furnished        0\nfurnished_Not Furnished          0\nfurnished_Partially Furnished    0\nnearby_railways_no               0\nnearby_railways_yes              0\nLength: 63, dtype: int64\n\n\n\n\nStandarisasi\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Buat fungsi\ndef standardizerData(data):\n    \"\"\"\n    Fungsi untuk melakukan standarisasi data\n    :param data: &lt;pandas dataframe&gt; sampel data\n    :return standardized_data: &lt;pandas dataframe&gt; sampel data standard\n    :return standardizer: method untuk standardisasi data\n    \"\"\"\n    data_columns = data.columns  # agar nama kolom tidak hilang\n    data_index = data.index  # agar index tidak hilang\n\n    # buat (fit) standardizer\n    standardizer = StandardScaler()\n    standardizer.fit(data)\n\n    # transform data\n    standardized_data_raw = standardizer.transform(data)\n    standardized_data = pd.DataFrame(standardized_data_raw)\n    standardized_data.columns = data_columns\n    standardized_data.index = data_index\n\n    return standardized_data, standardizer\n\n\n\n\nCode\nX_train_clean, standardizer = standardizerData(data = X_train_concat)\n\n\n\n\n\nTraining Machine Learning\n\nBaseline with Mean value\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n#baseline\ny_baseline = np.ones(len(y_train)) * y_train.mean()\n\n# Predict using the train data\ny_pred_train_mean = y_baseline\n\n# Calculate R-squared\nr2_baseline = r2_score(y_train, \n                       y_pred_train_mean)\n\n#calculate MAE\nmae_baseline = mean_absolute_error(y_train, \n                                   y_pred_train_mean)\n\n\n\n\nLinear Regression\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Train the linear regression model\nlin_reg = LinearRegression().fit(X_train_clean, \n                                 y_train)\n\n# Predict using the train data\ny_pred_train_linreg = lin_reg.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_linreg = mean_absolute_error(y_train, \n                                 y_pred_train_linreg)\n\n# Calculate R-squared\nr2_linreg = r2_score(y_train, \n                     y_pred_train_linreg)\n\n\n\n\nGradientBoosting\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Build random forest\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\n# Fit random forest\ngrad_tree.fit(X_train_clean, y_train)\n\n# Predict\ny_pred_train_gb = grad_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_gb = mean_absolute_error(y_train, \n                             y_pred_train_gb)\n\n# Calculate R-squared\nr2_gb = r2_score(y_train, \n                 y_pred_train_gb)\n\n\n\n\nCode\n#gridsearch\nfrom sklearn.model_selection import GridSearchCV \n\nparams = {'n_estimators': [100, 200, 300, 400, 500],\n              'learning_rate': [0.1, 0.05, 0.01]}\n\n# Buat gridsearch\ngrad_tree = GradientBoostingRegressor(random_state = 123)\n\ngrad_tree_cv = GridSearchCV(estimator = grad_tree,\n                           param_grid = params,\n                           cv = 5,\n                           scoring = \"neg_mean_absolute_error\")\n# Fit grid search cv\ngrad_tree_cv.fit(X_train_clean, \n                 y_train)\n\n# Best params\ngrad_tree_cv.best_params_\n\n\n{'learning_rate': 0.1, 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the GB\ngrad_tree = GradientBoostingRegressor(n_estimators = 500,\n                                      learning_rate=0.1,\n                                      random_state = 123)\n\ngrad_tree.fit(X_train_clean, \n              y_train)\n\n\nGradientBoostingRegressor(n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train_gbcv = grad_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_gb_cv = mean_absolute_error(y_train, \n                                y_pred_train_gbcv)\n\n# Calculate R-squared\nr2_gb_cv = r2_score(y_train, \n                    y_pred_train_gbcv)\n\n\n\n\nRandom Forest\n\n\nCode\n# Build random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrf_tree = RandomForestRegressor(n_estimators = 100,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n# Fit random forest\nrf_tree.fit(X_train_clean, \n            y_train)\n\n\nRandomForestRegressor(max_features='sqrt', random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train_rf = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf = mean_absolute_error(y_train, \n                             y_pred_train_rf)\n\n# Calculate R-squared\nr2_rf = r2_score(y_train, \n                 y_pred_train_rf)\n\nprint(f\"R2-score: {r2_rf:.4f} and MAE score: {mae_rf:.4f}\")\n\n\nR2-score: 0.9577 and MAE score: 100.8408\n\n\n\n\nCode\n#gridsearch\nparams = {\"n_estimators\": [100, 200, 300, 500 ],\n          \"max_features\": [\"sqrt\", \"log2\"]}\n\n# Buat gridsearch\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                random_state = 123)\n\nrf_tree_cv = GridSearchCV(estimator = rf_tree,\n                          param_grid = params,\n                          cv = 5,\n                          scoring = \"neg_mean_absolute_error\")\n# Fit grid search cv\nrf_tree_cv.fit(X_train_clean, \n               y_train)\n\n# Best params\nrf_tree_cv.best_params_\n\n\n{'max_features': 'sqrt', 'n_estimators': 500}\n\n\n\n\nCode\n# Refit the Random Forest\nrf_tree = RandomForestRegressor(criterion = \"squared_error\",\n                                max_features = 'sqrt',\n                                n_estimators = 500,\n                                random_state = 123)\n\n#refit\nrf_tree.fit(X_train_clean, \n            y_train)\n\n\nRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt', n_estimators=500, random_state=123)\n\n\n\n\nCode\n# Predict\ny_pred_train_rfcv = rf_tree.predict(X_train_clean)\n\n# Calculate mean absolute error\nmae_rf_cv = mean_absolute_error(y_train, \n                                y_pred_train_rfcv)\n\n# # Calculate R-squared\nr2_rf_cv = r2_score(y_train, \n                    y_pred_train_rfcv)"
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#results",
    "href": "posts/006-easy-report-machine-learning/index.html#results",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Results",
    "text": "Results\nFigure¬†6 shows the result of all model tested on train dataset.\n\n\nCode\nsns.scatterplot(x=y_train, \n                y=y_pred_train_mean)\nplt.show()\nsns.scatterplot(x=y_train, \n                y=y_pred_train_linreg)\nplt.show()\nsns.scatterplot(x=y_train, \n                y=y_pred_train_gb)\nplt.show()\nsns.scatterplot(x=y_train, \n                y=y_pred_train_gbcv)\nplt.show()\nsns.scatterplot(x=y_train, \n                y=y_pred_train_rf)\nplt.show()\nsns.scatterplot(x=y_train, \n                y=y_pred_train_rfcv)\nplt.show()\n\n\n\n\n\n\nFigure¬†1: Mean\n\n\n\n\n\n\n\nFigure¬†2: Linear Regression\n\n\n\n\n\n\n\n\n\nFigure¬†3: Gradient Boosting\n\n\n\n\n\n\n\nFigure¬†4: Gradient Boosting with CV\n\n\n\n\n\n\n\n\n\nFigure¬†5: Random Forest\n\n\n\n\n\n\n\nFigure¬†6: Random Forest with CV\n\n\n\n\n\n\n\n\nBest Model from Train Dataset\n\n\nCode\nmae_score = [mae_baseline, mae_linreg, \n             mae_gb, mae_gb_cv,\n             mae_rf, mae_rf_cv]\n\nr2_score = [r2_baseline, r2_linreg, \n            r2_gb, r2_gb_cv, \n            r2_rf, r2_rf_cv]\n\nindexes = [\"baseline\", \"linear regression\", \n           \"gradient boosting\", \"gradient boosting with CV\",\n           \"random forest\",  \"random forest with CV\"]\n\nsummary_df = pd.DataFrame({\n    \"MAE Train\": mae_score,\n    \"R2-Score\": r2_score,\n},index = indexes)\n\n#plotting\nfig, axs = plt.subplots(ncols=2, \n                        nrows=1, \n                        figsize=(6,4), \n                        sharey=True)\n\nsummary_df.sort_values(by='R2-Score', \n                       ascending=False).plot(kind='barh', \n                                             y='R2-Score', \n                                             ax=axs[0])\n\nsummary_df.sort_values(by='R2-Score', \n                       ascending=False).plot(kind='barh', \n                                             y='MAE Train', \n                                             ax=axs[1])\nplt.show()\n\n\n\n\nFigure¬†7: Comparison Chart of R2 and MAE for all Models\n\n\n\n\n\n\n\nCode\nsummary_df.applymap(lambda x: round(x, 2))\n\n\n\n\n\n\n\n\n\nMAE Train\nR2-Score\n\n\n\n\nbaseline\n562.37\n0.00\n\n\nlinear regression\n319.22\n0.65\n\n\ngradient boosting\n281.68\n0.72\n\n\ngradient boosting with CV\n228.02\n0.82\n\n\nrandom forest\n100.84\n0.96\n\n\nrandom forest with CV\n99.80\n0.96\n\n\n\n\n\n\n\nAfter several model tested on the train dataset, Random Forest with Hyperparameter tuning has the best R2-score and MAE value as shown in the Figure¬†7. The best model plotted below as reference:\n\n\nBest Model - RF with CV\n\n\n\n\n\nApplied Model on Test Dataset\n\n\nCode\n# libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n#setting up\nrf_tree = RandomForestRegressor(n_estimators = 500,\n                                criterion = \"squared_error\",\n                                max_features = \"sqrt\",\n                                random_state = 123)\n\n#read cleaned test data\nX_test_clean = pd.read_csv(\"./X_test_clean.csv\")\n\n#fit model train\nrf_tree.fit(X_train_clean, \n            y_train)\n\n# Predict model test\ny_pred_test = rf_tree.predict(X_test_clean)\n\n# Calculate mean absolute error\nmae_rf_cv_test = mean_absolute_error(y_test, \n                                     y_pred_test)\n\n# # Calculate R-squared\nr2_rf_cv_test = r2_score(y_test, \n                         y_pred_test)\n\nprint(f\"R2-score: {r2_rf_cv_test:.3f} and MAE score: +/-{mae_rf_cv_test:.2f} RM\")\n\nsns.scatterplot(x=y_test, y=y_pred_test )\nplt.plot([0, 5500], [0,5500], \"--r\")\nplt.xlim(0, 5500)\nplt.xlabel(\"Actual Monthly Rent\")\nplt.ylim(0,5500)\nplt.ylabel(\"Predicted Monthly Rent\")\nplt.suptitle(\"Random Forest - Test Dataset\")\nplt.show()\n\n\nR2-score: 0.803 and MAE score: +/-214.08 RM\n\n\n\n\n\n\n\nCode\nmae_score = [mae_rf_cv,\n             mae_rf_cv_test]\n\nr2_score = [r2_rf_cv, \n            r2_rf_cv_test]\n\nindexes = [\"train\", \"test\"]\n\nsummary_df_train_test = pd.DataFrame({\n    \"MAE Train\": mae_score,\n    \"R2-Score\": r2_score,\n},index = indexes)\n\nsummary_df_train_test.applymap(lambda x: round(x, 2)) \n\n\n\n\n\n\n\n\n\nMAE Train\nR2-Score\n\n\n\n\ntrain\n99.80\n0.96\n\n\ntest\n214.08\n0.80\n\n\n\n\n\n\n\n\n\nFeature Importance\n\n\nCode\n# calculate the feature importances\nimportances = rf_tree.feature_importances_\n\n# rescale the importances back to the original scale of the features\nimportances = importances * X_train_clean.std()\n\n# sort the feature importances in descending order\nsorted_index = importances.argsort()[::-1]\n\n# print the feature importances\ndict_feature_importance = {}\nfor i in sorted_index:\n    # print(\"{}: {}\".format(X_train_clean.columns[i], importances[i]))\n    dict_feature_importance.update({X_train_clean.columns[i]: importances[i]})\n    \n# Create a DataFrame from the dictionary\ndf = pd.DataFrame.from_dict(dict_feature_importance, orient='index', columns=['values'])\n\n# Reset the index to become a column\ndf = df.reset_index()\n\n# Rename the columns\ndf.columns = ['feature', 'importance_value']\n\n#plot\nfig, axs = plt.subplots(figsize=(6,4))\n(df\n .sort_values(by='importance_value', ascending=False)\n .head(10)\n .sort_index(ascending=False)\n .plot(kind='barh', x='feature', ax=axs)\n);"
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#conclusions",
    "href": "posts/006-easy-report-machine-learning/index.html#conclusions",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Conclusions",
    "text": "Conclusions\n\nResult indicates that the best model for prediction is Random Forest with hyperparameter tuning, scoring 95% on R2-score, and a shy 100 RM on MAE. This proves to be a good model since the test dataset gives a scoring of 80% on R2, and 240 RM on MAE.\nThere are some factors that author believed to be affecting the result/ performance of the model:\n\nDropping missing value reduces the performance! Initial model uses half of the data (4-5k rows) and gives poorer performance on R2 and MAE. Imputation and keeping the number of rows close to the original dataset (9k rows) proves to be improving the model. Especially on test dataset.\nFeature selection importance can be seen on the last table, but initially the selection was based on paper and intuition of the author (author lives and work in KL, Malaysia for 5 years). Feature such as completion_year and nearby_railways are important in improving the model.\nLast but not least is the outlier identification. The best practice for me is using jointplot to see not only the distribution of the data in 2-dimension, but also in the third dimension (the density) of the data.\n\nSome insights after feature importance are the size plays a big role in determining the unit price, following size, the furniture availability apparently makes a big impact on the price. This gives an insight to owner of a unit to equip their unit with furniture to fully_furnished should they want to increase their unit market value.\nSome of the feature that were believed to be quite important even before doing the modeling is size_sqft, furnished and location. All three is available within the 10-most features affecting the modeling. As a context, location in KLCC is like Pondok Indah in South Jakarta and location in Kiara is like BSD in South Tangerang, therefore it makes senses to see those locations increasing the price of a rent."
  },
  {
    "objectID": "posts/006-easy-report-machine-learning/index.html#future-works",
    "href": "posts/006-easy-report-machine-learning/index.html#future-works",
    "title": "Easy Report: Malaysia Property Pricing",
    "section": "Future works",
    "text": "Future works\n\nOne of the feature that author thinks is significant but not appearing on the 10-best important feature is nearby_railways. This column is showing if a certain property has a close proximity to a railways (KTM/LRT). The issue is, half of the data is missing, hence the imputation. Author believes, the proximity to nearby railways line can be approximated using Manhanttan distance of railways line to each property unit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nDecoupling GDP from CO2 Emission - Only If You‚Äôre Rich Enough?\n\n\n\n\n\n\n\nenergy\n\n\nCO2\n\n\neconomy\n\n\n\n\nDeveloped countries were able to decoupled their GDP from CO2 emissions, while in developing countries, CO2 emissions is an inevitable consequences of their economic growth.\n\n\n\n\n\n\nJul 23, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotpetrophysics\n\n\n\n\n\n\n\nstreamlit\n\n\ngeoscience\n\n\nproject\n\n\n\n\nA web-application to do petrophysical analysis using LAS file well-log data\n\n\n\n\n\n\nJul 15, 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPetunjuk Hidup dan Menetap di Stavanger, Norway\n\n\n\n\n\n\n\nnorway\n\n\nstavanger\n\n\nlife\n\n\nbahasa-indonesia\n\n\n\n\nHal-hal yang saya harapkan saya tahu dan pahami sebelum saya pindah ke Norway [Bahasa Indonesia]\n\n\n\n\n\n\nJun 26, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nPersonal Website using Jupyter Notebook and Quarto\n\n\n\n\n\n\n\ntutorial\n\n\nquarto\n\n\npython\n\n\nvscode\n\n\n\n\nA Long But Worth It, tutorial on how to make a personal website out of a jupyter notebook, github account, and quarto.\n\n\n\n\n\n\nMar 20, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nEasy Report: Malaysia Property Pricing\n\n\n\n\n\n\n\nproject\n\n\ndata-science\n\n\npython\n\n\nwebscraping\n\n\nreport\n\n\n\n\nTLDR version of web-scraping property ads listing in Kuala Lumpur, Malaysia, and built a machine learning model to predict the rent price.\n\n\n\n\n\n\nFeb 12, 2023\n\n\n25 min\n\n\n\n\n\n\n  \n\n\n\n\nMalaysia Property Pricing - Webscraping & Machine Learning Model\n\n\n\n\n\n\n\nproject\n\n\ndata-science\n\n\npython\n\n\nwebscraping\n\n\n\n\nFull details on creating property dataset using webscraping, and building machine learning model to predict the rent price\n\n\n\n\n\n\nFeb 10, 2023\n\n\n27 min\n\n\n\n\n\n\n  \n\n\n\n\nWebscraping National Exams of Indonesia\n\n\n\n\n\n\n\nproject\n\n\nwebscraping\n\n\ndata-science\n\n\npython\n\n\n\n\nNational Exams 2015-2019. Is Education still decentralized in Java island and its surroundings?\n\n\n\n\n\n\nJan 31, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMedical Insurance Cost - Exploratory Analysis\n\n\n\n\n\n\n\nproject\n\n\ndata-science\n\n\npython\n\n\n\n\nUnderstanding Insurance Medical dataset to answer if smoking, sex, body mass index (BMI) affects the medical charges.\n\n\n\n\n\n\nOct 27, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nMigrating My Personal Blog to Quarto\n\n\n\n\n\n\n\nblog\n\n\nquarto\n\n\n\n\nSome pros and cons that I learnt when I moved my personal website to Quarto from Jekyll-based theme. Difficulties along the way, and what I hoped to be the future of Quarto.\n\n\n\n\n\n\nOct 21, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nMy First Interview in 500 Fortune Company\n\n\n\n\n\n\n\ncareer\n\n\n\n\nDuring the interview, the user asked me a question that was attempted to test my honesty. I said I don‚Äôt know, so I got the job.\n\n\n\n\n\n\nAug 29, 2022\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Twitter\n  \n  \n    \n     Medium\nHi there üëã I am Arie!\nI am currently working as a Geoscientist in the Multi-national company in the field of Energy, i.e.¬†subsurface analytics, energy exploration, and geoscience analytics, in Nordic Country of Norway!\nMy experience in coding started out in 2020, where I have a need to help students learn about petrophysics online. Problem is I could not find a good solution that is free and OS-agnositc. So I developed a web application to load well-log data in LAS file format and do basic petrophysical analysis at plotpetrophysics.streamlit.app.\nI am interested in learning about public policy and its intersection with energy in tackling climate change. I think subsurface analytics will play a role in helping the world navigate around climate challenges. I have a unique skillsets that intersects between energy and data. Personally, I am also interested to learn about policy making, to make more impact - by using data.\nFull detail is available in my resumes:\n- data science resume\n- geoscientist resume"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nB.Eng. in Geological Engineering (Specialised in Energy, and Reservoir Analysis), 2012\nUniversity Gadjah Mada, Indonesia"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nCooking\nRudimental Drums\nData Visualisation\nHiking\nFishing"
  }
]