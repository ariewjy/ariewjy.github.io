{
  "hash": "ffbd600e056a265799952b449a87c63a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Parsing 10+ GB CSV file in Python - Larger than Memory\ndescription: |  \n  A personal quest to find the most effective solution to parse more than tens gigabytes of CSV file in python. \n  Filezie is a poor metric to predict the performance \ntitle-block-banner: true\ndate: '2024-05-19'\ndate-meta: last-modified\ndate-modified: last-modified\ncategories:\n  - python\n  - data\ndraft: false\nfig-cap-location: bottom\nfig-align: left\nlinkcolor: 'green'\nimage: images/plot.png\nexecute: \n  eval: false\ncode-fold: false\ncitation-location: document\n---\n\n\n## Where Am I Coming From\n\nFor context, this comes as my personal side-project from work. The query was started with parsing multiple CSV files, which one of them being more than 10 Gigabytes in size. If you ask why such a huge file size - there are two reasons.\n\n![Borehole Image example - Wikipedia](images/bh_image.png){#fig-bh-img width=\"50%\"}\n\nFor one, a subsurface data comes in many type, one of them is image data @fig-bh-img. Image data is basically a list of array, for high resolution data like in subsurface, this list of array can be a thousands values in a single row - hence the huge file size. Secondly, saving it into the most ineffective file format (csv) is not really helping either. These two reasons make the csv file (@fig-csv-file) becomes huge.\n\n::: callout-important\nDisclaimer: probably not a proper benchmarking from software engineering POV (I am not - but mostly related to my use case.\n:::\n\n## Larger Than Memory\n\nWithout getting into too much details, about 40-50% of RAM size usually the spare of RAM one can use (as we can't really use the whole RAM otherwise, our laptop would freeze). Larger than memory just means the data we try to keep in the RAM is way bigger than the RAM can handle. A 10GB csv file is definitely larger than 8GB RAM, not to mention not all the 8GB can be used.\n\n::: callout-note\nThere is a swapped memory concept here (since we use fast SSD nowadays), but we would not go there for the purpose of this writing.\n:::\n\nIdeally for 10GB csv file, I should have at least 50GB worth of RAM[^1]. So clearly this is something we cannot really handle easily.\n\n[^1]: \"Have 5 to 10 times as much RAM as the size of your dataset\" (Wes Mckinney)\n\nCurrently the script was run with the *de-facto* library a.k.a. **pandas**, so my first instinct was to try different library such as pandas with **pyarrow engine,** **polars**, **dask**, and **duckdb**. At first one might think one library will rule them all - but that will be a mistake, because just a file size is a **poor metric** to evaluate how good the library may performs - we will soon see why.\n\n> I am running all of these tests offline, in my macbook air M1 8GB RAM, 256GB SSD.\n\n![The CSV files with varying Sizes](images/CSVfiles.png){#fig-csv-file width=\"800\"}\n\n## Dummy Data First\n\nSince I can't really use \"actual\" data in this writing, I try to simulate as best as I can by creating a dataset consist of three columns, time, and two columns of list array consist of 10,000 values. The file ends up to be 12GB instead of 10GB, but this would do still, so don't worry.\n\n::: {#f7fcc148 .cell execution_count=1}\n``` {.python .cell-code}\n#create dummy dataset\nimport numpy as np\nimport pandas as pd\n\ndef generate_columns_of_random_numbers(length, total_numbers, rows):\n    start_date = pd.to_datetime('2020-01-01')\n    date_columns = start_date + pd.to_timedelta(np.arange(rows), unit='s')\n\n    horizontal_lists_1 = np.apply_along_axis(\n        lambda _: ', '.join(''.join(np.random.choice(\n          list('0123456789'), length)) for _ in range(total_numbers)),\n        axis=1,\n        arr=np.empty((rows, 1))\n    )\n    horizontal_lists_2 = np.apply_along_axis(\n        lambda _: ', '.join(''.join(np.random.choice(\n          list('0123456789'), length)) for _ in range(total_numbers)),\n        axis=1,\n        arr=np.empty((rows, 1))\n    )\n\n    return list(zip(date_columns, horizontal_lists_1, horizontal_lists_2))\n\n#columns each row consists of a horizontal string of random 5-digit numbers\ncolumns = generate_columns_of_random_numbers(4, 10_000, 100_000)\n\n# DataFrame from the columns\ndf = pd.DataFrame(columns, columns=['Time', 'A', 'B'])\ndf.to_csv(\"test.csv\", index=False)\n```\n:::\n\n\nThe above code will generate exactly the csv file I have, 1GB and 12GB file sizes (we can play around with the number of rows to change the file size). The additional 10GB csv on [NYC taxi from Kaggle](https://www.kaggle.com/datasets/microize/newyork-yellow-taxi-trip-data-2020-2019) that I put as a baseline/ additional data point cause it has similar file size.\n\nNeed to remember that the dummy dataset contains A and B columns where it has array of *strings* even though the values are in numbers. **This is a crucial information and we will back referring this again in the later finding**.\n\n![Creating Dummy Dataset](images/CSVfiles.png)\n\n[**So, with everything set up, let the fun begins!**]{.underline}\n\n## 1. Pandas\n\nThe original script from my work used pandas as is, without chunks at all, and without using pyarrow as a backend. In the following test, we will try to use both, to see if there is any limitation in handling large dataset.\n\n### 1.1. Pandas with Pyarrow\n\nThe new version of pandas allows us to use pyarrow instead of numpy as a backend, with a promise that it will imporove the parsing speed. Below code was used to do just that, as I am parsing three different files (two dummy files, and one kaggle file). Do beware that after every parsing, I cleanup the memory by deleting the dataframe: `del dff`.\n\n::: {#1ce00b1c .cell execution_count=2}\n``` {.python .cell-code}\n#using pandas pyarrow\nimport pandas as pd\nimport time\n\n#defining file\nnyc = \"2018_Yellow_Taxi_Trip_Data.csv\"\ncsv_10gb = \"10GB_file.csv\"\ncsv_1gb = \"1GB_file.csv\"\n\ndef pandas_arrow(url):\n  start = time.time()\n  dff = pd.read_csv(url, \n                    engine='pyarrow'\n                  )\n  memory_use = dff.memory_usage(deep=True).sum()/1_000_000_000\n  print(f\"{dff}\\n{20*'--'}\\n\")\n  print(f\"memory use = {memory_use:.3f} Gb\")\n  print(f\"processing time = {time.time() - start:.3f}s\\n\")\n  del dff #cleanup RAM\n\n#running in three csv\npandas_arrow(csv_1gb)\npandas_arrow(csv_10gb)\npandas_arrow(nyc)\n```\n:::\n\n\nThe result shows both dummy csv with more than 10GB of data parsed successfully, while the nyc csv is failed - likely because out of memory. Using pandas with pyarrow, the 1GB dummy data parsed within 4s, 12GB within 379s (almost 100x longer than 1GB).\n\n![Using Pandas with Pyarrow Backend](images/python_pyarrow.png)\n\n### 1.2. Pandas with Chunks\n\nPandas with numpy backend, although not as fast as pyarrow, offers a benefit of using chunksize in the parameters, to load the data into memory in \"chunks\" of the total data, portion not the entire thing. This helps effective parsing, and avoid killing the RAM early.\n\n::: {#17822052 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport time\n\n#defining file\nnyc = \"2018_Yellow_Taxi_Trip_Data.csv\"\ncsv_10gb = \"10GB_file.csv\"\ncsv_1gb = \"1GB_file.csv\"\n\ndef parsing_pandas(url):\n    start = time.time()\n    chunksize = 10_000\n    dff = pd.DataFrame()\n    for chunk in pd.read_csv(url, \n                                chunksize=chunksize, \n                                low_memory=False\n                                ):\n        dfx = chunk\n        dff = pd.concat([dff, dfx])\n    del dfx\n    memory_use = dff.memory_usage(deep=True).sum()/1_000_000_000\n\n    print(f\"memory use = {memory_use:.2f} Gb\")\n    return dff, print(dff), print(f\"processing {url} time took {time.time() - start}s\")\n    \n#running in three csv files\nparsing_pandas(csv_1gb)\nparsing_pandas(csv_10gb)\nparsing_pandas(nyc)\n```\n:::\n\n\nWorks on my dummy dataset, however - there is no autodetection on datatype, unlike the next two libraries. This means, each of column will be treated as they were a string object - highly ineffecient.\n\nIt also failed in parsing nyc dataset. Ended up killing the process. I don't have the answer why - but I would argue that the number of columns also affect the pandas performance quite a bit, and the memory just failed even after using chunks.\n\n> It may works, but you really have to pay attention to the memory, and making sure we were using the best balanced chunks size parameter - doable, but tricky.\n\n![Using Pandas with Chunks](images/python_pandas_chunks.png)\n\n## 2. Duckdb\n\nThese last two are the best library I have seen so far in handling large dataset. The first library is Duckdb, a library based on SQL, a language optimized for query. Duckdb, unlike any other SQL library, it does not need a defnitive database- until we told it to.\n\n::: {#0e649c1d .cell execution_count=4}\n``` {.python .cell-code}\nimport duckdb\nconn = duckdb.connect()\nimport time\n\nnyc = \"2018_Yellow_Taxi_Trip_Data.csv\"\ncsv_10gb = \"10GB_file.csv\"\ncsv_1gb = \"1GB_file.csv\"\n\nstart = time.time()\n\n#defining function to parse csv\ndef parsing_csv_duckdb(csv_dir):\n    # Define the query dynamically\n    start = time.time()\n    query = f\"\"\"\n    SELECT *\n    FROM read_csv_auto('{csv_dir}')\n    ;\n    \"\"\"\n    # Execute the query\n    result = duckdb.sql(query)\n    return result.show(), print(f\"Processing {csv_dir} time is {time.time() - start:.2f}s\")\n\n# Call the function with the provided variables\nparsing_csv_duckdb(csv_dir=nyc)\nparsing_csv_duckdb(csv_dir=csv_10gb)\nparsing_csv_duckdb(csv_dir=csv_1gb)\n```\n:::\n\n\nNow, my first instinct is saying that it will be able to load the nyc taxi dataset - which it did, at some speed too! However, the most interesting part is, **it was parsing the dummy dataset at slower speed than pandas!** This is more significant (**up to 5x slower than pandas**) on the 10GB csv file! See below:\n\n-   Duckdb 10GB CSV file -\\> 128s, whereas Pandas is 105s -\\> 20% faster\n-   Duckdb 1GB CSV file -\\> 53s, whereas Pandas is 10s -\\> 500% faster\n\nAnother important observations are:\n\n-   Duckdb autocsv detection succeed to detect columns dtype with good accuracy (varchar, double, int64). *Spoiler alert, this will be a recurring theme from this point onwards*.\n-   While it succeed detecting the Time column to be timestamp dtype, it is also classify the A and B columns as string (varchar) as expected. *This maybe the reason why duckb is parsing CSV from dummy dataset slower than pandas with chunk setting*.\n\n![Using Duckdb](images/python_duckdb.png)\n\n## 3. Polars\n\nPolars is another dataframe library just like pandas, unlike duckdb that was based on SQL language, Polars is pretty much the distant brother of Pandas. Although the syntax are not exactly the same, but you will not feel out of place using Polars once we get a grip on its philosophy (expressive language).\n\nThere are four things I valued in Polars:\n\n1.  It was built on Rust language with python binding - **zero dependencies**.\n2.  It was built on Rust language - a low level language - **faster** runtime compared to pandas.\n3.  It uses **no index!** I like index when trying to slicing stuffs, but this concept is more of a hassle than a helper.\n4.  It encourage the use of **expressive method** naming, e.g. it encourage user to use `.ge` as opposed to `>=`, `select` to select multuple columns rather than using double square-bracket `[[\"some list of columns\"]]` to filter a columns.\n\n### 3.1. Eager Evaluation\n\nPolars has two mode when it comes to processing, one is eager mode where everything will be run in one go - as instructed by the user (based on code sequence). Or, using lazy mode where the query will then be optimized in the background - and will not be returned until user decided to do so (collect).\n\nI have a feeling that since the code is fairly simple - I do not think it will give much of a difference to choose one over another.\n\nBelow is the eager evaluation where we need to be careful and make sure that either we put `ignore_errors=True`, or increase the `infer schema length`. Polars error message @fig-polars-largedata-error is the best I have seen - provides a real practical solution to the error. In this case, the error is caused by one value where the number is decimal, inside a **column 12** where most of them are integer - so polars think it should not be `int64`. I chose to use `ignore_errors=True`.\n\n![Errors when Parsing Large CSV with Polars](images/polars_error.png){#fig-polars-largedata-error width=\"600%\"}\n\nAnother suggestion from the offical docs is - whenever we need to use polars for benchamarking, we need to set `rechunk=False`, I tried both (with and without), and surprisingly it runs faster without `rechunk` set as `False`, especially on the 10GB csv file - by about 1-4ms faster (e.g., 111ms vs 114ms). TBH, It does not really matter in this case, so I leave it at `False`.\n\n::: {#c566ce4c .cell execution_count=5}\n``` {.python .cell-code}\nimport polars as pl\nimport time\n\n#defining file\nnyctaxi_10gb = '2018_Yellow_Taxi_Trip_Data.csv'\ncsvFile_10gb = '10GB_file.csv'\ncsvFile_1gb = '1GB_file.csv'\n\ndef parsing_polars(url):\n  start = time.time()\n  dff = (pl.read_csv(url, rechunk=False,\n              ignore_errors=True\n              ))\n  print(dff)\n  print(f\"processing time took {time.time() - start:.3f}s\")\n\n#running in three csv\nparsing_polars(nyctaxi_10gb)\nparsing_polars(csvFile_10gb)\nparsing_polars(csvFile_1gb)\n```\n:::\n\n\n![Using Polars with Eager Evaluation](images/polars_eager.png){#fig-polars-eagers}\n\nObservations:\n\n-   So the polars library can handle three csv as expected, just like duckdb. However, as I mentioned earlier, the duckdb is the best at identifying a time-format automatically from a csv file. This time around, polars has to give it to duckdb, because even polars still detecting the `tpep_pickup_datetime` in nyc dataset as `string`.\n\n-   Another worthy mention is, it took polars close to 50s to what duckdb 0.26s, it is not even a comparison. Parsing nyc dataset, duckdb is just trashed polars in simple parsing test.\n\n### 3.2. Lazy Evaluation\n\nAs mentioned before, the second mode that polars offers is lazy evaluation - where the query will be optimized in the background before users decided to collect the query at the end using 'collect()' method. To optimize this, I also set the streaming=True. The code is similar to before, but now with 'scan_csv' instead of 'read_csv'.\n\n::: {#ef255c1b .cell execution_count=6}\n``` {.python .cell-code}\nimport polars as pl\nimport time\n\n#defining file\nnyctaxi_10gb = '2018_Yellow_Taxi_Trip_Data.csv'\ncsvFile_10gb = '10GB_file.csv'\ncsvFile_1gb = '1GB_file.csv'\n\ndef parsing_polars(url):\n  start = time.time()\n  dff = (pl.scan_csv(url, rechunk=False,\n              ignore_errors=True\n              ))\n  print(dff)\n  print(f\"processing time took {time.time() - start:.3f}s\")\n\n#running in three csv\nparsing_polars(nyctaxi_10gb)\nparsing_polars(csvFile_10gb)\nparsing_polars(csvFile_1gb)\n```\n:::\n\n\n![Using Polars with Lazy Evaluation](images/polars_lazy.png){#fig-polars-lazy}\n\nMy experience, if the query is simple - it tends to same/slower down the runtime, so just run `read_csv` is better as you don't have to think about collecting the result at the end. See @fig-polars-lazy, it runs faster on the nyc dataset by a slight amount (48s eager vs 42s lazy) - call it a tie.\n\n## 4. Dask\n\nDask is another dataframe library, but it is basically pandas in steroid. Instead of using a single-threaded core in computation, dask uses paralel computation. By default it uses lazy evaluation - and `compute()` method to finally query the result. The idea sound simple - pandas in multi-threaded cores, but that means whatever things we see in pandas will appear again e.g., indexes, dependencies to name a few.\n\nThe runtime is fast, as we can see in @fig-dask, dask parsed 10GB csv file in 12s, and 1GB file in 8s - a comparable performance to polars and duckdb. However,there is also errors related to dtypes when trying to parse nyc dataset - so, pick your poison.\n\n![Dask Dataframe](images/dask.png){#fig-dask}\n\n## Verdict\n\nCompiling all the performance runtime on each individual test as follows:\n\n::: {#aa7fe025 .cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nresult = {\n    'Method' : ['Pandas with Pyarrow',\n                 'Pandas with Chunk',\n                 'Duckdb', \n                 'Polars Eager',\n                  'Polars Lazy',\n                  'Dask'\n                  ],\n    '1GB(s)' : [4, 10.3, 53, 7.4, 7.96, 8],\n    '10GB(s)' : [378.5, 105.6, 128.4, 116.7, 171, 12.7],\n    'NYC(s)' : [np.nan, np.nan, 0.26, 48.6, 42, np.nan],\n    'Library': ['pandas', 'pandas', 'duckdb', 'polars', 'polars', 'dask']\n}\n\ndf_result = pd.DataFrame(result)\ndf_result\n\n\ndf_result.plot(kind='barh', x='Method')\n# plt.xscale('log')\nplt.show()\n```\n:::\n\n\nBased on the @fig-conclusion below, and considering all the csv files, take it as you want, but my thoughts are:\n\n![Comparison Speed](images/plot_nonlog.png){#fig-conclusion}\n\n1.  **DuckDB and Poilars** are the only two libraries that perform very well in all csv file sizes (1GB to 10GB).\n2.  For **well-formatted CSV with large size (10GB), duckdb** will be the best library.\n3.  For **everything else, polars** is good enough - especially for a smaller file size (1GB range).\n4.  For relatively low number of columns, and file size around 1GB, use pandas with chunksize or dask.\n5.  ***Don't use pandas with pyarrow. LOL***\n\n#### TLDR;\n\n\n```{mermaid}\n%%| echo: false\n%%| eval: true\n\nflowchart LR\n    A[CSV Filesize] --> B{Around 10GB}\n    B --> |No, Around 1GB| C[Polars, Pandas with Chunksize]\n    B --> |Yes| D{Well-formatted CSV}\n    D --> |Yes| E[DuckDB]\n    D --> |No| F[Polars]\n\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}